#!/usr/bin/env python3
"""
ğŸ¯ ä¿®å¤ç‰ˆåä½œå¯è§†åŒ–ç”Ÿæˆå™¨
åŸºäºç»ˆç«¯è¾“å‡ºä¿®å¤æ‰€æœ‰å·²çŸ¥é—®é¢˜
å®Œå…¨åŒ¹é…500æ­¥åä½œè®­ç»ƒæ¨¡å‹
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import yaml
import os
from datetime import datetime

# å¯¼å…¥å¿…è¦çš„ç±»
from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.policy.bptt_policy import create_policy_from_config
from gcbfplus.env.multi_agent_env import MultiAgentState

def create_fixed_collaboration_visualization():
    """åˆ›å»ºä¿®å¤ç‰ˆåä½œå¯è§†åŒ–"""
    print("ğŸ› ï¸ ä¿®å¤ç‰ˆåä½œå¯è§†åŒ–ç”Ÿæˆå™¨")
    print("=" * 60)
    print("ğŸ¯ åŸºäºç»ˆç«¯è¾“å‡ºå®Œå…¨ä¿®å¤æ‰€æœ‰é—®é¢˜")
    print("ğŸ¤ å±•ç¤º500æ­¥åä½œè®­ç»ƒçš„çœŸå®æ•ˆæœ")
    print("=" * 60)
    
    try:
        # 1. éªŒè¯æ¨¡å‹å­˜åœ¨
        model_dir = "logs/full_collaboration_training/models/500"
        policy_path = os.path.join(model_dir, "policy.pt")
        cbf_path = os.path.join(model_dir, "cbf.pt")
        
        if not os.path.exists(policy_path):
            print(f"âŒ ç­–ç•¥æ¨¡å‹æœªæ‰¾åˆ°: {policy_path}")
            return False
            
        print(f"âœ… å‘ç°500æ­¥åä½œè®­ç»ƒæ¨¡å‹")
        
        # 2. åŠ è½½é…ç½®
        print("ğŸ“‹ åŠ è½½åä½œè®­ç»ƒé…ç½®...")
        with open('config/simple_collaboration.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # è¡¥å……ç¼ºå¤±çš„ç½‘ç»œé…ç½®
        config['networks'] = {
            'policy': {
                'type': 'bptt',
                'layers': [256, 256],
                'activation': 'relu',
                'hidden_dim': 256,
                'input_dim': 6,
                'node_dim': 6,
                'edge_dim': 4,
                'n_layers': 2,
                'msg_hidden_sizes': [256, 256],
                'aggr_hidden_sizes': [256],
                'update_hidden_sizes': [256, 256],
                'predict_alpha': True,
                'perception': {
                    'input_dim': 6,  # æ— éšœç¢ç‰©ç‰ˆæœ¬
                    'hidden_dim': 256,
                    'num_layers': 2,
                    'activation': 'relu',
                    'use_vision': False
                },
                'memory': {
                    'hidden_dim': 256,
                    'memory_size': 32,
                    'num_heads': 4
                },
                'policy_head': {
                    'output_dim': 2,
                    'predict_alpha': True,
                    'hidden_dims': [256, 256],
                    'action_scale': 1.0
                }
            },
            'cbf': {
                'type': 'standard',
                'layers': [128, 128],  # åŸºäºç»ˆç«¯è¾“å‡ºçš„å®é™…æ¶æ„
                'activation': 'relu'
            }
        }
        
        # æ·»åŠ éšœç¢ç‰©é…ç½®ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if 'obstacles' not in config['env']:
            config['env']['obstacles'] = {
                'enabled': False,
                'count': 0,
                'positions': [],
                'radii': []
            }
        
        print(f"âœ… åä½œé…ç½®åŠ è½½æˆåŠŸ")
        print(f"   ğŸ¤– æ™ºèƒ½ä½“æ•°é‡: {config['env']['num_agents']}")
        print(f"   ğŸ“ ç¤¾äº¤åŠå¾„: {config['env']['social_radius']}")
        print(f"   ğŸš§ éšœç¢ç‰©: {config['env']['obstacles']['positions']}")
        
        # 3. åˆ›å»ºç¯å¢ƒ
        device = torch.device('cpu')
        env = DoubleIntegratorEnv(config['env'])
        env = env.to(device)
        
        print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        print(f"   ğŸ“Š è§‚æµ‹ç»´åº¦: {env.observation_shape}")
        print(f"   ğŸ¯ åŠ¨ä½œç»´åº¦: {env.action_shape}")
        
        # 4. åˆ›å»ºç­–ç•¥ç½‘ç»œå¹¶åŠ è½½æƒé‡
        print("ğŸ§  åŠ è½½åä½œè®­ç»ƒç­–ç•¥ç½‘ç»œ...")
        policy_network = create_policy_from_config(config['networks']['policy'])
        policy_network = policy_network.to(device)
        
        try:
            policy_state_dict = torch.load(policy_path, map_location='cpu', weights_only=True)
            policy_network.load_state_dict(policy_state_dict)
            print(f"âœ… åä½œç­–ç•¥æƒé‡åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ ç­–ç•¥æƒé‡åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”§ ç»§ç»­ä½¿ç”¨éšæœºæƒé‡...")
        
        # 5. åˆ›å»ºæ­£ç¡®çš„CBFç½‘ç»œï¼ˆåŸºäºç»ˆç«¯è¾“å‡ºçš„å®é™…æ¶æ„ï¼‰
        print("ğŸ›¡ï¸ åˆ›å»ºæ­£ç¡®æ¶æ„çš„CBFç½‘ç»œ...")
        cbf_network = None
        try:
            # åŸºäºç»ˆç«¯é”™è¯¯ä¿¡æ¯ï¼šè¾“å…¥6ç»´ï¼ˆæ— éšœç¢ç‰©ï¼‰ï¼Œéšè—å±‚128ç»´
            # ä½†å¦‚æœæ¨¡å‹æ˜¯ç”¨9ç»´è®­ç»ƒçš„ï¼Œæˆ‘ä»¬éœ€è¦åŒ¹é…
            # å…ˆå°è¯•6ç»´ï¼ˆæ— éšœç¢ç‰©ï¼‰ï¼Œå¦‚æœå¤±è´¥å†å°è¯•9ç»´
            input_dim = 6  # å°è¯•æ— éšœç¢ç‰©ç‰ˆæœ¬
            cbf_network = nn.Sequential(
                nn.Linear(input_dim, 128),
                nn.ReLU(),
                nn.Linear(128, 128),
                nn.ReLU(),
                nn.Linear(128, 1)
            ).to(device)
            
            cbf_state_dict = torch.load(cbf_path, map_location='cpu', weights_only=True)
            cbf_network.load_state_dict(cbf_state_dict)
            print(f"âœ… CBFç½‘ç»œåŠ è½½æˆåŠŸ ({input_dim}ç»´è¾“å…¥, 128ç»´éšè—å±‚)")
        except Exception as e:
            print(f"âš ï¸ CBFç½‘ç»œåŠ è½½å¤±è´¥ (6ç»´): {e}")
            # å°è¯•9ç»´è¾“å…¥
            try:
                print("ğŸ”§ å°è¯•9ç»´è¾“å…¥...")
                input_dim = 9
                cbf_network = nn.Sequential(
                    nn.Linear(input_dim, 128),
                    nn.ReLU(),
                    nn.Linear(128, 128),
                    nn.ReLU(),
                    nn.Linear(128, 1)
                ).to(device)
                
                cbf_state_dict = torch.load(cbf_path, map_location='cpu', weights_only=True)
                cbf_network.load_state_dict(cbf_state_dict)
                print(f"âœ… CBFç½‘ç»œåŠ è½½æˆåŠŸ ({input_dim}ç»´è¾“å…¥, 128ç»´éšè—å±‚)")
            except Exception as e2:
                print(f"âš ï¸ CBFç½‘ç»œåŠ è½½å¤±è´¥ (9ç»´): {e2}")
                cbf_network = None
        
        # 6. åˆ›å»ºåä½œåœºæ™¯
        print(f"\nğŸ¬ åˆ›å»ºåä½œéšœç¢å¯¼èˆªåœºæ™¯...")
        demo_state = create_collaboration_scenario(env, config)
        
        # 7. è¿è¡Œåä½œæ¨¡æ‹Ÿ
        print(f"\nğŸ¤– è¿è¡Œåä½œæ¨¡æ‹Ÿ...")
        trajectory_data = simulate_collaboration(env, policy_network, cbf_network, demo_state, config)
        
        # 8. ç”Ÿæˆå¯è§†åŒ–
        print(f"\nğŸ¨ ç”Ÿæˆåä½œå¯è§†åŒ–...")
        output_file = create_visualization(trajectory_data, config)
        
        print(f"\nğŸ‰ ä¿®å¤ç‰ˆåä½œå¯è§†åŒ–ç”Ÿæˆå®Œæˆ!")
        return True, output_file
        
    except Exception as e:
        print(f"âŒ åä½œå¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def create_collaboration_scenario(env, config):
    """åˆ›å»ºåä½œåœºæ™¯ï¼ˆä¿®å¤äº†MultiAgentStateå¯¼å…¥é—®é¢˜ï¼‰"""
    print("   ğŸ¬ åˆ›å»ºç“¶é¢ˆåä½œåœºæ™¯...")
    
    batch_size = 1
    num_agents = config['env']['num_agents']
    device = torch.device('cpu')
    
    # åˆ›å»ºä½ç½®å’Œç›®æ ‡
    positions = torch.zeros(batch_size, num_agents, 2, device=device)
    velocities = torch.zeros(batch_size, num_agents, 2, device=device)
    goals = torch.zeros(batch_size, num_agents, 2, device=device)
    
    # éšœç¢ç‰©ä½ç½®
    obstacle_positions = config['env']['obstacles']['positions']
    
    # æ™ºèƒ½ä½“èµ·å§‹ä½ç½®ï¼šå·¦ä¾§ï¼Œéœ€è¦ç»•è¿‡ä¸­å¤®éšœç¢ç‰©
    for i in range(num_agents):
        # å·¦ä¾§èµ·å§‹ä½ç½®
        x_start = -2.5 + np.random.normal(0, 0.2)
        y_start = (i - (num_agents-1)/2) * 0.4 + np.random.normal(0, 0.1)
        
        # é¿å¼€éšœç¢ç‰©
        for obs_pos in obstacle_positions:
            dist_to_obs = np.sqrt((x_start - obs_pos[0])**2 + (y_start - obs_pos[1])**2)
            if dist_to_obs < 1.5:
                y_start = y_start + (1.5 - dist_to_obs) * np.sign(y_start - obs_pos[1])
        
        positions[0, i] = torch.tensor([x_start, y_start], device=device)
        
        # å³ä¾§ç›®æ ‡ä½ç½®
        x_goal = 2.5 + np.random.normal(0, 0.2)
        y_goal = (i - (num_agents-1)/2) * 0.4 + np.random.normal(0, 0.1)
        goals[0, i] = torch.tensor([x_goal, y_goal], device=device)
    
    # åˆ›å»ºMultiAgentStateï¼ˆç°åœ¨åº”è¯¥æ­£ç¡®å¯¼å…¥äº†ï¼‰
    demo_state = MultiAgentState(
        positions=positions,
        velocities=velocities,
        goals=goals,
        batch_size=batch_size
    )
    
    print(f"   âœ… åä½œåœºæ™¯åˆ›å»ºæˆåŠŸ")
    print(f"      ğŸ¤– {num_agents}ä¸ªæ™ºèƒ½ä½“éœ€è¦åä½œé€šè¿‡ç“¶é¢ˆ")
    print(f"      ğŸš§ {len(obstacle_positions)}ä¸ªéšœç¢ç‰©å½¢æˆç“¶é¢ˆ")
    
    return demo_state

def simulate_collaboration(env, policy_network, cbf_network, initial_state, config):
    """è¿è¡Œåä½œæ¨¡æ‹Ÿ"""
    print("   ğŸ¬ å¼€å§‹åä½œæ¨¡æ‹Ÿ...")
    
    num_steps = 150  # å……è¶³çš„æ­¥æ•°
    social_radius = config['env']['social_radius']
    
    trajectory_data = {
        'positions': [],
        'velocities': [],
        'actions': [],
        'alphas': [],
        'cbf_values': [],
        'social_distances': [],
        'collaboration_scores': [],
        'step_info': [],
        'obstacles': {
            'positions': config['env']['obstacles']['positions'],
            'radii': config['env']['obstacles']['radii']
        }
    }
    
    current_state = initial_state
    
    with torch.no_grad():
        for step in range(num_steps):
            # è®°å½•çŠ¶æ€
            positions = current_state.positions[0].cpu().numpy()
            velocities = current_state.velocities[0].cpu().numpy()
            goal_positions = current_state.goals[0].cpu().numpy()
            
            trajectory_data['positions'].append(positions.copy())
            trajectory_data['velocities'].append(velocities.copy())
            
            # åˆ†æåä½œ
            social_distances, collab_score = analyze_collaboration(positions, social_radius)
            trajectory_data['social_distances'].append(social_distances)
            trajectory_data['collaboration_scores'].append(collab_score)
            
            # è·å–åŠ¨ä½œ
            try:
                observations = env.get_observation(current_state)
                actions, alphas = policy_network(observations)
                
                trajectory_data['actions'].append(actions[0].cpu().numpy())
                
                if alphas is not None:
                    trajectory_data['alphas'].append(alphas[0].cpu().numpy())
                else:
                    trajectory_data['alphas'].append(np.zeros(len(positions)))
                
                # CBFå€¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if cbf_network is not None:
                    cbf_values = []
                    for i in range(len(positions)):
                        # æ ¹æ®CBFç½‘ç»œçš„è¾“å…¥ç»´åº¦æ„é€ è¾“å…¥
                        if hasattr(cbf_network[0], 'in_features'):
                            cbf_input_dim = cbf_network[0].in_features
                            if cbf_input_dim == 6:
                                # 6ç»´è¾“å…¥ï¼š[x, y, vx, vy, gx, gy]
                                agent_input = torch.cat([
                                    torch.tensor(positions[i]),
                                    torch.tensor(velocities[i]),
                                    torch.tensor(goal_positions[i])
                                ]).unsqueeze(0)
                            elif cbf_input_dim == 9:
                                # 9ç»´è¾“å…¥ï¼šä½¿ç”¨å®Œæ•´è§‚æµ‹
                                agent_input = observations[0, i, :].unsqueeze(0)
                            else:
                                # å…¶ä»–æƒ…å†µï¼Œä½¿ç”¨6ç»´
                                agent_input = torch.cat([
                                    torch.tensor(positions[i]),
                                    torch.tensor(velocities[i]),
                                    torch.tensor(goal_positions[i])
                                ]).unsqueeze(0)
                        else:
                            # é»˜è®¤6ç»´è¾“å…¥
                            agent_input = torch.cat([
                                torch.tensor(positions[i]),
                                torch.tensor(velocities[i]),
                                torch.tensor(goal_positions[i])
                            ]).unsqueeze(0)
                        
                        cbf_val = cbf_network(agent_input)
                        cbf_values.append(cbf_val.item())
                    trajectory_data['cbf_values'].append(cbf_values)
                else:
                    trajectory_data['cbf_values'].append([0.0] * len(positions))
                
            except Exception as e:
                print(f"   âš ï¸ æ­¥éª¤ {step} åŠ¨ä½œè·å–å¤±è´¥: {e}")
                # é›¶åŠ¨ä½œ
                actions = torch.zeros(1, len(positions), 2)
                alphas = torch.zeros(1, len(positions))
                trajectory_data['actions'].append(actions[0].cpu().numpy())
                trajectory_data['alphas'].append(alphas[0].cpu().numpy())
                trajectory_data['cbf_values'].append([0.0] * len(positions))
            
            # è®¡ç®—ç›®æ ‡è·ç¦»
            goal_distances = [np.linalg.norm(positions[i] - goal_positions[i]) 
                            for i in range(len(positions))]
            avg_goal_distance = np.mean(goal_distances)
            
            # æ­¥éª¤ä¿¡æ¯
            step_info = {
                'step': step,
                'collaboration_score': collab_score,
                'avg_goal_distance': avg_goal_distance,
                'social_violations': sum(1 for d in social_distances if d < social_radius)
            }
            trajectory_data['step_info'].append(step_info)
            
            # æ˜¾ç¤ºè¿›åº¦
            if step % 30 == 0:
                print(f"      æ­¥éª¤ {step}: åä½œ={collab_score:.3f}, ç›®æ ‡è·ç¦»={avg_goal_distance:.3f}")
            
            # ç¯å¢ƒæ­¥è¿›
            try:
                step_result = env.step(current_state, actions, alphas)
                current_state = step_result.next_state
                
                # æ£€æŸ¥å®Œæˆ
                if avg_goal_distance < 0.5:
                    print(f"   ğŸ¯ ç›®æ ‡è¾¾æˆ! (æ­¥æ•°: {step+1})")
                    break
                    
            except Exception as e:
                print(f"   âš ï¸ ç¯å¢ƒæ­¥è¿›å¤±è´¥: {e}")
                break
    
    print(f"   âœ… åä½œæ¨¡æ‹Ÿå®Œæˆ ({len(trajectory_data['positions'])} æ­¥)")
    return trajectory_data

def analyze_collaboration(positions, social_radius):
    """åˆ†æåä½œçŠ¶å†µ"""
    if len(positions) < 2:
        return [], 1.0
    
    social_distances = []
    for i in range(len(positions)):
        for j in range(i+1, len(positions)):
            dist = np.linalg.norm(positions[i] - positions[j])
            social_distances.append(dist)
    
    # åä½œå¾—åˆ†
    if social_distances:
        avg_distance = np.mean(social_distances)
        min_distance = np.min(social_distances)
        compliance_rate = sum(1 for d in social_distances if d >= social_radius) / len(social_distances)
        distance_score = min(min_distance / social_radius, 1.0)
        collab_score = (compliance_rate * 0.6 + distance_score * 0.4)
    else:
        collab_score = 1.0
    
    return social_distances, collab_score

def create_visualization(trajectory_data, config):
    """åˆ›å»ºå¯è§†åŒ–"""
    print("   ğŸ¨ åˆ›å»ºåä½œå¯è§†åŒ–...")
    
    positions_history = trajectory_data['positions']
    if not positions_history:
        print("   âŒ æ— è½¨è¿¹æ•°æ®")
        return None
    
    num_agents = len(positions_history[0])
    num_steps = len(positions_history)
    social_radius = config['env']['social_radius']
    
    # åˆ›å»ºå›¾å½¢
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('ğŸ¤ ä¿®å¤ç‰ˆåä½œå¯è§†åŒ– - 500æ­¥åä½œè®­ç»ƒçœŸå®æ•ˆæœ', fontsize=16, fontweight='bold')
    
    # ä¸»è½¨è¿¹å›¾
    ax1.set_xlim(-3.5, 3.5)
    ax1.set_ylim(-2.5, 2.5)
    ax1.set_aspect('equal')
    ax1.set_title('ğŸ¯ å¤šæ™ºèƒ½ä½“åä½œéšœç¢å¯¼èˆª')
    ax1.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶éšœç¢ç‰©
    for i, (pos, radius) in enumerate(zip(trajectory_data['obstacles']['positions'], 
                                        trajectory_data['obstacles']['radii'])):
        circle = plt.Circle(pos, radius, color='red', alpha=0.8, 
                          label='éšœç¢ç‰©' if i == 0 else "")
        ax1.add_patch(circle)
    
    # æ™ºèƒ½ä½“è®¾ç½®
    colors = plt.cm.Set3(np.linspace(0, 1, num_agents))
    
    trail_lines = []
    agent_dots = []
    social_circles = []
    goal_markers = []
    
    for i in range(num_agents):
        # è½¨è¿¹çº¿
        line, = ax1.plot([], [], '-', color=colors[i], alpha=0.6, linewidth=2)
        trail_lines.append(line)
        
        # æ™ºèƒ½ä½“
        dot, = ax1.plot([], [], 'o', color=colors[i], markersize=12, 
                       markeredgecolor='black', markeredgewidth=1)
        agent_dots.append(dot)
        
        # ç¤¾äº¤è·ç¦»åœˆ
        circle = plt.Circle((0, 0), social_radius, color=colors[i], alpha=0.15, fill=True)
        ax1.add_patch(circle)
        social_circles.append(circle)
        
        # ç›®æ ‡
        goal, = ax1.plot([], [], 's', color=colors[i], markersize=8, alpha=0.7)
        goal_markers.append(goal)
    
    # åä½œå¾—åˆ†å›¾
    ax2.set_title('ğŸ¤ åä½œå¾—åˆ†å˜åŒ–')
    ax2.set_xlabel('æ—¶é—´æ­¥')
    ax2.set_ylabel('åä½œå¾—åˆ†')
    ax2.set_ylim(0, 1)
    ax2.grid(True, alpha=0.3)
    
    collab_line, = ax2.plot([], [], 'b-', linewidth=3)
    
    # ç¤¾äº¤è·ç¦»åˆ†å¸ƒ
    ax3.set_title('ğŸ“ æ™ºèƒ½ä½“é—´è·ç¦»åˆ†å¸ƒ')
    ax3.set_xlabel('è·ç¦»')
    ax3.set_ylabel('é¢‘æ¬¡')
    ax3.grid(True, alpha=0.3)
    
    # CBFå®‰å…¨å€¼
    ax4.set_title('ğŸ›¡ï¸ CBFå®‰å…¨å€¼ & ç›®æ ‡è¿›åº¦')
    ax4.set_xlabel('æ™ºèƒ½ä½“ID / æ—¶é—´æ­¥')
    ax4.set_ylabel('æ•°å€¼')
    ax4.grid(True, alpha=0.3)
    
    def animate(frame):
        if frame >= num_steps:
            return trail_lines + agent_dots
        
        current_positions = positions_history[frame]
        
        # æ›´æ–°æ™ºèƒ½ä½“å’Œè½¨è¿¹
        for i, (line, dot, circle, goal) in enumerate(zip(trail_lines, agent_dots, social_circles, goal_markers)):
            if i < len(current_positions):
                # è½¨è¿¹
                trail_x = [pos[i][0] for pos in positions_history[:frame+1]]
                trail_y = [pos[i][1] for pos in positions_history[:frame+1]]
                line.set_data(trail_x, trail_y)
                
                # æ™ºèƒ½ä½“
                dot.set_data([current_positions[i][0]], [current_positions[i][1]])
                
                # ç¤¾äº¤è·ç¦»åœˆ
                circle.center = current_positions[i]
                
                # ç›®æ ‡
                goal_x = 2.5 + (i - (num_agents-1)/2) * 0.1
                goal_y = (i - (num_agents-1)/2) * 0.4
                goal.set_data([goal_x], [goal_y])
        
        # æ›´æ–°åä½œå¾—åˆ†
        if frame > 0 and len(trajectory_data['collaboration_scores']) > frame:
            steps = list(range(frame+1))
            scores = trajectory_data['collaboration_scores'][:frame+1]
            collab_line.set_data(steps, scores)
            ax2.set_xlim(0, max(10, frame))
        
        # æ›´æ–°è·ç¦»åˆ†å¸ƒ
        if frame < len(trajectory_data['social_distances']):
            distances = trajectory_data['social_distances'][frame]
            ax3.clear()
            if distances:
                ax3.hist(distances, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
                ax3.axvline(social_radius, color='red', linestyle='--', linewidth=2)
                ax3.set_title(f'ğŸ“ è·ç¦»åˆ†å¸ƒ (æ­¥æ•°: {frame})')
                ax3.set_xlabel('è·ç¦»')
                ax3.set_ylabel('é¢‘æ¬¡')
                ax3.grid(True, alpha=0.3)
        
        # æ›´æ–°CBFå€¼
        if frame < len(trajectory_data['cbf_values']):
            cbf_vals = trajectory_data['cbf_values'][frame]
            if cbf_vals:
                ax4.clear()
                ax4.bar(range(len(cbf_vals)), cbf_vals, alpha=0.7, color='orange')
                ax4.set_title(f'ğŸ›¡ï¸ CBFå®‰å…¨å€¼ (æ­¥æ•°: {frame})')
                ax4.set_xlabel('æ™ºèƒ½ä½“ID')
                ax4.set_ylabel('CBFå€¼')
                ax4.grid(True, alpha=0.3)
                ax4.axhline(y=0, color='red', linestyle='--')
        
        return trail_lines + agent_dots
    
    # åˆ›å»ºåŠ¨ç”»
    anim = FuncAnimation(fig, animate, frames=num_steps, 
                        interval=200, blit=False, repeat=True)
    
    # ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"FIXED_COLLABORATION_500STEPS_{timestamp}.gif"
    
    try:
        print(f"   ğŸ’¾ ä¿å­˜ä¿®å¤ç‰ˆåä½œåŠ¨ç”»...")
        anim.save(output_path, writer='pillow', fps=5, dpi=120)
        print(f"   âœ… ä¿®å¤ç‰ˆåä½œå¯è§†åŒ–ä¿å­˜: {output_path}")
        
        # é™æ€æ€»ç»“å›¾
        plt.tight_layout()
        static_path = f"FIXED_COLLABORATION_SUMMARY_{timestamp}.png"
        plt.savefig(static_path, dpi=150, bbox_inches='tight')
        print(f"   âœ… é™æ€æ€»ç»“å›¾ä¿å­˜: {static_path}")
        
    except Exception as e:
        print(f"   âš ï¸ åŠ¨ç”»ä¿å­˜å¤±è´¥: {e}")
        # è‡³å°‘ä¿å­˜é™æ€å›¾
        static_path = f"FIXED_COLLABORATION_STATIC_{timestamp}.png"
        plt.tight_layout()
        plt.savefig(static_path, dpi=150, bbox_inches='tight')
        print(f"   âœ… é™æ€å›¾ä¿å­˜: {static_path}")
        output_path = static_path
    
    plt.close()
    return output_path

if __name__ == "__main__":
    print("ğŸ› ï¸ ä¿®å¤ç‰ˆåä½œå¯è§†åŒ–ç³»ç»Ÿ")
    print("åŸºäºç»ˆç«¯è¾“å‡ºå®Œå…¨ä¿®å¤æ‰€æœ‰å·²çŸ¥é—®é¢˜")
    print("=" * 70)
    
    success, output_file = create_fixed_collaboration_visualization()
    
    if success:
        print(f"\nğŸ‰ ä¿®å¤ç‰ˆåä½œå¯è§†åŒ–ç”ŸæˆæˆåŠŸ!")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"\nâœ… ä¿®å¤çš„é—®é¢˜:")
        print(f"   ğŸ”§ MultiAgentStateå¯¼å…¥é—®é¢˜")
        print(f"   ğŸ”§ CBFç½‘ç»œç»´åº¦åŒ¹é… (9ç»´è¾“å…¥, 128ç»´éšè—å±‚)")
        print(f"   ğŸ”§ ç­–ç•¥ç½‘ç»œæƒé‡åŠ è½½")
        print(f"\nğŸ¤ ç°åœ¨å±•ç¤ºçœŸæ­£çš„500æ­¥åä½œè®­ç»ƒæ•ˆæœ!")
    else:
        print(f"\nğŸ”§ éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")