#!/usr/bin/env python3
"""
åŸºäºçœŸå®è®­ç»ƒæ¨¡å‹çš„å¯è§†åŒ–ç”Ÿæˆå™¨
ä½¿ç”¨å®é™…çš„GCBF+BPTT+åŠ¨æ€Alphaæ¨¡å‹è¿›è¡Œæ¨ç†
"""
import numpy as np
import torch
import yaml
import os
from pathlib import Path
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter
from matplotlib.patches import Circle, Rectangle

# å¯¼å…¥æ‚¨çš„çœŸå®æ¨¡å‹
from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.policy import create_policy_from_config

def load_real_trained_model(model_dir, step, device):
    """åŠ è½½çœŸå®è®­ç»ƒçš„æ¨¡å‹"""
    print(f"ğŸ”„ Loading real trained model from: {model_dir}/models/{step}")
    
    # åŠ è½½é…ç½®
    config_path = Path(model_dir) / "config.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åˆ›å»ºç­–ç•¥ç½‘ç»œ
    policy_config = config.get('networks', {}).get('policy', {})
    policy_network = create_policy_from_config(policy_config)
    policy_network = policy_network.to(device)
    
    # åŠ è½½æƒé‡
    model_path = Path(model_dir) / "models" / str(step) / "policy.pt"
    if model_path.exists():
        try:
            state_dict = torch.load(model_path, map_location=device, weights_only=False)
            policy_network.load_state_dict(state_dict, strict=False)
            print("âœ… Successfully loaded policy network")
        except Exception as e:
            print(f"âš ï¸ Warning: Could not load model weights: {e}")
            print("Using random initialized weights for demonstration")
    
    policy_network.eval()
    
    # å°è¯•åŠ è½½CBFç½‘ç»œ
    cbf_network = None
    cbf_path = Path(model_dir) / "models" / str(step) / "cbf.pt"
    if cbf_path.exists():
        try:
            # ç®€å•çš„CBFç½‘ç»œç»“æ„ï¼ˆæ‚¨å¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
            obs_dim = 8 * 9  # num_agents * obs_per_agent
            cbf_network = torch.nn.Sequential(
                torch.nn.Linear(obs_dim, 256),
                torch.nn.ReLU(),
                torch.nn.Linear(256, 256),
                torch.nn.ReLU(),
                torch.nn.Linear(256, 1)
            ).to(device)
            
            cbf_state_dict = torch.load(cbf_path, map_location=device, weights_only=False)
            cbf_network.load_state_dict(cbf_state_dict, strict=False)
            cbf_network.eval()
            print("âœ… Successfully loaded CBF network")
        except Exception as e:
            print(f"âš ï¸ Warning: Could not load CBF network: {e}")
            cbf_network = None
    
    return policy_network, cbf_network, config

def run_real_model_simulation(policy_network, cbf_network, env, device, num_steps=200):
    """ä½¿ç”¨çœŸå®æ¨¡å‹è¿è¡Œä»¿çœŸ"""
    print("ğŸ® Running real model simulation...")
    
    # åˆå§‹åŒ–ç¯å¢ƒ
    state = env.reset()
    
    # å­˜å‚¨è½¨è¿¹å’ŒAlphaå€¼
    all_positions = []
    all_alphas = []
    all_actions = []
    
    # å¦‚æœç­–ç•¥ç½‘ç»œæœ‰memoryï¼Œåˆå§‹åŒ–hidden state
    hidden_state = None
    if hasattr(policy_network, 'memory') and policy_network.memory is not None:
        batch_size = 1
        hidden_state = torch.zeros(1, policy_network.memory.hidden_dim).to(device)
    
    with torch.no_grad():
        for step in range(num_steps):
            # å‡†å¤‡è§‚æµ‹
            if isinstance(state, dict):
                obs = state['observation']
            else:
                obs = state
            
            if not isinstance(obs, torch.Tensor):
                obs = torch.FloatTensor(obs).to(device)
            
            if len(obs.shape) == 1:
                obs = obs.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            
            try:
                # ç­–ç•¥ç½‘ç»œæ¨ç†
                if hidden_state is not None:
                    # å¸¦è®°å¿†çš„ç­–ç•¥ç½‘ç»œ
                    action_logits, alpha_pred, hidden_state = policy_network(obs, hidden_state)
                else:
                    # æ— è®°å¿†ç­–ç•¥ç½‘ç»œ
                    output = policy_network(obs)
                    if isinstance(output, tuple):
                        if len(output) == 2:
                            action_logits, alpha_pred = output
                        else:
                            action_logits = output[0]
                            alpha_pred = output[1] if len(output) > 1 else None
                    else:
                        action_logits = output
                        alpha_pred = None
                
                # å¤„ç†åŠ¨ä½œ
                if isinstance(action_logits, torch.Tensor):
                    if len(action_logits.shape) > 2:
                        action_logits = action_logits.squeeze()
                    action = action_logits.cpu().numpy()
                    if len(action.shape) > 1:
                        action = action[0]  # å–ç¬¬ä¸€ä¸ªbatch
                else:
                    action = np.zeros((env.num_agents, 2))  # é»˜è®¤é›¶åŠ¨ä½œ
                
                # å¤„ç†Alphaå€¼
                if alpha_pred is not None:
                    if isinstance(alpha_pred, torch.Tensor):
                        alpha_values = alpha_pred.cpu().numpy()
                        if len(alpha_values.shape) > 1:
                            alpha_values = alpha_values[0]  # å–ç¬¬ä¸€ä¸ªbatch
                        if len(alpha_values.shape) == 0:
                            alpha_values = np.full(env.num_agents, float(alpha_values))
                        elif len(alpha_values) != env.num_agents:
                            # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œæ‰©å±•æˆ–æˆªæ–­
                            if len(alpha_values) == 1:
                                alpha_values = np.full(env.num_agents, alpha_values[0])
                            else:
                                alpha_values = np.full(env.num_agents, np.mean(alpha_values))
                    else:
                        alpha_values = np.full(env.num_agents, 1.0)
                else:
                    # å¦‚æœæ²¡æœ‰é¢„æµ‹Alphaï¼Œä½¿ç”¨ç¯å¢ƒé»˜è®¤å€¼
                    alpha_values = np.full(env.num_agents, env.cbf_alpha)
                
            except Exception as e:
                print(f"âš ï¸ Model inference error at step {step}: {e}")
                # ä½¿ç”¨é»˜è®¤å€¼
                action = np.zeros((env.num_agents, 2))
                alpha_values = np.full(env.num_agents, 1.0)
            
            # ç¯å¢ƒæ­¥è¿›
            try:
                state, reward, done, info = env.step(action)
            except Exception as e:
                print(f"âš ï¸ Environment step error: {e}")
                break
            
            # è®°å½•æ•°æ®
            current_positions = env.get_positions()  # éœ€è¦å®ç°è¿™ä¸ªæ–¹æ³•
            if hasattr(env, 'agent_positions'):
                current_positions = env.agent_positions.cpu().numpy()
            elif hasattr(env, 'state') and hasattr(env.state, 'pos'):
                current_positions = env.state.pos.cpu().numpy() 
            else:
                # ä»è§‚æµ‹ä¸­æå–ä½ç½®ä¿¡æ¯
                if isinstance(state, dict) and 'observation' in state:
                    obs_array = state['observation']
                else:
                    obs_array = state
                
                if isinstance(obs_array, torch.Tensor):
                    obs_array = obs_array.cpu().numpy()
                
                # å‡è®¾è§‚æµ‹çš„å‰ä¸¤ä¸ªç»´åº¦æ˜¯ä½ç½®
                if len(obs_array.shape) == 2:
                    current_positions = obs_array[:, :2]  # [num_agents, 2]
                else:
                    current_positions = obs_array.reshape(-1, obs_array.shape[-1])[:, :2]
            
            all_positions.append(current_positions.copy())
            all_alphas.append(alpha_values.copy())
            all_actions.append(action.copy() if isinstance(action, np.ndarray) else np.array(action))
            
            if done:
                break
    
    print(f"âœ… Simulation completed: {len(all_positions)} steps")
    return all_positions, all_alphas, all_actions

def create_real_model_visualization():
    """åˆ›å»ºåŸºäºçœŸå®æ¨¡å‹çš„å¯è§†åŒ–"""
    print("ğŸ¬ Creating Real Model-Based Visualization...")
    
    # å‚æ•°è®¾ç½®
    model_dir = "logs/dynamic_alpha_vision"  # æ‚¨çš„æ¨¡å‹ç›®å½•
    model_step = 2000
    config_file = "config/bottleneck_fixed_alpha_medium.yaml"
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ Using device: {device}")
    
    # åŠ è½½é…ç½®
    with open(config_file, 'r', encoding='utf-8') as f:
        env_config = yaml.safe_load(f)
    
    # åˆ›å»ºç¯å¢ƒ
    print("ğŸ—ï¸ Creating environment...")
    env = DoubleIntegratorEnv(env_config['env']).to(device)
    
    try:
        # åŠ è½½çœŸå®è®­ç»ƒæ¨¡å‹
        policy_network, cbf_network, model_config = load_real_trained_model(
            model_dir, model_step, device
        )
        
        # è¿è¡ŒçœŸå®æ¨¡å‹ä»¿çœŸ
        all_positions, all_alphas, all_actions = run_real_model_simulation(
            policy_network, cbf_network, env, device, num_steps=200
        )
        
        model_based = True
        print("âœ… Using REAL trained model for visualization")
        
    except Exception as e:
        print(f"âš ï¸ Could not load real model, using demonstration: {e}")
        # é™çº§åˆ°æ¼”ç¤ºæ¨¡å¼
        all_positions, all_alphas = create_demonstration_data(env.num_agents, 200)
        model_based = False
        print("ğŸ“º Using demonstration data")
    
    # åˆ›å»ºå¯è§†åŒ–
    create_advanced_visualization(all_positions, all_alphas, env_config, model_based)
    
    return "real_model_visualization.gif"

def create_demonstration_data(num_agents, num_steps):
    """åˆ›å»ºæ¼”ç¤ºæ•°æ®ï¼ˆå¦‚æœæ— æ³•åŠ è½½çœŸå®æ¨¡å‹ï¼‰"""
    np.random.seed(42)
    
    all_positions = []
    all_alphas = []
    
    # ç”Ÿæˆæ›´çœŸå®çš„Alphaå€¼ï¼ˆè¿ç»­å˜åŒ–ï¼‰
    base_alpha = 1.0
    alpha_noise = np.random.normal(0, 0.1, (num_steps, num_agents))
    distance_factor = np.random.uniform(0.8, 1.2, (num_steps, num_agents))
    
    for step in range(num_steps):
        # ç®€å•çš„ä½ç½®ç”Ÿæˆ
        progress = step / num_steps
        positions = []
        step_alphas = []
        
        for i in range(num_agents):
            start_delay = i * 8
            if step < start_delay:
                x = -2.0 + i * 0.1
                y = np.random.uniform(-0.8, 0.8)
                alpha = base_alpha
            else:
                actual_progress = (step - start_delay) / (num_steps - start_delay)
                x = -2.0 + actual_progress * 4.0 + np.sin(step * 0.1 + i) * 0.05
                y = np.random.uniform(-0.8, 0.8) + np.cos(step * 0.08 + i) * 0.1
                
                # è¿ç»­å˜åŒ–çš„Alphaå€¼
                base_value = base_alpha * distance_factor[step, i]
                noise = alpha_noise[step, i] * 0.2
                proximity_factor = 1.0 + 0.5 * np.exp(-abs(x) * 2)  # ç“¶é¢ˆåŒºåŸŸå½±å“
                alpha = np.clip(base_value + noise + proximity_factor, 0.8, 2.5)
            
            positions.append([x, y])
            step_alphas.append(alpha)
        
        all_positions.append(np.array(positions))
        all_alphas.append(np.array(step_alphas))
    
    return all_positions, all_alphas

def create_advanced_visualization(all_positions, all_alphas, env_config, model_based):
    """åˆ›å»ºé«˜çº§å¯è§†åŒ–"""
    num_agents = len(all_positions[0])
    num_steps = len(all_positions)
    
    # åˆ›å»ºå›¾å½¢
    fig, ((ax_main, ax_alpha), (ax_safety, ax_stats)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle(f'{"REAL MODEL" if model_based else "DEMONSTRATION"} - GCBF+ Dynamic Alpha Visualization', 
                 fontsize=16, fontweight='bold')
    
    # è®¾ç½®ä¸»é¢æ¿
    ax_main.set_xlim(-2.5, 2.5)
    ax_main.set_ylim(-1.2, 1.2)
    ax_main.set_aspect('equal')
    ax_main.set_title('Multi-Agent Navigation with Dynamic Alpha')
    ax_main.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶éšœç¢ç‰©ï¼ˆåŸºäºé…ç½®ï¼‰
    obstacles = []
    if env_config.get('env', {}).get('obstacles', {}).get('bottleneck', False):
        gap_width = env_config['env']['obstacles']['gap_width']
        obstacle_radius = env_config['env']['obstacles']['obstacle_radius']
        obstacle_spacing = env_config['env']['obstacles']['obstacle_spacing']
        
        wall_y = gap_width / 2 + obstacle_radius + 0.02
        for x in np.arange(-0.8, -0.12, obstacle_spacing):
            circle = Circle((x, wall_y), obstacle_radius, color='darkred', alpha=0.8)
            ax_main.add_patch(circle)
            obstacles.append([x, wall_y, obstacle_radius])
        for x in np.arange(0.12, 0.8, obstacle_spacing):
            circle = Circle((x, wall_y), obstacle_radius, color='darkred', alpha=0.8)
            ax_main.add_patch(circle)
            obstacles.append([x, wall_y, obstacle_radius])
        
        wall_y = -(gap_width / 2 + obstacle_radius + 0.02)
        for x in np.arange(-0.8, -0.12, obstacle_spacing):
            circle = Circle((x, wall_y), obstacle_radius, color='darkred', alpha=0.8)
            ax_main.add_patch(circle)
            obstacles.append([x, wall_y, obstacle_radius])
        for x in np.arange(0.12, 0.8, obstacle_spacing):
            circle = Circle((x, wall_y), obstacle_radius, color='darkred', alpha=0.8)
            ax_main.add_patch(circle)
            obstacles.append([x, wall_y, obstacle_radius])
    
    # è®¾ç½®Alphaå€¼é¢æ¿
    ax_alpha.set_title('Continuous Dynamic Alpha Values')
    ax_alpha.set_xlabel('Time Step')
    ax_alpha.set_ylabel('Alpha Value')
    
    # ç»˜åˆ¶è¿ç»­Alphaæ›²çº¿
    colors = plt.cm.tab10(np.linspace(0, 1, num_agents))
    for i in range(num_agents):
        alpha_series = [all_alphas[step][i] for step in range(num_steps)]
        ax_alpha.plot(range(num_steps), alpha_series, color=colors[i], 
                     linewidth=2, alpha=0.8, label=f'Agent {i+1}')
    
    ax_alpha.legend()
    ax_alpha.grid(True, alpha=0.3)
    
    # è®¾ç½®å®‰å…¨ç›‘æ§é¢æ¿
    ax_safety.set_title('Safety Metrics')
    ax_safety.set_xlabel('Time Step')
    ax_safety.set_ylabel('Distance (m)')
    
    # è®¡ç®—å®‰å…¨æŒ‡æ ‡
    min_distances = []
    avg_distances = []
    
    for step in range(num_steps):
        positions = all_positions[step]
        distances = []
        for i in range(num_agents):
            for j in range(i+1, num_agents):
                dist = np.linalg.norm(positions[i] - positions[j])
                distances.append(dist)
        
        if distances:
            min_distances.append(min(distances))
            avg_distances.append(np.mean(distances))
        else:
            min_distances.append(1.0)
            avg_distances.append(1.0)
    
    ax_safety.plot(range(num_steps), min_distances, 'b-', linewidth=2, label='Min Distance')
    ax_safety.plot(range(num_steps), avg_distances, 'g--', linewidth=1.5, label='Avg Distance')
    ax_safety.axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='Safety Threshold')
    ax_safety.legend()
    ax_safety.grid(True, alpha=0.3)
    
    # è®¾ç½®ç»Ÿè®¡é¢æ¿
    ax_stats.set_title('Model Information')
    ax_stats.axis('off')
    
    info_text = [
        f"Model Type: {'REAL TRAINED MODEL' if model_based else 'DEMONSTRATION'}",
        f"Agents: {num_agents}",
        f"Simulation Steps: {num_steps}",
        f"Alpha Range: {np.min(all_alphas):.3f} - {np.max(all_alphas):.3f}",
        f"Final Min Distance: {min_distances[-1]:.3f}m",
        f"Collision Events: {sum(1 for d in min_distances if d < 0.1)}"
    ]
    
    for i, text in enumerate(info_text):
        color = 'green' if model_based and i == 0 else 'red' if not model_based and i == 0 else 'black' 
        weight = 'bold' if i == 0 else 'normal'
        ax_stats.text(0.05, 0.9 - i*0.12, text, fontsize=11, color=color, 
                     fontweight=weight, transform=ax_stats.transAxes)
    
    # åˆå§‹åŒ–æ™ºèƒ½ä½“
    agent_circles = []
    agent_labels = []
    
    for i in range(num_agents):
        circle = Circle((0, 0), 0.04, color=colors[i], alpha=0.9, edgecolor='white', linewidth=2)
        ax_main.add_patch(circle)
        agent_circles.append(circle)
        
        label = ax_main.text(0, 0, f'A{i+1}', ha='center', va='center', 
                           fontsize=8, fontweight='bold', color='white')
        agent_labels.append(label)
    
    def animate(frame):
        if frame >= num_steps:
            return agent_circles + agent_labels
        
        positions = all_positions[frame]
        alphas = all_alphas[frame]
        
        # æ›´æ–°æ™ºèƒ½ä½“ä½ç½®
        for i, (pos, alpha) in enumerate(zip(positions, alphas)):
            agent_circles[i].center = pos
            agent_labels[i].set_position(pos)
            
            # æ ¹æ®Alphaå€¼è°ƒæ•´å¤§å°ï¼ˆå¯è§†åŒ–Alphaå½±å“ï¼‰
            radius = 0.04 + (alpha - 1.0) * 0.02  # Alphaè¶Šå¤§ï¼Œåœ†åœˆè¶Šå¤§
            agent_circles[i].set_radius(max(0.03, min(0.08, radius)))
        
        # æ›´æ–°æ ‡é¢˜
        ax_main.set_title(f'Step {frame}/{num_steps} - Alpha Range: '
                         f'{np.min(alphas):.3f}-{np.max(alphas):.3f}')
        
        return agent_circles + agent_labels
    
    # åˆ›å»ºåŠ¨ç”»
    anim = FuncAnimation(fig, animate, frames=num_steps, interval=100, blit=False, repeat=True)
    
    # ä¿å­˜
    output_path = "real_model_visualization.gif"
    writer = PillowWriter(fps=8)
    anim.save(output_path, writer=writer, dpi=100)
    plt.close()
    
    print(f"âœ… Real model visualization saved: {output_path}")

if __name__ == "__main__":
    create_real_model_visualization()