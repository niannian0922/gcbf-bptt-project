#!/usr/bin/env python3
"""
ç›´æ¥æ¨¡å‹å¯è¦–åŒ– - ä½¿ç”¨å·²çŸ¥æˆåŠŸçš„é…ç½®
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import os

from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.policy import BPTTPolicy


def create_exact_config():
    """
    ä½¿ç”¨å¾å¯¦éš›æ¨¡å‹æ¬Šé‡åˆ†æå¾—å‡ºçš„ç²¾ç¢ºé…ç½®
    """
    # æ ¹æ“šå¯¦éš›æ¨¡å‹æ¬Šé‡æ§‹å»ºé…ç½®
    config = {
        'perception': {
            'use_vision': False,
            'input_dim': 9,
            'output_dim': 256,
            'hidden_dims': [256, 256],  # ç¬¬ä¸€å±¤256â†’256ï¼Œç¬¬äºŒå±¤256â†’256
            'activation': 'relu'
        },
        'memory': {
            'input_dim': 256,  # å°‡åœ¨BPTTPolicyä¸­è‡ªå‹•è¨­ç½®
            'hidden_dim': 256,
            'num_layers': 1
        },
        'policy_head': {
            'input_dim': 256,  # å°‡åœ¨BPTTPolicyä¸­è‡ªå‹•è¨­ç½®
            'output_dim': 2,
            'hidden_dims': [256, 256, 2],  # æ ¹æ“šå¯¦éš›æ¬Šé‡ï¼š0â†’256, 2â†’256, 4â†’2
            'activation': 'relu',
            'predict_alpha': True,
            'alpha_hidden_dims': [128, 1]  # æ ¹æ“šå¯¦éš›æ¬Šé‡ï¼š0â†’128, 2â†’1
        }
    }
    
    return config


def load_model_direct(model_path, device='cpu'):
    """
    ç›´æ¥åŠ è¼‰æ¨¡å‹ï¼Œä½¿ç”¨ç²¾ç¢ºåŒ¹é…çš„é…ç½®
    """
    print(f"ğŸ¯ ç›´æ¥åŠ è¼‰æ¨¡å‹: {model_path}")
    
    # å‰µå»ºç²¾ç¢ºé…ç½®
    policy_config = create_exact_config()
    print(f"ğŸ“‹ ä½¿ç”¨é…ç½®: {policy_config}")
    
    # å‰µå»ºç¶²çµ¡
    policy_network = BPTTPolicy(policy_config)
    policy_network = policy_network.to(device)
    
    # åŠ è¼‰æ¬Šé‡
    try:
        state_dict = torch.load(model_path, map_location=device, weights_only=True)
        
        # ä½¿ç”¨strict=Falseå…è¨±éƒ¨åˆ†åŠ è¼‰
        missing_keys, unexpected_keys = policy_network.load_state_dict(state_dict, strict=False)
        
        print(f"âœ… æ¨¡å‹åŠ è¼‰æˆåŠŸ")
        if missing_keys:
            print(f"âš ï¸ ç¼ºå°‘çš„éµ: {missing_keys[:5]}{'...' if len(missing_keys) > 5 else ''}")
        if unexpected_keys:
            print(f"âš ï¸ é¡å¤–çš„éµ: {unexpected_keys[:5]}{'...' if len(unexpected_keys) > 5 else ''}")
        
        return policy_network
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
        return None


def test_model_inference(policy_network, env, device):
    """
    æ¸¬è©¦æ¨¡å‹æ¨ç†
    """
    print(f"ğŸ§ª æ¸¬è©¦æ¨¡å‹æ¨ç†")
    
    policy_network.eval()
    
    # é‡ç½®ç’°å¢ƒ
    state = env.reset()
    observations = env.get_observations(state).to(device)
    
    print(f"ğŸ“ è§€æ¸¬å½¢ç‹€: {observations.shape}")
    
    with torch.no_grad():
        try:
            # ç­–ç•¥æ¨ç†
            policy_output = policy_network(observations)
            
            if hasattr(policy_output, 'actions'):
                actions = policy_output.actions
                print(f"âœ… å‹•ä½œè¼¸å‡ºå½¢ç‹€: {actions.shape}")
                print(f"ğŸ“Š å‹•ä½œç¯„åœ: [{torch.min(actions):.6f}, {torch.max(actions):.6f}]")
                print(f"ğŸ“Š å‹•ä½œå¼·åº¦: {torch.norm(actions, dim=-1).mean():.6f}")
            else:
                print(f"âŒ ç„¡æ³•ç²å–å‹•ä½œè¼¸å‡º")
                return False
            
            if hasattr(policy_output, 'alphas'):
                alphas = policy_output.alphas
                print(f"âœ… Alphaè¼¸å‡ºå½¢ç‹€: {alphas.shape}")
                print(f"ğŸ“Š Alphaç¯„åœ: [{torch.min(alphas):.6f}, {torch.max(alphas):.6f}]")
            else:
                print(f"âš ï¸ æ²’æœ‰Alphaè¼¸å‡º")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False


def run_full_simulation(policy_network, env, device, num_steps=100):
    """
    é‹è¡Œå®Œæ•´ä»¿çœŸ
    """
    print(f"ğŸ¬ é‹è¡Œå®Œæ•´ä»¿çœŸ ({num_steps} æ­¥)")
    
    policy_network.eval()
    
    # åˆå§‹åŒ–
    state = env.reset()
    trajectory_positions = []
    trajectory_actions = []
    trajectory_alphas = []
    
    with torch.no_grad():
        for step in range(num_steps):
            # è¨˜éŒ„ä½ç½®
            current_positions = state.positions[0].cpu().numpy()
            trajectory_positions.append(current_positions.copy())
            
            # ç²å–è§€æ¸¬
            observations = env.get_observations(state).to(device)
            
            try:
                # ç­–ç•¥æ¨ç†
                policy_output = policy_network(observations)
                actions = policy_output.actions
                alphas = getattr(policy_output, 'alphas', torch.ones_like(actions[:, :, :1]) * 0.5)
                
                # è¨˜éŒ„æ•¸æ“š
                trajectory_actions.append(actions[0].cpu().numpy())
                trajectory_alphas.append(alphas[0].cpu().numpy())
                
                # ç’°å¢ƒæ­¥é€²
                step_result = env.step(state, actions, alphas)
                state = step_result.next_state
                
                # é€²åº¦å ±å‘Š
                if step % 25 == 0:
                    action_magnitude = torch.norm(actions, dim=-1).mean().item()
                    print(f"æ­¥é©Ÿ {step}: å‹•ä½œå¼·åº¦={action_magnitude:.6f}")
                
            except Exception as e:
                print(f"âŒ æ­¥é©Ÿ {step} å¤±æ•—: {e}")
                break
    
    print(f"âœ… ä»¿çœŸå®Œæˆï¼Œå…± {len(trajectory_positions)} æ­¥")
    
    return {
        'positions': trajectory_positions,
        'actions': trajectory_actions,
        'alphas': trajectory_alphas
    }


def create_professional_visualization(trajectory_data, output_path):
    """
    å‰µå»ºå°ˆæ¥­çš„å¯è¦–åŒ–
    """
    print(f"ğŸ¨ å‰µå»ºå°ˆæ¥­å¯è¦–åŒ–")
    
    positions = trajectory_data['positions']
    actions = trajectory_data['actions']
    alphas = trajectory_data['alphas']
    
    if not positions:
        print(f"âŒ æ²’æœ‰è»Œè·¡æ•¸æ“š")
        return False
    
    num_steps = len(positions)
    num_agents = len(positions[0])
    
    # å‰µå»ºåœ–å½¢
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle('ğŸ¯ æœ€çµ‚çµ±ä¸€å¯è¦–åŒ–çµæœ - 100%çœŸå¯¦æ¨¡å‹', fontsize=20, fontweight='bold')
    
    # ä¸»è»Œè·¡åœ–
    ax1.set_xlim(-3.5, 3.5)
    ax1.set_ylim(-2.5, 2.5)
    ax1.set_aspect('equal')
    ax1.set_title('ğŸš å¤šæ™ºèƒ½é«”å”ä½œè»Œè·¡', fontsize=16, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    # éšœç¤™ç‰©
    obstacles = [
        {'pos': [0.0, -0.8], 'radius': 0.4},
        {'pos': [0.0, 0.8], 'radius': 0.4}
    ]
    
    for obs in obstacles:
        circle = plt.Circle(obs['pos'], obs['radius'], color='red', alpha=0.8, 
                          label='éšœç¤™ç‰©' if obs == obstacles[0] else "")
        ax1.add_patch(circle)
    
    # å€åŸŸæ¨™è¨˜
    start_zone = plt.Rectangle((-3.0, -2.0), 1.0, 4.0, fill=False, 
                              edgecolor='green', linestyle='--', linewidth=3, 
                              alpha=0.9, label='èµ·å§‹å€åŸŸ')
    ax1.add_patch(start_zone)
    
    target_zone = plt.Rectangle((2.0, -2.0), 1.0, 4.0, fill=False, 
                               edgecolor='blue', linestyle='--', linewidth=3, 
                               alpha=0.9, label='ç›®æ¨™å€åŸŸ')
    ax1.add_patch(target_zone)
    
    # æ™ºèƒ½é«”é¡è‰²
    colors = ['#FF2D2D', '#2DFF2D', '#2D2DFF', '#FF8C2D', '#FF2D8C', '#2DFFFF'][:num_agents]
    
    # è»Œè·¡ç·šå’Œæ™ºèƒ½é«”
    trail_lines = []
    agent_dots = []
    
    for i in range(num_agents):
        line, = ax1.plot([], [], '-', color=colors[i], alpha=0.8, linewidth=3,
                        label=f'æ™ºèƒ½é«”{i+1}' if i < 3 else "")
        trail_lines.append(line)
        
        dot, = ax1.plot([], [], 'o', color=colors[i], markersize=16, 
                       markeredgecolor='black', markeredgewidth=2, zorder=5)
        agent_dots.append(dot)
    
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # å‹•ä½œåˆ†æåœ–
    ax2.set_title('ğŸ§  ç­–ç•¥ç¶²çµ¡è¼¸å‡ºåˆ†æ', fontsize=14, fontweight='bold')
    ax2.set_xlabel('æ™‚é–“æ­¥')
    ax2.set_ylabel('å‹•ä½œå¼·åº¦')
    ax2.grid(True, alpha=0.3)
    
    # Alphaå€¼ç›£æ§
    ax3.set_title('âš–ï¸ å‹•æ…‹Alphaå€¼ç›£æ§', fontsize=14, fontweight='bold')
    ax3.set_xlabel('æ™‚é–“æ­¥')
    ax3.set_ylabel('Alphaå€¼')
    ax3.grid(True, alpha=0.3)
    
    # é‹å‹•çµ±è¨ˆ
    ax4.set_title('ğŸ“Š é‹å‹•çµ±è¨ˆåˆ†æ', fontsize=14, fontweight='bold')
    ax4.set_xlabel('æ™‚é–“æ­¥')
    ax4.set_ylabel('å¹³å‡ä½ç§»')
    ax4.grid(True, alpha=0.3)
    
    # å‹•ç•«ä¿¡æ¯æ–‡æœ¬
    info_text = ax1.text(0.02, 0.98, '', transform=ax1.transAxes, 
                        verticalalignment='top', fontsize=12,
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))
    
    def animate(frame):
        if frame >= num_steps:
            return trail_lines + agent_dots + [info_text]
        
        current_pos = positions[frame]
        
        # æ›´æ–°è»Œè·¡å’Œæ™ºèƒ½é«”
        for i in range(num_agents):
            trail_x = [pos[i, 0] for pos in positions[:frame+1]]
            trail_y = [pos[i, 1] for pos in positions[:frame+1]]
            trail_lines[i].set_data(trail_x, trail_y)
            
            agent_dots[i].set_data([current_pos[i, 0]], [current_pos[i, 1]])
        
        # æ›´æ–°åˆ†æåœ–è¡¨
        if frame > 5:
            steps = list(range(frame+1))
            
            # å‹•ä½œåˆ†æ
            if frame < len(actions):
                action_magnitudes = []
                for step in range(frame+1):
                    if step < len(actions):
                        step_actions = actions[step]
                        avg_magnitude = np.mean([np.linalg.norm(a) for a in step_actions])
                        action_magnitudes.append(avg_magnitude)
                    else:
                        action_magnitudes.append(0)
                
                ax2.clear()
                ax2.plot(steps, action_magnitudes, 'purple', linewidth=3, label='å¹³å‡å‹•ä½œå¼·åº¦')
                ax2.fill_between(steps, action_magnitudes, alpha=0.3, color='purple')
                ax2.set_title(f'ğŸ§  ç­–ç•¥ç¶²çµ¡è¼¸å‡ºåˆ†æ (æ­¥æ•¸: {frame})')
                ax2.set_xlabel('æ™‚é–“æ­¥')
                ax2.set_ylabel('å‹•ä½œå¼·åº¦')
                ax2.legend()
                ax2.grid(True, alpha=0.3)
            
            # Alphaå€¼åˆ†æ
            if frame < len(alphas):
                alpha_values = []
                for step in range(frame+1):
                    if step < len(alphas):
                        avg_alpha = np.mean(alphas[step])
                        alpha_values.append(avg_alpha)
                    else:
                        alpha_values.append(0.5)
                
                ax3.clear()
                ax3.plot(steps, alpha_values, 'orange', linewidth=3, label='å¹³å‡Alphaå€¼')
                ax3.fill_between(steps, alpha_values, alpha=0.3, color='orange')
                ax3.set_title(f'âš–ï¸ å‹•æ…‹Alphaå€¼ç›£æ§ (æ­¥æ•¸: {frame})')
                ax3.set_xlabel('æ™‚é–“æ­¥')
                ax3.set_ylabel('Alphaå€¼')
                ax3.legend()
                ax3.grid(True, alpha=0.3)
            
            # é‹å‹•çµ±è¨ˆ
            displacements = []
            initial_pos = positions[0]
            for step in range(frame+1):
                if step < len(positions):
                    current_pos = positions[step]
                    avg_displacement = np.mean([
                        np.linalg.norm(current_pos[i] - initial_pos[i]) 
                        for i in range(num_agents)
                    ])
                    displacements.append(avg_displacement)
                else:
                    displacements.append(0)
            
            ax4.clear()
            ax4.plot(steps, displacements, 'green', linewidth=3, label='å¹³å‡ä½ç§»')
            ax4.fill_between(steps, displacements, alpha=0.3, color='green')
            ax4.set_title(f'ğŸ“Š é‹å‹•çµ±è¨ˆåˆ†æ (æ­¥æ•¸: {frame})')
            ax4.set_xlabel('æ™‚é–“æ­¥')
            ax4.set_ylabel('å¹³å‡ä½ç§»')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        
        # è¨ˆç®—ç•¶å‰çµ±è¨ˆ
        if frame > 0:
            initial_pos = positions[0]
            current_pos = positions[frame]
            total_displacement = np.mean([
                np.linalg.norm(current_pos[i] - initial_pos[i]) 
                for i in range(num_agents)
            ])
            
            action_magnitude = 0
            if frame < len(actions):
                action_magnitude = np.mean([np.linalg.norm(a) for a in actions[frame]])
        else:
            total_displacement = 0
            action_magnitude = 0
        
        info_text.set_text(
            f'æ­¥é©Ÿ: {frame}\n'
            f'æ™ºèƒ½é«”æ•¸: {num_agents}\n'
            f'å¹³å‡ä½ç§»: {total_displacement:.4f}\n'
            f'å‹•ä½œå¼·åº¦: {action_magnitude:.6f}'
        )
        
        return trail_lines + agent_dots + [info_text]
    
    # å‰µå»ºå‹•ç•«
    anim = FuncAnimation(fig, animate, frames=num_steps, interval=120, blit=False, repeat=True)
    
    # ä¿å­˜
    try:
        print(f"ğŸ’¾ ä¿å­˜å°ˆæ¥­å¯è¦–åŒ–: {output_path}")
        if output_path.endswith('.mp4'):
            anim.save(output_path, writer='ffmpeg', fps=8, dpi=150)
        else:
            anim.save(output_path, writer='pillow', fps=6, dpi=150)
        
        file_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"âœ… ä¿å­˜æˆåŠŸ: {file_size:.2f}MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±æ•—: {e}")
        return False
    finally:
        plt.close()


def main():
    """
    ä¸»å‡½æ•¸
    """
    print(f"ğŸ¯ ç›´æ¥æ¨¡å‹å¯è¦–åŒ–ç³»çµ±")
    print(f"=" * 70)
    
    # è¨­ç½®
    model_dir = 'logs/full_collaboration_training'
    device = torch.device('cpu')
    
    # æ‰¾æœ€æ–°æ¨¡å‹
    models_dir = os.path.join(model_dir, 'models')
    steps = [int(d) for d in os.listdir(models_dir) if d.isdigit()]
    latest_step = max(steps)
    
    policy_path = os.path.join(model_dir, 'models', str(latest_step), 'policy.pt')
    print(f"ğŸ“ æ¨¡å‹è·¯å¾‘: {policy_path}")
    
    # å‰µå»ºç’°å¢ƒ
    env_config = {
        'area_size': 3.0,
        'car_radius': 0.15,
        'comm_radius': 1.0,
        'dt': 0.05,
        'mass': 0.1,
        'max_force': 1.0,
        'max_steps': 100,
        'num_agents': 6,
        'obstacles': {
            'enabled': True,
            'bottleneck': True,
            'positions': [[0.0, -0.8], [0.0, 0.8]],
            'radii': [0.4, 0.4]
        }
    }
    
    env = DoubleIntegratorEnv(env_config)
    env = env.to(device)
    
    print(f"ğŸŒ ç’°å¢ƒ: {env.observation_shape} â†’ {env.action_shape}")
    
    # åŠ è¼‰æ¨¡å‹
    policy_network = load_model_direct(policy_path, device)
    if policy_network is None:
        print(f"âŒ ç³»çµ±å¤±æ•—")
        return
    
    # æ¸¬è©¦æ¨ç†
    if not test_model_inference(policy_network, env, device):
        print(f"âŒ æ¨ç†æ¸¬è©¦å¤±æ•—")
        return
    
    # é‹è¡Œä»¿çœŸ
    trajectory_data = run_full_simulation(policy_network, env, device, 120)
    
    # å‰µå»ºå¯è¦–åŒ–
    output_path = 'DIRECT_FINAL_COLLABORATION_RESULT.mp4'
    success = create_professional_visualization(trajectory_data, output_path)
    
    if success:
        print(f"\nğŸ‰ ç›´æ¥å¯è¦–åŒ–æˆåŠŸå®Œæˆ!")
        print(f"ğŸ“ çµæœæ–‡ä»¶: {output_path}")
        print(f"âœ… é€™æ˜¯æ‚¨çœŸå¯¦è¨“ç·´æ¨¡å‹çš„è¡¨ç¾")
        
        # é‹å‹•åˆ†æ
        positions = trajectory_data['positions']
        if positions:
            initial_pos = positions[0]
            final_pos = positions[-1]
            total_displacement = np.mean([
                np.linalg.norm(final_pos[i] - initial_pos[i]) 
                for i in range(len(initial_pos))
            ])
            print(f"ğŸ“Š å¹³å‡ç¸½ä½ç§»: {total_displacement:.4f}")
            
            if total_displacement < 0.01:
                print(f"âš ï¸ æ™ºèƒ½é«”åŸºæœ¬éœæ­¢ï¼Œå¯èƒ½å­˜åœ¨è¨“ç·´å•é¡Œ")
            else:
                print(f"âœ… æ™ºèƒ½é«”æ­£å¸¸é‹å‹•")
    else:
        print(f"\nâŒ å¯è¦–åŒ–å¤±æ•—")


if __name__ == '__main__':
    main()
 
"""
ç›´æ¥æ¨¡å‹å¯è¦–åŒ– - ä½¿ç”¨å·²çŸ¥æˆåŠŸçš„é…ç½®
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import os

from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.policy import BPTTPolicy


def create_exact_config():
    """
    ä½¿ç”¨å¾å¯¦éš›æ¨¡å‹æ¬Šé‡åˆ†æå¾—å‡ºçš„ç²¾ç¢ºé…ç½®
    """
    # æ ¹æ“šå¯¦éš›æ¨¡å‹æ¬Šé‡æ§‹å»ºé…ç½®
    config = {
        'perception': {
            'use_vision': False,
            'input_dim': 9,
            'output_dim': 256,
            'hidden_dims': [256, 256],  # ç¬¬ä¸€å±¤256â†’256ï¼Œç¬¬äºŒå±¤256â†’256
            'activation': 'relu'
        },
        'memory': {
            'input_dim': 256,  # å°‡åœ¨BPTTPolicyä¸­è‡ªå‹•è¨­ç½®
            'hidden_dim': 256,
            'num_layers': 1
        },
        'policy_head': {
            'input_dim': 256,  # å°‡åœ¨BPTTPolicyä¸­è‡ªå‹•è¨­ç½®
            'output_dim': 2,
            'hidden_dims': [256, 256, 2],  # æ ¹æ“šå¯¦éš›æ¬Šé‡ï¼š0â†’256, 2â†’256, 4â†’2
            'activation': 'relu',
            'predict_alpha': True,
            'alpha_hidden_dims': [128, 1]  # æ ¹æ“šå¯¦éš›æ¬Šé‡ï¼š0â†’128, 2â†’1
        }
    }
    
    return config


def load_model_direct(model_path, device='cpu'):
    """
    ç›´æ¥åŠ è¼‰æ¨¡å‹ï¼Œä½¿ç”¨ç²¾ç¢ºåŒ¹é…çš„é…ç½®
    """
    print(f"ğŸ¯ ç›´æ¥åŠ è¼‰æ¨¡å‹: {model_path}")
    
    # å‰µå»ºç²¾ç¢ºé…ç½®
    policy_config = create_exact_config()
    print(f"ğŸ“‹ ä½¿ç”¨é…ç½®: {policy_config}")
    
    # å‰µå»ºç¶²çµ¡
    policy_network = BPTTPolicy(policy_config)
    policy_network = policy_network.to(device)
    
    # åŠ è¼‰æ¬Šé‡
    try:
        state_dict = torch.load(model_path, map_location=device, weights_only=True)
        
        # ä½¿ç”¨strict=Falseå…è¨±éƒ¨åˆ†åŠ è¼‰
        missing_keys, unexpected_keys = policy_network.load_state_dict(state_dict, strict=False)
        
        print(f"âœ… æ¨¡å‹åŠ è¼‰æˆåŠŸ")
        if missing_keys:
            print(f"âš ï¸ ç¼ºå°‘çš„éµ: {missing_keys[:5]}{'...' if len(missing_keys) > 5 else ''}")
        if unexpected_keys:
            print(f"âš ï¸ é¡å¤–çš„éµ: {unexpected_keys[:5]}{'...' if len(unexpected_keys) > 5 else ''}")
        
        return policy_network
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
        return None


def test_model_inference(policy_network, env, device):
    """
    æ¸¬è©¦æ¨¡å‹æ¨ç†
    """
    print(f"ğŸ§ª æ¸¬è©¦æ¨¡å‹æ¨ç†")
    
    policy_network.eval()
    
    # é‡ç½®ç’°å¢ƒ
    state = env.reset()
    observations = env.get_observations(state).to(device)
    
    print(f"ğŸ“ è§€æ¸¬å½¢ç‹€: {observations.shape}")
    
    with torch.no_grad():
        try:
            # ç­–ç•¥æ¨ç†
            policy_output = policy_network(observations)
            
            if hasattr(policy_output, 'actions'):
                actions = policy_output.actions
                print(f"âœ… å‹•ä½œè¼¸å‡ºå½¢ç‹€: {actions.shape}")
                print(f"ğŸ“Š å‹•ä½œç¯„åœ: [{torch.min(actions):.6f}, {torch.max(actions):.6f}]")
                print(f"ğŸ“Š å‹•ä½œå¼·åº¦: {torch.norm(actions, dim=-1).mean():.6f}")
            else:
                print(f"âŒ ç„¡æ³•ç²å–å‹•ä½œè¼¸å‡º")
                return False
            
            if hasattr(policy_output, 'alphas'):
                alphas = policy_output.alphas
                print(f"âœ… Alphaè¼¸å‡ºå½¢ç‹€: {alphas.shape}")
                print(f"ğŸ“Š Alphaç¯„åœ: [{torch.min(alphas):.6f}, {torch.max(alphas):.6f}]")
            else:
                print(f"âš ï¸ æ²’æœ‰Alphaè¼¸å‡º")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False


def run_full_simulation(policy_network, env, device, num_steps=100):
    """
    é‹è¡Œå®Œæ•´ä»¿çœŸ
    """
    print(f"ğŸ¬ é‹è¡Œå®Œæ•´ä»¿çœŸ ({num_steps} æ­¥)")
    
    policy_network.eval()
    
    # åˆå§‹åŒ–
    state = env.reset()
    trajectory_positions = []
    trajectory_actions = []
    trajectory_alphas = []
    
    with torch.no_grad():
        for step in range(num_steps):
            # è¨˜éŒ„ä½ç½®
            current_positions = state.positions[0].cpu().numpy()
            trajectory_positions.append(current_positions.copy())
            
            # ç²å–è§€æ¸¬
            observations = env.get_observations(state).to(device)
            
            try:
                # ç­–ç•¥æ¨ç†
                policy_output = policy_network(observations)
                actions = policy_output.actions
                alphas = getattr(policy_output, 'alphas', torch.ones_like(actions[:, :, :1]) * 0.5)
                
                # è¨˜éŒ„æ•¸æ“š
                trajectory_actions.append(actions[0].cpu().numpy())
                trajectory_alphas.append(alphas[0].cpu().numpy())
                
                # ç’°å¢ƒæ­¥é€²
                step_result = env.step(state, actions, alphas)
                state = step_result.next_state
                
                # é€²åº¦å ±å‘Š
                if step % 25 == 0:
                    action_magnitude = torch.norm(actions, dim=-1).mean().item()
                    print(f"æ­¥é©Ÿ {step}: å‹•ä½œå¼·åº¦={action_magnitude:.6f}")
                
            except Exception as e:
                print(f"âŒ æ­¥é©Ÿ {step} å¤±æ•—: {e}")
                break
    
    print(f"âœ… ä»¿çœŸå®Œæˆï¼Œå…± {len(trajectory_positions)} æ­¥")
    
    return {
        'positions': trajectory_positions,
        'actions': trajectory_actions,
        'alphas': trajectory_alphas
    }


def create_professional_visualization(trajectory_data, output_path):
    """
    å‰µå»ºå°ˆæ¥­çš„å¯è¦–åŒ–
    """
    print(f"ğŸ¨ å‰µå»ºå°ˆæ¥­å¯è¦–åŒ–")
    
    positions = trajectory_data['positions']
    actions = trajectory_data['actions']
    alphas = trajectory_data['alphas']
    
    if not positions:
        print(f"âŒ æ²’æœ‰è»Œè·¡æ•¸æ“š")
        return False
    
    num_steps = len(positions)
    num_agents = len(positions[0])
    
    # å‰µå»ºåœ–å½¢
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle('ğŸ¯ æœ€çµ‚çµ±ä¸€å¯è¦–åŒ–çµæœ - 100%çœŸå¯¦æ¨¡å‹', fontsize=20, fontweight='bold')
    
    # ä¸»è»Œè·¡åœ–
    ax1.set_xlim(-3.5, 3.5)
    ax1.set_ylim(-2.5, 2.5)
    ax1.set_aspect('equal')
    ax1.set_title('ğŸš å¤šæ™ºèƒ½é«”å”ä½œè»Œè·¡', fontsize=16, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    # éšœç¤™ç‰©
    obstacles = [
        {'pos': [0.0, -0.8], 'radius': 0.4},
        {'pos': [0.0, 0.8], 'radius': 0.4}
    ]
    
    for obs in obstacles:
        circle = plt.Circle(obs['pos'], obs['radius'], color='red', alpha=0.8, 
                          label='éšœç¤™ç‰©' if obs == obstacles[0] else "")
        ax1.add_patch(circle)
    
    # å€åŸŸæ¨™è¨˜
    start_zone = plt.Rectangle((-3.0, -2.0), 1.0, 4.0, fill=False, 
                              edgecolor='green', linestyle='--', linewidth=3, 
                              alpha=0.9, label='èµ·å§‹å€åŸŸ')
    ax1.add_patch(start_zone)
    
    target_zone = plt.Rectangle((2.0, -2.0), 1.0, 4.0, fill=False, 
                               edgecolor='blue', linestyle='--', linewidth=3, 
                               alpha=0.9, label='ç›®æ¨™å€åŸŸ')
    ax1.add_patch(target_zone)
    
    # æ™ºèƒ½é«”é¡è‰²
    colors = ['#FF2D2D', '#2DFF2D', '#2D2DFF', '#FF8C2D', '#FF2D8C', '#2DFFFF'][:num_agents]
    
    # è»Œè·¡ç·šå’Œæ™ºèƒ½é«”
    trail_lines = []
    agent_dots = []
    
    for i in range(num_agents):
        line, = ax1.plot([], [], '-', color=colors[i], alpha=0.8, linewidth=3,
                        label=f'æ™ºèƒ½é«”{i+1}' if i < 3 else "")
        trail_lines.append(line)
        
        dot, = ax1.plot([], [], 'o', color=colors[i], markersize=16, 
                       markeredgecolor='black', markeredgewidth=2, zorder=5)
        agent_dots.append(dot)
    
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # å‹•ä½œåˆ†æåœ–
    ax2.set_title('ğŸ§  ç­–ç•¥ç¶²çµ¡è¼¸å‡ºåˆ†æ', fontsize=14, fontweight='bold')
    ax2.set_xlabel('æ™‚é–“æ­¥')
    ax2.set_ylabel('å‹•ä½œå¼·åº¦')
    ax2.grid(True, alpha=0.3)
    
    # Alphaå€¼ç›£æ§
    ax3.set_title('âš–ï¸ å‹•æ…‹Alphaå€¼ç›£æ§', fontsize=14, fontweight='bold')
    ax3.set_xlabel('æ™‚é–“æ­¥')
    ax3.set_ylabel('Alphaå€¼')
    ax3.grid(True, alpha=0.3)
    
    # é‹å‹•çµ±è¨ˆ
    ax4.set_title('ğŸ“Š é‹å‹•çµ±è¨ˆåˆ†æ', fontsize=14, fontweight='bold')
    ax4.set_xlabel('æ™‚é–“æ­¥')
    ax4.set_ylabel('å¹³å‡ä½ç§»')
    ax4.grid(True, alpha=0.3)
    
    # å‹•ç•«ä¿¡æ¯æ–‡æœ¬
    info_text = ax1.text(0.02, 0.98, '', transform=ax1.transAxes, 
                        verticalalignment='top', fontsize=12,
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))
    
    def animate(frame):
        if frame >= num_steps:
            return trail_lines + agent_dots + [info_text]
        
        current_pos = positions[frame]
        
        # æ›´æ–°è»Œè·¡å’Œæ™ºèƒ½é«”
        for i in range(num_agents):
            trail_x = [pos[i, 0] for pos in positions[:frame+1]]
            trail_y = [pos[i, 1] for pos in positions[:frame+1]]
            trail_lines[i].set_data(trail_x, trail_y)
            
            agent_dots[i].set_data([current_pos[i, 0]], [current_pos[i, 1]])
        
        # æ›´æ–°åˆ†æåœ–è¡¨
        if frame > 5:
            steps = list(range(frame+1))
            
            # å‹•ä½œåˆ†æ
            if frame < len(actions):
                action_magnitudes = []
                for step in range(frame+1):
                    if step < len(actions):
                        step_actions = actions[step]
                        avg_magnitude = np.mean([np.linalg.norm(a) for a in step_actions])
                        action_magnitudes.append(avg_magnitude)
                    else:
                        action_magnitudes.append(0)
                
                ax2.clear()
                ax2.plot(steps, action_magnitudes, 'purple', linewidth=3, label='å¹³å‡å‹•ä½œå¼·åº¦')
                ax2.fill_between(steps, action_magnitudes, alpha=0.3, color='purple')
                ax2.set_title(f'ğŸ§  ç­–ç•¥ç¶²çµ¡è¼¸å‡ºåˆ†æ (æ­¥æ•¸: {frame})')
                ax2.set_xlabel('æ™‚é–“æ­¥')
                ax2.set_ylabel('å‹•ä½œå¼·åº¦')
                ax2.legend()
                ax2.grid(True, alpha=0.3)
            
            # Alphaå€¼åˆ†æ
            if frame < len(alphas):
                alpha_values = []
                for step in range(frame+1):
                    if step < len(alphas):
                        avg_alpha = np.mean(alphas[step])
                        alpha_values.append(avg_alpha)
                    else:
                        alpha_values.append(0.5)
                
                ax3.clear()
                ax3.plot(steps, alpha_values, 'orange', linewidth=3, label='å¹³å‡Alphaå€¼')
                ax3.fill_between(steps, alpha_values, alpha=0.3, color='orange')
                ax3.set_title(f'âš–ï¸ å‹•æ…‹Alphaå€¼ç›£æ§ (æ­¥æ•¸: {frame})')
                ax3.set_xlabel('æ™‚é–“æ­¥')
                ax3.set_ylabel('Alphaå€¼')
                ax3.legend()
                ax3.grid(True, alpha=0.3)
            
            # é‹å‹•çµ±è¨ˆ
            displacements = []
            initial_pos = positions[0]
            for step in range(frame+1):
                if step < len(positions):
                    current_pos = positions[step]
                    avg_displacement = np.mean([
                        np.linalg.norm(current_pos[i] - initial_pos[i]) 
                        for i in range(num_agents)
                    ])
                    displacements.append(avg_displacement)
                else:
                    displacements.append(0)
            
            ax4.clear()
            ax4.plot(steps, displacements, 'green', linewidth=3, label='å¹³å‡ä½ç§»')
            ax4.fill_between(steps, displacements, alpha=0.3, color='green')
            ax4.set_title(f'ğŸ“Š é‹å‹•çµ±è¨ˆåˆ†æ (æ­¥æ•¸: {frame})')
            ax4.set_xlabel('æ™‚é–“æ­¥')
            ax4.set_ylabel('å¹³å‡ä½ç§»')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        
        # è¨ˆç®—ç•¶å‰çµ±è¨ˆ
        if frame > 0:
            initial_pos = positions[0]
            current_pos = positions[frame]
            total_displacement = np.mean([
                np.linalg.norm(current_pos[i] - initial_pos[i]) 
                for i in range(num_agents)
            ])
            
            action_magnitude = 0
            if frame < len(actions):
                action_magnitude = np.mean([np.linalg.norm(a) for a in actions[frame]])
        else:
            total_displacement = 0
            action_magnitude = 0
        
        info_text.set_text(
            f'æ­¥é©Ÿ: {frame}\n'
            f'æ™ºèƒ½é«”æ•¸: {num_agents}\n'
            f'å¹³å‡ä½ç§»: {total_displacement:.4f}\n'
            f'å‹•ä½œå¼·åº¦: {action_magnitude:.6f}'
        )
        
        return trail_lines + agent_dots + [info_text]
    
    # å‰µå»ºå‹•ç•«
    anim = FuncAnimation(fig, animate, frames=num_steps, interval=120, blit=False, repeat=True)
    
    # ä¿å­˜
    try:
        print(f"ğŸ’¾ ä¿å­˜å°ˆæ¥­å¯è¦–åŒ–: {output_path}")
        if output_path.endswith('.mp4'):
            anim.save(output_path, writer='ffmpeg', fps=8, dpi=150)
        else:
            anim.save(output_path, writer='pillow', fps=6, dpi=150)
        
        file_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"âœ… ä¿å­˜æˆåŠŸ: {file_size:.2f}MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±æ•—: {e}")
        return False
    finally:
        plt.close()


def main():
    """
    ä¸»å‡½æ•¸
    """
    print(f"ğŸ¯ ç›´æ¥æ¨¡å‹å¯è¦–åŒ–ç³»çµ±")
    print(f"=" * 70)
    
    # è¨­ç½®
    model_dir = 'logs/full_collaboration_training'
    device = torch.device('cpu')
    
    # æ‰¾æœ€æ–°æ¨¡å‹
    models_dir = os.path.join(model_dir, 'models')
    steps = [int(d) for d in os.listdir(models_dir) if d.isdigit()]
    latest_step = max(steps)
    
    policy_path = os.path.join(model_dir, 'models', str(latest_step), 'policy.pt')
    print(f"ğŸ“ æ¨¡å‹è·¯å¾‘: {policy_path}")
    
    # å‰µå»ºç’°å¢ƒ
    env_config = {
        'area_size': 3.0,
        'car_radius': 0.15,
        'comm_radius': 1.0,
        'dt': 0.05,
        'mass': 0.1,
        'max_force': 1.0,
        'max_steps': 100,
        'num_agents': 6,
        'obstacles': {
            'enabled': True,
            'bottleneck': True,
            'positions': [[0.0, -0.8], [0.0, 0.8]],
            'radii': [0.4, 0.4]
        }
    }
    
    env = DoubleIntegratorEnv(env_config)
    env = env.to(device)
    
    print(f"ğŸŒ ç’°å¢ƒ: {env.observation_shape} â†’ {env.action_shape}")
    
    # åŠ è¼‰æ¨¡å‹
    policy_network = load_model_direct(policy_path, device)
    if policy_network is None:
        print(f"âŒ ç³»çµ±å¤±æ•—")
        return
    
    # æ¸¬è©¦æ¨ç†
    if not test_model_inference(policy_network, env, device):
        print(f"âŒ æ¨ç†æ¸¬è©¦å¤±æ•—")
        return
    
    # é‹è¡Œä»¿çœŸ
    trajectory_data = run_full_simulation(policy_network, env, device, 120)
    
    # å‰µå»ºå¯è¦–åŒ–
    output_path = 'DIRECT_FINAL_COLLABORATION_RESULT.mp4'
    success = create_professional_visualization(trajectory_data, output_path)
    
    if success:
        print(f"\nğŸ‰ ç›´æ¥å¯è¦–åŒ–æˆåŠŸå®Œæˆ!")
        print(f"ğŸ“ çµæœæ–‡ä»¶: {output_path}")
        print(f"âœ… é€™æ˜¯æ‚¨çœŸå¯¦è¨“ç·´æ¨¡å‹çš„è¡¨ç¾")
        
        # é‹å‹•åˆ†æ
        positions = trajectory_data['positions']
        if positions:
            initial_pos = positions[0]
            final_pos = positions[-1]
            total_displacement = np.mean([
                np.linalg.norm(final_pos[i] - initial_pos[i]) 
                for i in range(len(initial_pos))
            ])
            print(f"ğŸ“Š å¹³å‡ç¸½ä½ç§»: {total_displacement:.4f}")
            
            if total_displacement < 0.01:
                print(f"âš ï¸ æ™ºèƒ½é«”åŸºæœ¬éœæ­¢ï¼Œå¯èƒ½å­˜åœ¨è¨“ç·´å•é¡Œ")
            else:
                print(f"âœ… æ™ºèƒ½é«”æ­£å¸¸é‹å‹•")
    else:
        print(f"\nâŒ å¯è¦–åŒ–å¤±æ•—")


if __name__ == '__main__':
    main()
 
 
 
 