#!/usr/bin/env python3
"""
üöÄ Âä®ÊÄÅÂçè‰Ωú‰øÆÂ§ç
Á°Æ‰øùÊô∫ËÉΩ‰ΩìÊúâÊòéÊòæËøêÂä®ÔºåÂêåÊó∂‰øùÊåÅÂú®ÁîªÈù¢ÂÜÖ
Âπ≥Ë°°Âä®ÊÄÅÊÄßÂíåËæπÁïåÊéßÂà∂
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import yaml
import os
from datetime import datetime

def create_dynamic_collaboration():
    """ÂàõÂª∫ÁúüÊ≠£Âä®ÊÄÅÁöÑÂçè‰ΩúÂèØËßÜÂåñ"""
    print("üöÄ Âä®ÊÄÅÂçè‰Ωú‰øÆÂ§çÁ≥ªÁªü")
    print("=" * 50)
    print("üéØ ÁõÆÊ†á: Á°Æ‰øùÊô∫ËÉΩ‰ΩìÊúâÊòéÊòæËøêÂä®")
    print("üöÅ ÂÜÖÂÆπ: Âä®ÊÄÅÊó†‰∫∫Êú∫ÁºñÈòüÂçè‰Ωú")
    print("‚öñÔ∏è Âπ≥Ë°°: Âä®ÊÄÅÊÄß + ËæπÁïåÊéßÂà∂")
    print("=" * 50)
    
    # Âπ≥Ë°°ÁöÑÈÖçÁΩÆ
    config = {
        'env': {
            'name': 'DoubleIntegrator',
            'num_agents': 6,
            'area_size': 4.0,
            'dt': 0.03,  # ÈÄÇ‰∏≠ÁöÑÊó∂Èó¥Ê≠•Èïø
            'mass': 0.2,  # ÈÄÇ‰∏≠ÁöÑË¥®Èáè
            'agent_radius': 0.15,
            'comm_radius': 1.0,
            'max_force': 1.0,  # ÊÅ¢Â§çÂêàÁêÜÁöÑÊúÄÂ§ßÂäõ
            'max_steps': 150,
            'social_radius': 0.4,
            'obstacles': {
                'enabled': True,
                'count': 2,
                'positions': [[0, 0.7], [0, -0.7]],
                'radii': [0.3, 0.3]
            }
        }
    }
    
    try:
        # ÂØºÂÖ•
        from gcbfplus.env import DoubleIntegratorEnv
        from gcbfplus.env.multi_agent_env import MultiAgentState
        from gcbfplus.policy.bptt_policy import create_policy_from_config
        
        print("‚úÖ ÂØºÂÖ•ÊàêÂäü")
        
        # ÂàõÂª∫ÁéØÂ¢É
        device = torch.device('cpu')
        env = DoubleIntegratorEnv(config['env'])
        env = env.to(device)
        
        print(f"‚úÖ ÁéØÂ¢ÉÂàõÂª∫ÊàêÂäü")
        print(f"   üìä ËßÇÊµãÁª¥Â∫¶: {env.observation_shape}")
        print(f"   ‚ö° ÊúÄÂ§ßÂäõ: {env.max_force}")
        print(f"   ‚è∞ Êó∂Èó¥Ê≠•Èïø: {env.dt}")
        
        # ÂàõÂª∫ÁúüÂÆûÁöÑÂçè‰ΩúÂú∫ÊôØ
        initial_state = create_realistic_scenario(device, config['env']['num_agents'])
        
        print("‚úÖ Áé∞ÂÆûÂçè‰ΩúÂú∫ÊôØÂàõÂª∫ÊàêÂäü")
        
        # ËøêË°åÂº∫ÂåñÂä®ÊÄÅÊ®°Êãü
        trajectory_data = run_dynamic_simulation(env, initial_state, config)
        
        print(f"‚úÖ Âä®ÊÄÅÊ®°ÊãüÂÆåÊàê: {len(trajectory_data['positions'])} Ê≠•")
        
        # ÁîüÊàêÁúüÊ≠£Âä®ÊÄÅÁöÑÂèØËßÜÂåñ
        output_file = create_dynamic_visualization(trajectory_data, config)
        
        print(f"üéâ Âä®ÊÄÅÂèØËßÜÂåñÂÆåÊàê: {output_file}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ‰øÆÂ§çÂ§±Ë¥•: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_realistic_scenario(device, num_agents):
    """ÂàõÂª∫Áé∞ÂÆûÁöÑÂçè‰ΩúÂú∫ÊôØ"""
    from gcbfplus.env.multi_agent_env import MultiAgentState
    
    # Áé∞ÂÆûÁöÑËµ∑ÂßãÂíåÁõÆÊ†á‰ΩçÁΩÆ
    positions = torch.zeros(1, num_agents, 2, device=device)
    velocities = torch.zeros(1, num_agents, 2, device=device)
    goals = torch.zeros(1, num_agents, 2, device=device)
    
    # Â∑¶‰æßÁºñÈòüÔºåÈúÄË¶ÅÊòéÊòæÁßªÂä®Âà∞Âè≥‰æß
    start_x = -2.2
    target_x = 2.2
    
    print(f"   üìç ËÆæÁΩÆÁºñÈòüÂú∫ÊôØ:")
    print(f"      Ëµ∑ÂßãÂå∫Âüü: x = {start_x}")
    print(f"      ÁõÆÊ†áÂå∫Âüü: x = {target_x}")
    print(f"      ÁßªÂä®Ë∑ùÁ¶ª: {abs(target_x - start_x):.1f}")
    
    for i in range(num_agents):
        # VÂ≠óÂΩ¢ÁºñÈòü
        if i == 0:
            # È¢ÜÈòü
            start_pos = [start_x, 0]
            target_pos = [target_x, 0]
        else:
            # ÂÉöÊú∫
            side = 1 if i % 2 == 1 else -1
            rank = (i + 1) // 2
            start_pos = [start_x - rank * 0.15, side * rank * 0.35]
            target_pos = [target_x + rank * 0.15, side * rank * 0.35]
        
        positions[0, i] = torch.tensor(start_pos, device=device)
        goals[0, i] = torch.tensor(target_pos, device=device)
        
        print(f"      Êó†‰∫∫Êú∫{i+1}: {start_pos} ‚Üí {target_pos}")
    
    return MultiAgentState(
        positions=positions,
        velocities=velocities,
        goals=goals,
        batch_size=1
    )

def run_dynamic_simulation(env, initial_state, config):
    """ËøêË°åÂä®ÊÄÅÊ®°Êãü - Á°Æ‰øùÊúâÊòéÊòæËøêÂä®"""
    num_steps = 120
    
    trajectory_data = {
        'positions': [],
        'velocities': [],
        'actions': [],
        'goal_distances': [],
        'step_movements': [],
        'collaboration_scores': [],
        'config': config
    }
    
    current_state = initial_state
    previous_positions = initial_state.positions[0].cpu().numpy()
    
    print(f"   üé¨ ÂºÄÂßãÂä®ÊÄÅÊ®°Êãü...")
    
    for step in range(num_steps):
        positions = current_state.positions[0].cpu().numpy()
        velocities = current_state.velocities[0].cpu().numpy()
        goal_positions = current_state.goals[0].cpu().numpy()
        
        trajectory_data['positions'].append(positions.copy())
        trajectory_data['velocities'].append(velocities.copy())
        
        # ËÆ°ÁÆóÊô∫ËÉΩÂä®‰ΩúÔºàÁ°Æ‰øùÊúâËøêÂä®Ôºâ
        actions = compute_intelligent_actions(positions, velocities, goal_positions, 
                                            config['env']['obstacles'], 
                                            config['env']['social_radius'])
        
        # ËΩ¨Êç¢‰∏∫tensor
        actions_tensor = torch.tensor(actions, device=current_state.positions.device).unsqueeze(0)
        alphas = torch.ones(1, len(positions), device=current_state.positions.device) * 0.5
        
        trajectory_data['actions'].append(actions.copy())
        
        # ËÆ°ÁÆóÊ≠•ËøõËøêÂä®Ë∑ùÁ¶ª
        if step > 0:
            step_movement = np.linalg.norm(positions - previous_positions, axis=1)
            trajectory_data['step_movements'].append(step_movement)
        else:
            trajectory_data['step_movements'].append(np.zeros(len(positions)))
        
        previous_positions = positions.copy()
        
        # ÁõÆÊ†áË∑ùÁ¶ª
        goal_distances = [np.linalg.norm(positions[i] - goal_positions[i]) 
                         for i in range(len(positions))]
        trajectory_data['goal_distances'].append(goal_distances)
        
        # Âçè‰ΩúÂæóÂàÜ
        social_distances = []
        for i in range(len(positions)):
            for j in range(i+1, len(positions)):
                dist = np.linalg.norm(positions[i] - positions[j])
                social_distances.append(dist)
        
        if social_distances:
            min_dist = min(social_distances)
            social_radius = config['env']['social_radius']
            collab_score = min(min_dist / social_radius, 1.0)
        else:
            collab_score = 1.0
        
        trajectory_data['collaboration_scores'].append(collab_score)
        
        # ÁéØÂ¢ÉÊ≠•Ëøõ
        try:
            step_result = env.step(current_state, actions_tensor, alphas)
            current_state = step_result.next_state
            
            # ËΩØËæπÁïå - Â¶ÇÊûúË∂ÖÂá∫ËåÉÂõ¥ÔºåËΩªÊé®ÂõûÊù•
            current_positions = current_state.positions.clone()
            boundary = 2.8
            
            for i in range(current_positions.shape[1]):
                for j in range(2):  # x, y
                    if current_positions[0, i, j] > boundary:
                        current_positions[0, i, j] = boundary - 0.1
                    elif current_positions[0, i, j] < -boundary:
                        current_positions[0, i, j] = -boundary + 0.1
            
            current_state = MultiAgentState(
                positions=current_positions,
                velocities=current_state.velocities,
                goals=current_state.goals,
                batch_size=current_state.batch_size
            )
            
            # ÊòæÁ§∫ËøõÂ∫¶
            if step % 20 == 0:
                avg_goal_dist = np.mean(goal_distances)
                avg_movement = np.mean(trajectory_data['step_movements'][-1])
                action_mag = np.mean([np.linalg.norm(a) for a in actions])
                print(f"   Ê≠•È™§ {step:3d}: ËøêÂä®={avg_movement:.4f}, Âä®‰Ωú={action_mag:.3f}, ÁõÆÊ†áË∑ùÁ¶ª={avg_goal_dist:.3f}")
            
            # Ê£ÄÊü•ÂÆåÊàê
            if np.mean(goal_distances) < 0.4:
                print(f"   üéØ ÁºñÈòüÂà∞ËææÁõÆÊ†á! (Ê≠•Êï∞: {step+1})")
                break
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è ÁéØÂ¢ÉÊ≠•ËøõÂ§±Ë¥•: {e}")
            break
    
    # ËøêÂä®ÂàÜÊûê
    total_movements = [sum(movements) for movements in zip(*trajectory_data['step_movements'])]
    avg_total_movement = np.mean(total_movements)
    
    print(f"   üìä ËøêÂä®ÂàÜÊûê:")
    print(f"      Âπ≥ÂùáÊÄªËøêÂä®Ë∑ùÁ¶ª: {avg_total_movement:.3f}")
    print(f"      ËøêÂä®Áä∂ÊÄÅ: {'‚úÖ Âä®ÊÄÅ' if avg_total_movement > 1.0 else '‚ùå ÈùôÊÄÅ'}")
    
    return trajectory_data

def compute_intelligent_actions(positions, velocities, goals, obstacles, social_radius):
    """ËÆ°ÁÆóÊô∫ËÉΩÂä®‰Ωú - Á°Æ‰øùÊúâËøêÂä®"""
    actions = np.zeros_like(positions)
    
    for i in range(len(positions)):
        # 1. ÁõÆÊ†áÂê∏ÂºïÂäõ
        goal_direction = goals[i] - positions[i]
        goal_distance = np.linalg.norm(goal_direction)
        
        if goal_distance > 0.1:
            goal_force = (goal_direction / goal_distance) * min(goal_distance * 1.2, 0.8)
        else:
            goal_force = np.zeros(2)
        
        # 2. ÈöúÁ¢çÁâ©ÊéíÊñ•Âäõ
        obstacle_force = np.zeros(2)
        for obs_pos, obs_radius in zip(obstacles['positions'], obstacles['radii']):
            obs_vec = positions[i] - np.array(obs_pos)
            obs_distance = np.linalg.norm(obs_vec)
            
            if obs_distance < obs_radius + 0.8:  # ÂΩ±ÂìçËåÉÂõ¥
                if obs_distance > 0.01:
                    repulsion_strength = (obs_radius + 0.8 - obs_distance) / 0.8
                    obstacle_force += (obs_vec / obs_distance) * repulsion_strength * 0.6
        
        # 3. Á§æ‰∫§ÂäõÔºàÁª¥ÊåÅÁºñÈòüÔºâ
        social_force = np.zeros(2)
        for j in range(len(positions)):
            if i != j:
                diff = positions[i] - positions[j]
                distance = np.linalg.norm(diff)
                
                if distance < social_radius and distance > 0.01:
                    # ËΩªÂæÆÊéíÊñ•‰ª•Áª¥ÊåÅË∑ùÁ¶ª
                    social_force += (diff / distance) * 0.2
                elif distance > social_radius * 1.5 and distance > 0.01:
                    # ËΩªÂæÆÂê∏Âºï‰ª•‰øùÊåÅÁºñÈòü
                    social_force -= (diff / distance) * 0.1
        
        # 4. ÈÄüÂ∫¶ÈòªÂ∞ºÔºàÈò≤Ê≠¢ËøáÂø´Ôºâ
        velocity_damping = -velocities[i] * 0.1
        
        # ÂêàÊàêÂä®‰Ωú
        total_force = goal_force + obstacle_force + social_force + velocity_damping
        
        # ÈôêÂà∂Âä®‰ΩúÂ§ßÂ∞è‰ΩÜ‰øùËØÅÊúâÊïà
        force_magnitude = np.linalg.norm(total_force)
        if force_magnitude > 1.0:
            total_force = total_force / force_magnitude * 1.0
        elif force_magnitude < 0.05 and goal_distance > 0.2:
            # Á°Æ‰øùÊúÄÂ∞èÂä®‰Ωú‰ª•‰øùËØÅËøêÂä®
            total_force = (goal_direction / max(goal_distance, 0.01)) * 0.05
        
        actions[i] = total_force
    
    return actions

def create_dynamic_visualization(trajectory_data, config):
    """ÂàõÂª∫ÁúüÊ≠£Âä®ÊÄÅÁöÑÂèØËßÜÂåñ"""
    positions_history = trajectory_data['positions']
    if not positions_history:
        return None
    
    num_agents = len(positions_history[0])
    num_steps = len(positions_history)
    obstacles = config['env']['obstacles']
    
    # ÂàÜÊûêËøêÂä®ËåÉÂõ¥
    all_positions = np.concatenate(positions_history, axis=0)
    min_x, max_x = all_positions[:, 0].min() - 0.3, all_positions[:, 0].max() + 0.3
    min_y, max_y = all_positions[:, 1].min() - 0.3, all_positions[:, 1].max() + 0.3
    
    # Á°Æ‰øùÂêàÁêÜÁöÑÊòæÁ§∫ËåÉÂõ¥
    center_x = (min_x + max_x) / 2
    center_y = (min_y + max_y) / 2
    range_x = max(max_x - min_x, 3.0)
    range_y = max(max_y - min_y, 2.0)
    
    display_min_x = center_x - range_x / 2
    display_max_x = center_x + range_x / 2
    display_min_y = center_y - range_y / 2
    display_max_y = center_y + range_y / 2
    
    print(f"   üìè Âä®ÊÄÅÊòæÁ§∫ËåÉÂõ¥:")
    print(f"      X: [{display_min_x:.2f}, {display_max_x:.2f}]")
    print(f"      Y: [{display_min_y:.2f}, {display_max_y:.2f}]")
    
    # ÂàõÂª∫ÂõæÂΩ¢
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14))
    fig.suptitle('üöÄ Âä®ÊÄÅÊó†‰∫∫Êú∫ÁºñÈòüÂçè‰Ωú - Á°Æ‰øùËøêÂä®Áâà', fontsize=16, fontweight='bold')
    
    # ‰∏ªËΩ®ËøπÂõæ
    ax1.set_xlim(display_min_x, display_max_x)
    ax1.set_ylim(display_min_y, display_max_y)
    ax1.set_aspect('equal')
    ax1.set_title('üöÅ Âä®ÊÄÅÊó†‰∫∫Êú∫ÁºñÈòüÂçè‰Ωú')
    ax1.grid(True, alpha=0.3)
    
    # ÁªòÂà∂ÈöúÁ¢çÁâ©
    for i, (pos, radius) in enumerate(zip(obstacles['positions'], obstacles['radii'])):
        circle = plt.Circle(pos, radius, color='red', alpha=0.8, 
                          label='ÈöúÁ¢çÁâ©' if i == 0 else "")
        ax1.add_patch(circle)
    
    # ÁªòÂà∂Ëµ∑ÂßãÂíåÁõÆÊ†áÂå∫Âüü
    start_zone = plt.Rectangle((-2.5, -1.5), 0.6, 3.0, fill=False, 
                              edgecolor='green', linestyle='--', linewidth=2, 
                              alpha=0.7, label='Ëµ∑ÂßãÂå∫Âüü')
    ax1.add_patch(start_zone)
    
    target_zone = plt.Rectangle((1.9, -1.5), 0.6, 3.0, fill=False, 
                               edgecolor='blue', linestyle='--', linewidth=2, 
                               alpha=0.7, label='ÁõÆÊ†áÂå∫Âüü')
    ax1.add_patch(target_zone)
    
    # Êó†‰∫∫Êú∫È¢úËâ≤
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', '#FF9FF3']
    
    # ÂàùÂßãÂåñÂä®ÁîªÂÖÉÁ¥†
    trail_lines = []
    drone_dots = []
    velocity_arrows = []
    
    for i in range(num_agents):
        # ËΩ®ËøπÁ∫ø
        line, = ax1.plot([], [], '-', color=colors[i], alpha=0.8, linewidth=3)
        trail_lines.append(line)
        
        # Êó†‰∫∫Êú∫Ôºà‰∏âËßíÂΩ¢Ë°®Á§∫ÊñπÂêëÔºâ
        drone, = ax1.plot([], [], '^', color=colors[i], markersize=16, 
                         markeredgecolor='black', markeredgewidth=2, 
                         label=f'Êó†‰∫∫Êú∫{i+1}' if i < 3 else "")
        drone_dots.append(drone)
        
        # ÈÄüÂ∫¶ÁÆ≠Â§¥
        arrow = ax1.annotate('', xy=(0, 0), xytext=(0, 0),
                           arrowprops=dict(arrowstyle='->', color=colors[i], 
                                         lw=2, alpha=0.7))
        velocity_arrows.append(arrow)
    
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # ËøêÂä®Âº∫Â∫¶Âõæ
    ax2.set_title('üèÉ ËøêÂä®Âº∫Â∫¶ÁõëÊéß')
    ax2.set_xlabel('Êó∂Èó¥Ê≠•')
    ax2.set_ylabel('ËøêÂä®Ë∑ùÁ¶ª')
    ax2.grid(True, alpha=0.3)
    
    # ÁºñÈòüÂçè‰ΩúÂõæ
    ax3.set_title('ü§ù ÁºñÈòüÂçè‰ΩúÁä∂ÊÄÅ')
    ax3.set_xlabel('Êó∂Èó¥Ê≠•')
    ax3.set_ylabel('Âçè‰ΩúÂæóÂàÜ')
    ax3.set_ylim(0, 1)
    ax3.grid(True, alpha=0.3)
    
    # ‰ªªÂä°ËøõÂ∫¶Âõæ
    ax4.set_title('üéØ ‰ªªÂä°ÂÆåÊàêËøõÂ∫¶')
    ax4.set_xlabel('Êó∂Èó¥Ê≠•')
    ax4.set_ylabel('Âπ≥ÂùáÁõÆÊ†áË∑ùÁ¶ª')
    ax4.grid(True, alpha=0.3)
    
    def animate(frame):
        if frame >= num_steps:
            return trail_lines + drone_dots
        
        current_positions = positions_history[frame]
        current_velocities = trajectory_data['velocities'][frame] if frame < len(trajectory_data['velocities']) else np.zeros_like(current_positions)
        
        # Êõ¥Êñ∞Êó†‰∫∫Êú∫„ÄÅËΩ®ËøπÂíåÈÄüÂ∫¶ÁÆ≠Â§¥
        for i, (line, drone, arrow) in enumerate(zip(trail_lines, drone_dots, velocity_arrows)):
            if i < len(current_positions):
                # ËΩ®Ëøπ
                trail_x = [pos[i, 0] for pos in positions_history[:frame+1]]
                trail_y = [pos[i, 1] for pos in positions_history[:frame+1]]
                line.set_data(trail_x, trail_y)
                
                # Êó†‰∫∫Êú∫‰ΩçÁΩÆ
                drone.set_data([current_positions[i, 0]], [current_positions[i, 1]])
                
                # ÈÄüÂ∫¶ÁÆ≠Â§¥
                vel_scale = 0.5
                if frame < len(trajectory_data['velocities']):
                    vel = current_velocities[i] * vel_scale
                    arrow.set_position((current_positions[i, 0], current_positions[i, 1]))
                    arrow.xy = (current_positions[i, 0] + vel[0], 
                              current_positions[i, 1] + vel[1])
        
        # Êõ¥Êñ∞ÁõëÊéßÂõæË°®
        if frame > 0:
            steps = list(range(frame+1))
            
            # ËøêÂä®Âº∫Â∫¶
            if len(trajectory_data['step_movements']) > frame:
                avg_movements = [np.mean(movements) for movements in trajectory_data['step_movements'][:frame+1]]
                ax2.clear()
                ax2.plot(steps, avg_movements, 'orange', linewidth=3, label='Âπ≥ÂùáËøêÂä®Ë∑ùÁ¶ª')
                ax2.fill_between(steps, avg_movements, alpha=0.3, color='orange')
                ax2.set_title(f'üèÉ ËøêÂä®Âº∫Â∫¶ (Ê≠•Êï∞: {frame})')
                ax2.set_xlabel('Êó∂Èó¥Ê≠•')
                ax2.set_ylabel('ËøêÂä®Ë∑ùÁ¶ª')
                ax2.legend()
                ax2.grid(True, alpha=0.3)
            
            # ÁºñÈòüÂçè‰Ωú
            if len(trajectory_data['collaboration_scores']) > frame:
                collab_scores = trajectory_data['collaboration_scores'][:frame+1]
                ax3.clear()
                ax3.plot(steps, collab_scores, 'purple', linewidth=3, label='Âçè‰ΩúÂæóÂàÜ')
                ax3.fill_between(steps, collab_scores, alpha=0.3, color='purple')
                ax3.set_title(f'ü§ù ÁºñÈòüÂçè‰Ωú (Ê≠•Êï∞: {frame})')
                ax3.set_xlabel('Êó∂Èó¥Ê≠•')
                ax3.set_ylabel('Âçè‰ΩúÂæóÂàÜ')
                ax3.set_ylim(0, 1)
                ax3.legend()
                ax3.grid(True, alpha=0.3)
            
            # ‰ªªÂä°ËøõÂ∫¶
            if len(trajectory_data['goal_distances']) > frame:
                avg_goal_dists = [np.mean(dists) for dists in trajectory_data['goal_distances'][:frame+1]]
                ax4.clear()
                ax4.plot(steps, avg_goal_dists, 'green', linewidth=3, label='Âπ≥ÂùáÁõÆÊ†áË∑ùÁ¶ª')
                ax4.fill_between(steps, avg_goal_dists, alpha=0.3, color='green')
                ax4.set_title(f'üéØ ‰ªªÂä°ËøõÂ∫¶ (Ê≠•Êï∞: {frame})')
                ax4.set_xlabel('Êó∂Èó¥Ê≠•')
                ax4.set_ylabel('Âπ≥ÂùáÁõÆÊ†áË∑ùÁ¶ª')
                ax4.legend()
                ax4.grid(True, alpha=0.3)
        
        return trail_lines + drone_dots
    
    # ÂàõÂª∫Âä®Áîª
    anim = FuncAnimation(fig, animate, frames=num_steps, 
                        interval=120, blit=False, repeat=True)
    
    # ‰øùÂ≠ò
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"DYNAMIC_COLLABORATION_{timestamp}.gif"
    
    try:
        print(f"üíæ ‰øùÂ≠òÂä®ÊÄÅÂèØËßÜÂåñ...")
        anim.save(output_path, writer='pillow', fps=8, dpi=130)
        print(f"‚úÖ ‰øùÂ≠òÊàêÂäü: {output_path}")
        
        # ËÆ°ÁÆóÊñá‰ª∂Â§ßÂ∞è
        file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
        print(f"üìÅ Êñá‰ª∂Â§ßÂ∞è: {file_size:.1f}MB")
        
        # ËøêÂä®ÂàÜÊûê
        if trajectory_data['step_movements']:
            total_movements = [sum(movements) for movements in zip(*trajectory_data['step_movements'])]
            avg_total_movement = np.mean(total_movements)
            max_movement = max([max(movements) for movements in trajectory_data['step_movements']])
            
            print(f"üìä ËøêÂä®È™åËØÅ:")
            print(f"   Âπ≥ÂùáÊÄªËøêÂä®: {avg_total_movement:.3f}")
            print(f"   ÊúÄÂ§ßÂçïÊ≠•ËøêÂä®: {max_movement:.3f}")
            print(f"   Âä®ÊÄÅÁä∂ÊÄÅ: {'‚úÖ Âä®ÊÄÅËøêÂä®' if avg_total_movement > 0.5 else '‚ùå ÈùôÊÄÅ'}")
            print(f"   Êñá‰ª∂Ë¥®Èáè: {'‚úÖ Ê≠£Â∏∏Â§ßÂ∞è' if file_size > 1.0 else '‚ö†Ô∏è ÂèØËÉΩÈùôÊÄÅ'}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è ‰øùÂ≠òÂ§±Ë¥•: {e}")
        # ‰øùÂ≠òÈùôÊÄÅÂõæ
        static_path = f"DYNAMIC_STATIC_{timestamp}.png"
        plt.tight_layout()
        plt.savefig(static_path, dpi=150, bbox_inches='tight')
        print(f"‚úÖ ÈùôÊÄÅÂõæ‰øùÂ≠ò: {static_path}")
        output_path = static_path
    
    plt.close()
    return output_path

if __name__ == "__main__":
    print("üöÄ Âä®ÊÄÅÂçè‰Ωú‰øÆÂ§çÁ≥ªÁªü")
    print("Ëß£ÂÜ≥ÈùôÊÄÅÈóÆÈ¢òÔºåÁ°Æ‰øùÊô∫ËÉΩ‰ΩìÊúâÊòéÊòæËøêÂä®")
    print("Âπ≥Ë°°Âä®ÊÄÅÊÄßÂíåËæπÁïåÊéßÂà∂")
    print("=" * 70)
    
    success = create_dynamic_collaboration()
    
    if success:
        print(f"\nüéâ Âä®ÊÄÅÂçè‰Ωú‰øÆÂ§çÊàêÂäü!")
        print(f"üöÅ ÁîüÊàê‰∫ÜÁúüÊ≠£Âä®ÊÄÅÁöÑÊó†‰∫∫Êú∫ÁºñÈòüÂçè‰ΩúÂèØËßÜÂåñ")
        print(f"üèÉ Êô∫ËÉΩ‰ΩìÁé∞Âú®ÊúâÊòéÊòæÁöÑËøêÂä®")
        print(f"üìÅ Ê£ÄÊü•Êñ∞ÁöÑÂä®ÊÄÅGIFÊñá‰ª∂")
    else:
        print(f"\n‚ùå ‰øÆÂ§çÂ§±Ë¥•ÔºåÈúÄË¶ÅËøõ‰∏ÄÊ≠•Ë∞ÉËØï")
 
 
 
 