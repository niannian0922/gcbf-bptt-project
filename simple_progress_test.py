#!/usr/bin/env python3
"""
ç°¡å–®æ¸¬è©¦é€²åº¦çå‹µåŠŸèƒ½
"""

import torch
import yaml
import numpy as np
from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.policy import BPTTPolicy, create_policy_from_config
from gcbfplus.trainer.bptt_trainer import BPTTTrainer

def test_progress_reward():
    """æ¸¬è©¦é€²åº¦çå‹µåŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦é€²åº¦çå‹µåŠŸèƒ½")
    
    # ç°¡å–®é…ç½®
    config = {
        'env': {
            'num_agents': 4,
            'area_size': 2.0,
            'car_radius': 0.1,
            'comm_radius': 0.5,
            'dt': 0.05,
            'obstacles': {'enabled': False}
        },
        'loss_weights': {
            'goal_weight': 1.0,
            'safety_weight': 2.0,
            'control_weight': 0.1,
            'progress_weight': 0.2  # æ¸¬è©¦é€²åº¦æ¬Šé‡
        },
        'training': {
            'training_steps': 10,  # å¾ˆçŸ­çš„æ¸¬è©¦
            'horizon_length': 5,
            'learning_rate': 0.01
        },
        'networks': {
            'policy': {}
        }
    }
    
    # å‰µå»ºç’°å¢ƒ
    env = DoubleIntegratorEnv(config['env'])
    print(f"âœ… ç’°å¢ƒå‰µå»ºæˆåŠŸ: {env.observation_shape}")
    
    # å‰µå»ºç°¡å–®ç­–ç•¥ç¶²çµ¡
    obs_shape = env.observation_shape
    action_shape = env.action_shape
    
    policy_config = {
        'perception': {
            'use_vision': False,
            'input_dim': obs_shape[-1],
            'output_dim': 64,
            'hidden_dims': [64, 64],
            'activation': 'relu'
        },
        'memory': {
            'hidden_dim': 64,
            'num_layers': 1
        },
        'policy_head': {
            'output_dim': action_shape[-1],
            'hidden_dims': [64],
            'activation': 'relu',
            'predict_alpha': True
        }
    }
    
    policy_network = create_policy_from_config(policy_config)
    print(f"âœ… ç­–ç•¥ç¶²çµ¡å‰µå»ºæˆåŠŸ")
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer_config = {
        'horizon_length': config['training']['horizon_length'],
        'learning_rate': config['training']['learning_rate'],
        'training_steps': config['training']['training_steps'],
        'goal_weight': config['loss_weights']['goal_weight'],
        'safety_weight': config['loss_weights']['safety_weight'],
        'control_weight': config['loss_weights']['control_weight'],
        'progress_weight': config['loss_weights']['progress_weight'],  # é—œéµï¼šé€²åº¦æ¬Šé‡
        'jerk_weight': 0.02,
        'alpha_reg_weight': 0.01,
        'cbf_alpha': 1.0,
        'device': 'cpu',
        'log_interval': 5,
        'save_interval': 10
    }
    
    trainer = BPTTTrainer(
        env=env,
        policy_network=policy_network,
        cbf_network=None,
        config=trainer_config
    )
    
    print(f"âœ… è¨“ç·´å™¨å‰µå»ºæˆåŠŸ")
    print(f"ğŸ“Š é€²åº¦æ¬Šé‡: {trainer.progress_weight}")
    
    # æ¸¬è©¦é€²åº¦æå¤±è¨ˆç®—
    print("\nğŸ” æ¸¬è©¦é€²åº¦æå¤±è¨ˆç®—...")
    
    # å‰µå»ºæ¨¡æ“¬è»Œè¿¹
    state1 = env.reset()
    state2 = env.reset()
    
    # æ‰‹å‹•ä¿®æ”¹ä½ç½®ä»¥æ¨¡æ“¬é€²åº¦
    state2.positions = state1.positions + torch.tensor([[[0.1, 0.1]]] * config['env']['num_agents'])
    
    trajectory_states = [state1, state2]
    
    try:
        progress_loss = trainer._calculate_progress_loss(trajectory_states)
        print(f"âœ… é€²åº¦æå¤±è¨ˆç®—æˆåŠŸ: {progress_loss.item():.6f}")
        
        if progress_loss.requires_grad:
            print("âœ… é€²åº¦æå¤±æ”¯æŒæ¢¯åº¦è¨ˆç®—")
        else:
            print("âš ï¸ é€²åº¦æå¤±ä¸æ”¯æŒæ¢¯åº¦è¨ˆç®—")
            
    except Exception as e:
        print(f"âŒ é€²åº¦æå¤±è¨ˆç®—å¤±æ•—: {e}")
        return False
    
    # é‹è¡Œä¸€å€‹è¶…çŸ­çš„è¨“ç·´æ­¥é©Ÿ
    print("\nğŸƒ é‹è¡ŒçŸ­è¨“ç·´æ¸¬è©¦...")
    
    try:
        # è¨­ç½®ä¿å­˜ç›®éŒ„
        import os
        save_dir = "logs/progress_test"
        if not os.path.exists(save_dir):
            os.makedirs(save_dir, exist_ok=True)
        
        trainer.model_dir = os.path.join(save_dir, "models")
        if not os.path.exists(trainer.model_dir):
            os.makedirs(trainer.model_dir, exist_ok=True)
        
        # åªé‹è¡Œå¹¾æ­¥è¨“ç·´ä¾†æ¸¬è©¦
        original_steps = trainer.training_steps
        trainer.training_steps = 3
        
        trainer.train()
        
        print("âœ… çŸ­è¨“ç·´æ¸¬è©¦æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ çŸ­è¨“ç·´æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ æˆ°ç•¥æ€§æ”¹é€² - é€²åº¦çå‹µæ¸¬è©¦")
    print("=" * 50)
    
    success = test_progress_reward()
    
    if success:
        print("\nğŸ‰ é€²åº¦çå‹µåŠŸèƒ½æ¸¬è©¦æˆåŠŸï¼")
        print("âœ… åŸºæ–¼æ½›åŠ›çš„çå‹µå¡‘å½¢å·²å¯¦æ–½")
        print("âœ… ç³»çµ±æº–å‚™å¥½é€²è¡Œèª²ç¨‹å­¸ç¿’")
        print("\nğŸš€ ä¸‹ä¸€æ­¥ï¼šé‹è¡Œå®Œæ•´çš„èª²ç¨‹å­¸ç¿’å¯¦é©—")
        print("   python run_curriculum_experiments.bat")
    else:
        print("\nâŒ é€²åº¦çå‹µåŠŸèƒ½æ¸¬è©¦å¤±æ•—")
        print("è«‹æª¢æŸ¥å¯¦æ–½ä¸¦ä¿®å¾©å•é¡Œ")
    
    return success

if __name__ == "__main__":
    main()
 
"""
ç°¡å–®æ¸¬è©¦é€²åº¦çå‹µåŠŸèƒ½
"""

import torch
import yaml
import numpy as np
from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.policy import BPTTPolicy, create_policy_from_config
from gcbfplus.trainer.bptt_trainer import BPTTTrainer

def test_progress_reward():
    """æ¸¬è©¦é€²åº¦çå‹µåŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦é€²åº¦çå‹µåŠŸèƒ½")
    
    # ç°¡å–®é…ç½®
    config = {
        'env': {
            'num_agents': 4,
            'area_size': 2.0,
            'car_radius': 0.1,
            'comm_radius': 0.5,
            'dt': 0.05,
            'obstacles': {'enabled': False}
        },
        'loss_weights': {
            'goal_weight': 1.0,
            'safety_weight': 2.0,
            'control_weight': 0.1,
            'progress_weight': 0.2  # æ¸¬è©¦é€²åº¦æ¬Šé‡
        },
        'training': {
            'training_steps': 10,  # å¾ˆçŸ­çš„æ¸¬è©¦
            'horizon_length': 5,
            'learning_rate': 0.01
        },
        'networks': {
            'policy': {}
        }
    }
    
    # å‰µå»ºç’°å¢ƒ
    env = DoubleIntegratorEnv(config['env'])
    print(f"âœ… ç’°å¢ƒå‰µå»ºæˆåŠŸ: {env.observation_shape}")
    
    # å‰µå»ºç°¡å–®ç­–ç•¥ç¶²çµ¡
    obs_shape = env.observation_shape
    action_shape = env.action_shape
    
    policy_config = {
        'perception': {
            'use_vision': False,
            'input_dim': obs_shape[-1],
            'output_dim': 64,
            'hidden_dims': [64, 64],
            'activation': 'relu'
        },
        'memory': {
            'hidden_dim': 64,
            'num_layers': 1
        },
        'policy_head': {
            'output_dim': action_shape[-1],
            'hidden_dims': [64],
            'activation': 'relu',
            'predict_alpha': True
        }
    }
    
    policy_network = create_policy_from_config(policy_config)
    print(f"âœ… ç­–ç•¥ç¶²çµ¡å‰µå»ºæˆåŠŸ")
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer_config = {
        'horizon_length': config['training']['horizon_length'],
        'learning_rate': config['training']['learning_rate'],
        'training_steps': config['training']['training_steps'],
        'goal_weight': config['loss_weights']['goal_weight'],
        'safety_weight': config['loss_weights']['safety_weight'],
        'control_weight': config['loss_weights']['control_weight'],
        'progress_weight': config['loss_weights']['progress_weight'],  # é—œéµï¼šé€²åº¦æ¬Šé‡
        'jerk_weight': 0.02,
        'alpha_reg_weight': 0.01,
        'cbf_alpha': 1.0,
        'device': 'cpu',
        'log_interval': 5,
        'save_interval': 10
    }
    
    trainer = BPTTTrainer(
        env=env,
        policy_network=policy_network,
        cbf_network=None,
        config=trainer_config
    )
    
    print(f"âœ… è¨“ç·´å™¨å‰µå»ºæˆåŠŸ")
    print(f"ğŸ“Š é€²åº¦æ¬Šé‡: {trainer.progress_weight}")
    
    # æ¸¬è©¦é€²åº¦æå¤±è¨ˆç®—
    print("\nğŸ” æ¸¬è©¦é€²åº¦æå¤±è¨ˆç®—...")
    
    # å‰µå»ºæ¨¡æ“¬è»Œè¿¹
    state1 = env.reset()
    state2 = env.reset()
    
    # æ‰‹å‹•ä¿®æ”¹ä½ç½®ä»¥æ¨¡æ“¬é€²åº¦
    state2.positions = state1.positions + torch.tensor([[[0.1, 0.1]]] * config['env']['num_agents'])
    
    trajectory_states = [state1, state2]
    
    try:
        progress_loss = trainer._calculate_progress_loss(trajectory_states)
        print(f"âœ… é€²åº¦æå¤±è¨ˆç®—æˆåŠŸ: {progress_loss.item():.6f}")
        
        if progress_loss.requires_grad:
            print("âœ… é€²åº¦æå¤±æ”¯æŒæ¢¯åº¦è¨ˆç®—")
        else:
            print("âš ï¸ é€²åº¦æå¤±ä¸æ”¯æŒæ¢¯åº¦è¨ˆç®—")
            
    except Exception as e:
        print(f"âŒ é€²åº¦æå¤±è¨ˆç®—å¤±æ•—: {e}")
        return False
    
    # é‹è¡Œä¸€å€‹è¶…çŸ­çš„è¨“ç·´æ­¥é©Ÿ
    print("\nğŸƒ é‹è¡ŒçŸ­è¨“ç·´æ¸¬è©¦...")
    
    try:
        # è¨­ç½®ä¿å­˜ç›®éŒ„
        import os
        save_dir = "logs/progress_test"
        if not os.path.exists(save_dir):
            os.makedirs(save_dir, exist_ok=True)
        
        trainer.model_dir = os.path.join(save_dir, "models")
        if not os.path.exists(trainer.model_dir):
            os.makedirs(trainer.model_dir, exist_ok=True)
        
        # åªé‹è¡Œå¹¾æ­¥è¨“ç·´ä¾†æ¸¬è©¦
        original_steps = trainer.training_steps
        trainer.training_steps = 3
        
        trainer.train()
        
        print("âœ… çŸ­è¨“ç·´æ¸¬è©¦æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ çŸ­è¨“ç·´æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ æˆ°ç•¥æ€§æ”¹é€² - é€²åº¦çå‹µæ¸¬è©¦")
    print("=" * 50)
    
    success = test_progress_reward()
    
    if success:
        print("\nğŸ‰ é€²åº¦çå‹µåŠŸèƒ½æ¸¬è©¦æˆåŠŸï¼")
        print("âœ… åŸºæ–¼æ½›åŠ›çš„çå‹µå¡‘å½¢å·²å¯¦æ–½")
        print("âœ… ç³»çµ±æº–å‚™å¥½é€²è¡Œèª²ç¨‹å­¸ç¿’")
        print("\nğŸš€ ä¸‹ä¸€æ­¥ï¼šé‹è¡Œå®Œæ•´çš„èª²ç¨‹å­¸ç¿’å¯¦é©—")
        print("   python run_curriculum_experiments.bat")
    else:
        print("\nâŒ é€²åº¦çå‹µåŠŸèƒ½æ¸¬è©¦å¤±æ•—")
        print("è«‹æª¢æŸ¥å¯¦æ–½ä¸¦ä¿®å¾©å•é¡Œ")
    
    return success

if __name__ == "__main__":
    main()
 
 
 
 