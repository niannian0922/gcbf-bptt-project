# Policy networks with dynamic alpha prediction for adaptive safety margins

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Tuple, Optional, List, Union, Any


class PerceptionModule(nn.Module):
    """
    æ„ŸçŸ¥æ¨¡å—ï¼Œç”¨äºŽå¤„ç†ä¼ æ„Ÿå™¨è¾“å…¥ã€‚
    
    è¯¥æ¨¡å—å¯é…ç½®ä¸ºå¤„ç†ä¸åŒç±»åž‹çš„è¾“å…¥ï¼š
    - åŸºäºŽè§†è§‰çš„è§‚æµ‹ï¼ˆä½¿ç”¨CNNå¤„ç†æ·±åº¦å›¾åƒï¼‰
    - å¯†é›†å‘é‡è§‚æµ‹ï¼ˆä½¿ç”¨MLPå¤„ç†çŠ¶æ€å‘é‡ï¼‰
    """
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–æ„ŸçŸ¥æ¨¡å—ã€‚
        
        å‚æ•°:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®å€¼ï¼š
                è§†è§‰æ¨¡å¼:
                - 'vision_enabled': æ˜¯å¦å¯ç”¨åŸºäºŽè§†è§‰çš„å¤„ç†
                - 'input_channels': è¾“å…¥é€šé“æ•°ï¼ˆæ·±åº¦å›¾é»˜è®¤ä¸º1ï¼‰
                - 'conv_channels': CNNé€šé“å¤§å°åˆ—è¡¨
                - 'kernel_sizes': æ¯ä¸ªå·ç§¯å±‚çš„æ ¸å¤§å°åˆ—è¡¨
                - 'image_size': è¾“å…¥å›¾åƒå¤§å°ï¼ˆå‡è®¾ä¸ºæ­£æ–¹å½¢ï¼‰
                
                çŠ¶æ€æ¨¡å¼:
                - 'input_dim': è¾“å…¥ç»´åº¦å¤§å°
                - 'hidden_dim': éšè—ç»´åº¦å¤§å°
                - 'num_layers': éšè—å±‚æ•°é‡
                - 'activation': æ¿€æ´»å‡½æ•°åç§°
                - 'use_batch_norm': æ˜¯å¦ä½¿ç”¨æ‰¹å½’ä¸€åŒ–
        """
        super(PerceptionModule, self).__init__()
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è§†è§‰æ¨¡å¼
        self.vision_enabled = config.get('vision_enabled', False)
        hidden_dim = config.get('hidden_dim', 64)
        activation = config.get('activation', 'relu')
        
        # é€‰æ‹©æ¿€æ´»å‡½æ•°
        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'leaky_relu':
            self.activation = nn.LeakyReLU(0.05)
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        else:
            self.activation = nn.ReLU()
        
        if self.vision_enabled:
            # åŸºäºŽè§†è§‰çš„CNNå¤„ç†
            input_channels = config.get('input_channels', 1)  # æ·±åº¦å›¾åƒ
            conv_channels = config.get('conv_channels', [32, 64, 128])
            kernel_sizes = config.get('kernel_sizes', [5, 3, 3])
            image_size = config.get('image_size', 64)
            
            # æž„å»ºCNNå±‚
            cnn_layers = []
            in_channels = input_channels
            
            for i, (out_channels, kernel_size) in enumerate(zip(conv_channels, kernel_sizes)):
                cnn_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, 
                                           stride=2, padding=kernel_size//2))
                cnn_layers.append(nn.BatchNorm2d(out_channels))
                cnn_layers.append(self.activation)
                in_channels = out_channels
            
            self.cnn = nn.Sequential(*cnn_layers)
            
            # è®¡ç®—å·ç§¯åŽçš„å°ºå¯¸
            # æ¯ä¸ªæ­¥é•¿ä¸º2çš„å·ç§¯å±‚å°†ç©ºé—´ç»´åº¦å‡åŠ
            final_size = image_size // (2 ** len(conv_channels))
            cnn_output_size = conv_channels[-1] * final_size * final_size
            
            # æœ€ç»ˆMLPèŽ·å¾—æœŸæœ›çš„è¾“å‡ºç»´åº¦
            self.cnn_projection = nn.Sequential(
                nn.Linear(cnn_output_size, hidden_dim),
                self.activation,
                nn.Linear(hidden_dim, hidden_dim)
            )
            
            self.output_dim = hidden_dim
            
        else:
            # åŸºäºŽçŠ¶æ€çš„MLPå¤„ç†ï¼ˆåŽŸå§‹å®žçŽ°ï¼‰
            input_dim = config.get('input_dim', 9)
            num_layers = config.get('num_layers', 2)
            use_batch_norm = config.get('use_batch_norm', False)
            
            # æž„å»ºMLPå±‚
            layers = []
            
            # è¾“å…¥å±‚
            layers.append(nn.Linear(input_dim, hidden_dim))
            if use_batch_norm:
                layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(self.activation)
            
            # éšè—å±‚
            for _ in range(num_layers - 1):
                layers.append(nn.Linear(hidden_dim, hidden_dim))
                if use_batch_norm:
                    layers.append(nn.BatchNorm1d(hidden_dim))
                layers.append(self.activation)
            
            self.mlp = nn.Sequential(*layers)
            self.output_dim = hidden_dim
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        é€šè¿‡æ„ŸçŸ¥æ¨¡å—å¤„ç†è¾“å…¥ã€‚
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ 
               - è§†è§‰æ¨¡å¼: [batch_size, n_agents, channels, height, width]
               - çŠ¶æ€æ¨¡å¼: [batch_size, n_agents, input_dim] æˆ– [batch_size, input_dim]
            
        Returns:
            å¤„ç†åŽçš„ç‰¹å¾ [batch_size, output_dim] æˆ– [batch_size, n_agents, output_dim]
        """
        original_shape = x.shape
        
        if self.vision_enabled:
            # å¤„ç†è§†è§‰è¾“å…¥: [batch_size, n_agents, channels, height, width]
            if len(original_shape) == 5:
                batch_size, n_agents, channels, height, width = original_shape
                
                # é‡å¡‘ä¸º [batch_size * n_agents, channels, height, width]
                x_flat = x.reshape(batch_size * n_agents, channels, height, width)
                
                # é€šè¿‡CNNå¤„ç†
                cnn_features = self.cnn(x_flat)  # [batch_size * n_agents, final_channels, final_h, final_w]
                
                # å±•å¹³ç©ºé—´ç»´åº¦
                cnn_flat = cnn_features.view(cnn_features.size(0), -1)  # [batch_size * n_agents, flat_size]
                
                # æŠ•å½±åˆ°æœŸæœ›çš„è¾“å‡ºç»´åº¦
                features = self.cnn_projection(cnn_flat)  # [batch_size * n_agents, output_dim]
                
                # é‡å¡‘å›ž [batch_size, n_agents, output_dim]
                return features.view(batch_size, n_agents, -1)
            else:
                raise ValueError(f"è§†è§‰æ¨¡å¼æœŸæœ›5Dè¾“å…¥ [batch, agents, channels, height, width]ï¼Œå¾—åˆ° {original_shape}")
        
        else:
            # å¤„ç†åŸºäºŽçŠ¶æ€çš„è¾“å…¥ï¼ˆåŽŸå§‹å®žçŽ°ï¼‰
            if len(original_shape) == 3:
                batch_size, n_agents, input_dim = original_shape
                
                # é‡å¡‘ä¸º [batch_size * n_agents, input_dim]
                x_flat = x.reshape(batch_size * n_agents, input_dim)
                
                # é€šè¿‡MLPå¤„ç†
                features = self.mlp(x_flat)
                
                # é‡å¡‘å›ž [batch_size, n_agents, output_dim]
                return features.view(batch_size, n_agents, -1)
            else:
                # ç®€å•æ‰¹å¤„ç† [batch_size, input_dim]
                return self.mlp(x)


class MemoryModule(nn.Module):
    """
    è®°å¿†æ¨¡å—ï¼Œç”¨äºŽç»´æŠ¤æ—¶åºçŠ¶æ€ä¿¡æ¯ã€‚
    
    ä½¿ç”¨GRUç½‘ç»œç»´æŠ¤æ™ºèƒ½ä½“çš„å†…éƒ¨çŠ¶æ€ï¼Œæ”¯æŒå¤šæ™ºèƒ½ä½“åœºæ™¯ã€‚
    å¯é€‰æ‹©æ˜¯å¦åœ¨ä¸åŒæ—¶é—´æ­¥ä¹‹é—´ä¿æŒè®°å¿†çŠ¶æ€ã€‚
    """
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–è®°å¿†æ¨¡å—ã€‚
        
        å‚æ•°:
            config: åŒ…å«è®°å¿†æ¨¡å—å‚æ•°çš„å­—å…¸
                å¿…éœ€é”®å€¼:
                - 'input_dim': è¾“å…¥ç»´åº¦
                - 'hidden_dim': éšè—çŠ¶æ€ç»´åº¦
                å¯é€‰é”®å€¼:
                - 'num_layers': GRUå±‚æ•°ï¼ˆé»˜è®¤1ï¼‰
                - 'dropout': dropoutçŽ‡ï¼ˆé»˜è®¤0.0ï¼‰
        """
        super(MemoryModule, self).__init__()
        
        self.input_dim = config['input_dim']
        self.hidden_dim = config['hidden_dim']
        self.num_layers = config.get('num_layers', 1)
        self.dropout = config.get('dropout', 0.0)
        
        # åˆ›å»ºGRUå•å…ƒ
        self.gru = nn.GRU(
            input_size=self.input_dim,
            hidden_size=self.hidden_dim,
            num_layers=self.num_layers,
            dropout=self.dropout if self.num_layers > 1 else 0.0,
            batch_first=True
        )
        
        self.hidden_state = None
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼šå¤„ç†è¾“å…¥å¹¶æ›´æ–°å†…éƒ¨çŠ¶æ€ã€‚
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º[batch_size, n_agents, input_dim]æˆ–[batch_size, input_dim]
               
        è¿”å›ž:
            è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º[batch_size, n_agents, hidden_dim]æˆ–[batch_size, hidden_dim]
        """
        if x.dim() == 3:  # å¤šæ™ºèƒ½ä½“æƒ…å†µ
            batch_size, n_agents, input_dim = x.shape
            
            # å¤„ç†å¤šæ™ºèƒ½ä½“è§‚æµ‹
            # é‡å¡‘ä¸º [batch_size * n_agents, 1, input_dim]ï¼Œå› ä¸ºGRUæœŸæœ›åºåˆ—é•¿åº¦ç»´åº¦
            x = x.view(batch_size * n_agents, 1, input_dim)
            
            # åˆå§‹åŒ–æˆ–é‡ç½®éšè—çŠ¶æ€ï¼ˆå¦‚æžœéœ€è¦ï¼‰
            if self.hidden_state is None or self.hidden_state.size(1) != batch_size * n_agents:
                self.hidden_state = torch.zeros(self.num_layers, batch_size * n_agents, self.hidden_dim, 
                                               device=x.device, dtype=x.dtype)
            
            # åˆ›å»ºæ–°çš„å¼ é‡è€Œä¸æ˜¯åŽŸåœ°ä¿®æ”¹
            if self.hidden_state.device != x.device:
                self.hidden_state = self.hidden_state.to(x.device)
            
            # æ›´æ–°éšè—çŠ¶æ€
            output, new_hidden = self.gru(x, self.hidden_state)
            
            # å­˜å‚¨æ–°çš„éšè—çŠ¶æ€ï¼ˆä¸ç ´åè®¡ç®—å›¾ï¼‰
            self.hidden_state = new_hidden.detach()
            
            # é‡å¡‘å›ž [batch_size, n_agents, hidden_dim]
            return output.view(batch_size, n_agents, self.hidden_dim)
        else:
            # ç®€å•æ‰¹å¤„ç†
            batch_size, input_dim = x.shape
            x = x.view(batch_size, 1, input_dim)
            
            # åˆå§‹åŒ–æˆ–é‡ç½®éšè—çŠ¶æ€ï¼ˆå¦‚æžœéœ€è¦ï¼‰
            if self.hidden_state is None or self.hidden_state.size(1) != batch_size:
                self.hidden_state = torch.zeros(self.num_layers, batch_size, self.hidden_dim, 
                                               device=x.device, dtype=x.dtype)
            
            # åˆ›å»ºæ–°çš„å¼ é‡è€Œä¸æ˜¯åŽŸåœ°ä¿®æ”¹
            if self.hidden_state.device != x.device:
                self.hidden_state = self.hidden_state.to(x.device)
            
            # æ›´æ–°éšè—çŠ¶æ€
            output, new_hidden = self.gru(x, self.hidden_state)
            
            # å­˜å‚¨æ–°çš„éšè—çŠ¶æ€ï¼ˆä¸ç ´åè®¡ç®—å›¾ï¼‰
            self.hidden_state = new_hidden.detach()
            
            return output.squeeze(1)  # ç§»é™¤åºåˆ—é•¿åº¦ç»´åº¦
    
    def reset(self) -> None:
        """é‡ç½®è®°å¿†çŠ¶æ€ã€‚"""
        self.hidden_state = None


class PolicyHeadModule(nn.Module):
    """
    ç­–ç•¥å¤´æ¨¡å—ï¼Œç”¨äºŽç”ŸæˆåŠ¨ä½œã€‚
    
    å°†ç‰¹å¾è½¬æ¢ä¸ºåŠ¨ä½œè¾“å‡ºï¼Œå¯é€‰æ‹©åº”ç”¨åŠ¨ä½œè¾¹ç•Œå’Œå…¶ä»–å˜æ¢ã€‚
    æ”¯æŒè‡ªé€‚åº”å®‰å…¨è¾¹è·ï¼ˆåŠ¨æ€Alphaï¼‰é¢„æµ‹ã€‚
    """
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–ç­–ç•¥å¤´æ¨¡å—ã€‚
        
        å‚æ•°:
            config: åŒ…å«ç­–ç•¥å¤´å‚æ•°çš„å­—å…¸
                å¿…éœ€é”®å€¼:
                - 'input_dim': è¾“å…¥ç‰¹å¾ç»´åº¦
                - 'output_dim': åŠ¨ä½œè¾“å‡ºç»´åº¦
                å¯é€‰é”®å€¼:
                - 'hidden_dims': éšè—å±‚ç»´åº¦åˆ—è¡¨
                - 'activation': æ¿€æ´»å‡½æ•°åç§°
                - 'output_activation': è¾“å‡ºå±‚æ¿€æ´»å‡½æ•°
                - 'action_scale': åŠ¨ä½œç¼©æ”¾å› å­
                - 'predict_alpha': æ˜¯å¦é¢„æµ‹åŠ¨æ€alphaï¼ˆé»˜è®¤Trueï¼‰
                - 'alpha_hidden_dim': alphaç½‘ç»œéšè—å±‚ç»´åº¦
        """
        super(PolicyHeadModule, self).__init__()
        
        self.input_dim = config['input_dim']
        self.output_dim = config['output_dim']
        self.hidden_dims = config.get('hidden_dims', [256, 256])
        self.action_scale = config.get('action_scale', 1.0)
        
        # æ¿€æ´»å‡½æ•°
        activation_name = config.get('activation', 'relu')
        self.activation = getattr(nn, activation_name.capitalize())() if hasattr(nn, activation_name.capitalize()) else nn.ReLU()
        
        output_activation = config.get('output_activation', None)
        self.output_activation = getattr(nn, output_activation.capitalize())() if output_activation and hasattr(nn, output_activation.capitalize()) else None
        
        # è‡ªé€‚åº”å®‰å…¨è¾¹è·é…ç½®
        self.predict_alpha = config.get('predict_alpha', True)
        self.predict_margin = config.get('predict_margin', False)  # æ–°å¢žï¼šæ˜¯å¦é¢„æµ‹åŠ¨æ€å®‰å…¨è£•åº¦
        
        # æž„å»ºåŠ¨ä½œé¢„æµ‹MLPå±‚
        self.action_layers = nn.ModuleList()
        
        # åŠ¨ä½œçš„éšè—å±‚
        layer_dims = [self.input_dim] + self.hidden_dims
        for i in range(len(layer_dims) - 1):
            self.action_layers.append(nn.Linear(layer_dims[i], layer_dims[i+1]))
            self.action_layers.append(self.activation)
        
        # åŠ¨ä½œè¾“å‡ºå±‚
        self.action_layers.append(nn.Linear(self.hidden_dims[-1] if self.hidden_dims else self.input_dim, self.output_dim))
        
        self.action_network = nn.Sequential(*self.action_layers)
        
        # ä»…å½“predict_alphaä¸ºTrueæ—¶æž„å»ºalphaé¢„æµ‹MLP
        if self.predict_alpha:
            alpha_hidden_dim = config.get('alpha_hidden_dim', self.hidden_dims[0] // 2 if self.hidden_dims else 32)
            self.alpha_network = nn.Sequential(
                nn.Linear(self.input_dim, alpha_hidden_dim),
                self.activation,
                nn.Linear(alpha_hidden_dim, 1),  # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“é¢„æµ‹å•ä¸ªalpha
                nn.Softplus()  # ç¡®ä¿alpha > 0
            )
        else:
            self.alpha_network = None
            
        # ðŸš€ CORE INNOVATION: åŠ¨æ€å®‰å…¨è£•åº¦é¢„æµ‹ç½‘ç»œ
        if self.predict_margin:
            margin_hidden_dim = config.get('margin_hidden_dim', self.hidden_dims[0] // 4 if self.hidden_dims else 16)
            self.margin_network = nn.Sequential(
                nn.Linear(self.input_dim, margin_hidden_dim),
                self.activation,
                nn.Linear(margin_hidden_dim, 1),  # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“é¢„æµ‹å•ä¸ªå®‰å…¨è£•åº¦
                nn.Sigmoid()  # è¾“å‡ºåˆ°(0, 1)èŒƒå›´ï¼Œç¨åŽæ˜ å°„åˆ°[min_margin, max_margin]
            )
        else:
            self.margin_network = None
    
    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­ï¼šç”ŸæˆåŠ¨ä½œã€å¯é€‰çš„alphaå€¼å’ŒåŠ¨æ€å®‰å…¨è£•åº¦ã€‚
        
        å‚æ•°:
            features: è¾“å…¥ç‰¹å¾ï¼Œå½¢çŠ¶ä¸º[batch_size, n_agents, input_dim]æˆ–[batch_size, input_dim]
               
        è¿”å›ž:
            å…ƒç»„(actions, alpha, dynamic_margins):
            - actions: åŠ¨ä½œå¼ é‡
            - alpha: åŠ¨æ€alphaå€¼ï¼ˆå¦‚æžœå¯ç”¨ï¼‰æˆ–None
            - dynamic_margins: åŠ¨æ€å®‰å…¨è£•åº¦ï¼ˆå¦‚æžœå¯ç”¨ï¼‰æˆ–None
        """
        if features.dim() == 3:  # å¤šæ™ºèƒ½ä½“æƒ…å†µ
            batch_size, n_agents, input_dim = features.shape
            
            # å¤„ç†å¤šæ™ºèƒ½ä½“ç‰¹å¾
            # é‡å¡‘ä¸º [batch_size * n_agents, input_dim]
            features_flat = features.view(-1, input_dim)
            
            # é€šè¿‡åŠ¨ä½œç½‘ç»œå¤„ç†
            actions_flat = self.action_network(features_flat)
            
            # åº”ç”¨è¾“å‡ºæ¿€æ´»å‡½æ•°
            if self.output_activation is not None:
                actions_flat = self.output_activation(actions_flat)
            
            # ç¼©æ”¾åŠ¨ä½œï¼ˆå¦‚æžœéœ€è¦ï¼‰
            if self.action_scale != 1.0:
                actions_flat = actions_flat * self.action_scale
            
            # é‡å¡‘åŠ¨ä½œå›ž [batch_size, n_agents, -1]
            actions = actions_flat.view(batch_size, n_agents, -1)
            
            # å¦‚æžœå¯ç”¨ï¼Œé€šè¿‡alphaç½‘ç»œå¤„ç†
            if self.alpha_network is not None:
                alpha_flat = self.alpha_network(features_flat)
                alpha = alpha_flat.view(batch_size, n_agents, 1)
            else:
                alpha = None
                
            # ðŸš€ CORE INNOVATION: å¦‚æžœå¯ç”¨ï¼Œé€šè¿‡åŠ¨æ€å®‰å…¨è£•åº¦ç½‘ç»œå¤„ç†
            if self.margin_network is not None:
                margin_flat = self.margin_network(features_flat)
                dynamic_margins = margin_flat.view(batch_size, n_agents, 1)
            else:
                dynamic_margins = None
                
            return actions, alpha, dynamic_margins
        else:
            # ç®€å•æ‰¹å¤„ç†
            actions = self.action_network(features)
            
            # ç¼©æ”¾åŠ¨ä½œï¼ˆå¦‚æžœéœ€è¦ï¼‰
            if self.action_scale != 1.0:
                actions = actions * self.action_scale
            
            # å¦‚æžœå¯ç”¨ï¼Œé€šè¿‡alphaç½‘ç»œå¤„ç†
            if self.alpha_network is not None:
                alpha = self.alpha_network(features)
            else:
                alpha = None
                
            # ðŸš€ CORE INNOVATION: å¦‚æžœå¯ç”¨ï¼Œé€šè¿‡åŠ¨æ€å®‰å…¨è£•åº¦ç½‘ç»œå¤„ç†
            if self.margin_network is not None:
                dynamic_margins = self.margin_network(features)
            else:
                dynamic_margins = None
                
            return actions, alpha, dynamic_margins


class BPTTPolicy(nn.Module):
    """
    æ—¶åºåå‘ä¼ æ’­ï¼ˆBPTTï¼‰ç­–ç•¥ç½‘ç»œã€‚
    
    ç»“åˆæ„ŸçŸ¥ã€è®°å¿†å’Œç­–ç•¥å¤´æ¨¡å—ï¼Œå®žçŽ°ç«¯åˆ°ç«¯çš„ç­–ç•¥å­¦ä¹ ã€‚
    æ”¯æŒå¤šæ™ºèƒ½ä½“åœºæ™¯å’Œè‡ªé€‚åº”å®‰å…¨è¾¹è·æœºåˆ¶ã€‚
    """
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–BPTTç­–ç•¥ç½‘ç»œã€‚
        
        å‚æ•°:
            config: åŒ…å«ç­–ç•¥ç½‘ç»œå®Œæ•´é…ç½®çš„å­—å…¸
        """
        super(BPTTPolicy, self).__init__()
        
        # æå–å­é…ç½®
        perception_config = config.get('perception', {})
        memory_config = config.get('memory', {})
        policy_head_config = config.get('policy_head', {})
        
        # åˆ›å»ºæ„ŸçŸ¥æ¨¡å—
        self.perception = PerceptionModule(perception_config)
        
        # åŸºäºŽæ„ŸçŸ¥è¾“å‡ºæ›´æ–°è®°å¿†è¾“å…¥ç»´åº¦
        memory_config['input_dim'] = self.perception.output_dim
        
        # åˆ›å»ºè®°å¿†æ¨¡å—
        self.memory = MemoryModule(memory_config)
        
        # åŸºäºŽè®°å¿†è¾“å‡ºæ›´æ–°ç­–ç•¥å¤´è¾“å…¥ç»´åº¦
        policy_head_config['input_dim'] = self.memory.hidden_dim
        
        # åˆ›å»ºç­–ç•¥å¤´æ¨¡å—
        self.policy_head = PolicyHeadModule(policy_head_config)
        
        # å­˜å‚¨é…ç½®
        self.config = config
    
    def forward(self, observations: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­ï¼šå°†è§‚æµ‹è½¬æ¢ä¸ºåŠ¨ä½œã€å¯é€‰çš„alphaå€¼å’ŒåŠ¨æ€å®‰å…¨è£•åº¦ã€‚
        
        å‚æ•°:
            observations: è§‚æµ‹å¼ é‡
               
        è¿”å›ž:
            å…ƒç»„(actions, alpha, dynamic_margins):
            - actions: åŠ¨ä½œå¼ é‡  
            - alpha: åŠ¨æ€alphaå€¼ï¼ˆå¦‚æžœå¯ç”¨ï¼‰æˆ–None
            - dynamic_margins: åŠ¨æ€å®‰å…¨è£•åº¦ï¼ˆå¦‚æžœå¯ç”¨ï¼‰æˆ–None
        """
        # é€šè¿‡æ„ŸçŸ¥æ¨¡å—å¤„ç†
        features = self.perception(observations)
        
        # é€šè¿‡è®°å¿†æ¨¡å—å¤„ç†
        memory_output = self.memory(features)
        
        # é€šè¿‡ç­–ç•¥å¤´ç”ŸæˆåŠ¨ä½œã€alphaå’ŒåŠ¨æ€å®‰å…¨è£•åº¦
        actions, alpha, dynamic_margins = self.policy_head(memory_output)
        
        return actions, alpha, dynamic_margins
    
    def reset(self) -> None:
        """é‡ç½®ç­–ç•¥çš„å†…éƒ¨çŠ¶æ€ï¼ˆä¾‹å¦‚è®°å¿†ï¼‰ã€‚"""
        if hasattr(self, 'memory'):
            self.memory.reset()


class EnsemblePolicy(nn.Module):
    """
    é›†æˆç­–ç•¥ç½‘ç»œã€‚
    
    ç»“åˆå¤šä¸ªç­–ç•¥ç½‘ç»œçš„è¾“å‡ºï¼Œæä¾›æ›´ç¨³å®šçš„åŠ¨ä½œé¢„æµ‹ã€‚
    æ”¯æŒç®€å•å¹³å‡å’ŒåŠ æƒå¹³å‡ä¸¤ç§é›†æˆæ–¹æ³•ã€‚
    """
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–é›†æˆç­–ç•¥ç½‘ç»œã€‚
        
        å‚æ•°:
            config: åŒ…å«é›†æˆç­–ç•¥é…ç½®çš„å­—å…¸
                å¿…éœ€é”®å€¼:
                - 'policies': ç­–ç•¥é…ç½®åˆ—è¡¨
                å¯é€‰é”®å€¼:
                - 'ensemble_method': é›†æˆæ–¹æ³•ï¼ˆ'mean'æˆ–'weighted'ï¼‰
                - 'num_policies': ç­–ç•¥æ•°é‡
        """
        super(EnsemblePolicy, self).__init__()
        
        # æå–é…ç½®å‚æ•°
        policies_config = config.get('policies', [])
        self.ensemble_method = config.get('ensemble_method', 'mean')
        self.num_policies = config.get('num_policies', len(policies_config))
        
        # åˆ›å»ºç­–ç•¥é›†æˆ
        self.policies = nn.ModuleList()
        for policy_config in policies_config:
            policy = BPTTPolicy(policy_config)
            self.policies.append(policy)
        
        # å¦‚æžœä½¿ç”¨åŠ æƒé›†æˆï¼Œåˆ›å»ºæƒé‡å‚æ•°
        if self.ensemble_method == 'weighted':
            self.ensemble_weights = nn.Parameter(torch.ones(self.num_policies))
            # è®¾å¤‡å°†åœ¨æ¨¡åž‹ç§»åŠ¨åˆ°è®¾å¤‡æ—¶è®¾ç½®
        
        # å­˜å‚¨é…ç½®
        self.config = config
    
    def forward(self, observations: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­ï¼šé€šè¿‡é›†æˆç­–ç•¥ç”ŸæˆåŠ¨ä½œå’Œalphaå€¼ã€‚
        
        å‚æ•°:
            observations: è§‚æµ‹å¼ é‡
               
        è¿”å›ž:
            å…ƒç»„(actions, alpha):
            - actions: é›†æˆåŽçš„åŠ¨ä½œå¼ é‡
            - alpha: é›†æˆåŽçš„alphaå€¼ï¼ˆå¦‚æžœå¯ç”¨ï¼‰æˆ–None
        """
        # ä»Žæ¯ä¸ªç­–ç•¥èŽ·å–åŠ¨ä½œå’Œalpha
        policy_outputs = []
        for policy in self.policies:
            actions, alpha = policy(observations)
            policy_outputs.append((actions, alpha))
        
        # åˆ†ç¦»åŠ¨ä½œå’Œalpha
        actions_list = [output[0] for output in policy_outputs]
        alphas_list = [output[1] for output in policy_outputs if output[1] is not None]
        
        # å †å ä»¥ä¾¿ç»„åˆ [num_policies, batch_size, action_dim/1]
        stacked_actions = torch.stack(actions_list, dim=0)
        stacked_alphas = torch.stack(alphas_list, dim=0) if alphas_list else None
        
        # åŸºäºŽé›†æˆæ–¹æ³•ç»„åˆåŠ¨ä½œå’Œalpha
        if self.ensemble_method == 'mean':
            # ç®€å•å¹³å‡
            final_actions = torch.mean(stacked_actions, dim=0)
            final_alpha = torch.mean(stacked_alphas, dim=0) if stacked_alphas is not None else None
        elif self.ensemble_method == 'weighted':
            # åŠ æƒå¹³å‡
            weights = torch.softmax(self.ensemble_weights, dim=0)
            weights = weights.view(-1, 1, 1, 1)  # å¹¿æ’­å½¢çŠ¶
            final_actions = torch.sum(stacked_actions * weights, dim=0)
            final_alpha = torch.sum(stacked_alphas * weights, dim=0) if stacked_alphas is not None else None
        else:
            # é»˜è®¤ä½¿ç”¨å‡å€¼
            final_actions = torch.mean(stacked_actions, dim=0)
            final_alpha = torch.mean(stacked_alphas, dim=0) if stacked_alphas is not None else None
        
        return final_actions, final_alpha
    
    def reset(self) -> None:
        """é‡ç½®é›†æˆä¸­çš„æ‰€æœ‰ç­–ç•¥ã€‚"""
        for policy in self.policies:
            policy.reset()


def create_policy_from_config(config: Dict) -> nn.Module:
    """
    Factory function to create a policy from configuration.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Policy network instance
    """
    policy_type = config.get('type', 'bptt')
    
    if policy_type == 'bptt':
        return BPTTPolicy(config)
    elif policy_type == 'ensemble':
        return EnsemblePolicy(config)
    else:
        raise ValueError(f"Unknown policy type: {policy_type}") 