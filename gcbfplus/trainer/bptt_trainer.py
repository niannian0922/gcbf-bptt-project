# End-to-end BPTT trainer with bottleneck scenario analysis

import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from tqdm import tqdm
from typing import Dict, Any, Optional, Tuple, List, Union, Callable

# å°è¯•å¯¼å…¥wandbï¼Œä½†è®¾ä¸ºå¯é€‰
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("è­¦å‘Šï¼šæœªæ‰¾åˆ°wandbã€‚è®­ç»ƒå°†ç»§ç»­è¿›è¡Œï¼Œä½†ä¸ä¼šè®°å½•åˆ°wandbã€‚")

from ..env.base_env import BaseEnv, EnvState
from ..env.multi_agent_env import MultiAgentEnv, MultiAgentState
from ..policy.bptt_policy import BPTTPolicy
from ..utils.episode_logger import EpisodeLogger, compute_min_distances_to_obstacles, compute_goal_distances


class BPTTTrainer:
    """
    å®ç°æ—¶åºåå‘ä¼ æ’­ï¼ˆBPTTï¼‰çš„è®­ç»ƒå™¨ï¼Œé€šè¿‡å¯å¾®åˆ†ç‰©ç†ä»¿çœŸå™¨è¿›è¡Œç­–ç•¥å’ŒCBFç½‘ç»œçš„ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚
    
    è¯¥è®­ç»ƒå™¨é€šè¿‡ä»¿çœŸå™¨çš„æ¢¯åº¦ç›´æ¥ä¼˜åŒ–ä¸¤ä¸ªç½‘ç»œï¼Œæ— éœ€Qå­¦ä¹ ã€ä¸“å®¶ç­–ç•¥å’Œé‡æ”¾ç¼“å†²åŒºã€‚
    æ”¯æŒè‡ªé€‚åº”å®‰å…¨è¾¹è·ï¼ˆåŠ¨æ€Alphaï¼‰çš„åˆ›æ–°è®­ç»ƒæ–¹æ³•ã€‚
    """
    
    def __init__(
        self,
        env: BaseEnv,
        policy_network: nn.Module,
        cbf_network: Optional[nn.Module] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        config: Optional[Dict[str, Any]] = None
    ):
        """
        åˆå§‹åŒ–BPTTè®­ç»ƒå™¨ã€‚
        
        å‚æ•°:
            env: å¯å¾®åˆ†ç¯å¢ƒå®ä¾‹
            policy_network: è¦è®­ç»ƒçš„ç­–ç•¥ç½‘ç»œ
            cbf_network: å¯é€‰çš„CBFå®‰å…¨ç½‘ç»œ
            optimizer: å¯é€‰çš„ä¼˜åŒ–å™¨ï¼ˆå¦‚æœä¸ºNoneå°†åˆ›å»ºé»˜è®¤å€¼ï¼‰
            config: é…ç½®å­—å…¸
        """
        # å­˜å‚¨ç¯å¢ƒå’Œç½‘ç»œ
        self.env = env
        self.policy_network = policy_network
        self.cbf_network = cbf_network
        
        # ä»ç­–ç•¥ç½‘ç»œè·å–è®¾å¤‡
        self.device = next(policy_network.parameters()).device
        
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œåˆ™è®¾ç½®é»˜è®¤é…ç½®
        self.config = {} if config is None else config
        
        # ä»é…ç½®ä¸­æå–å‚æ•°
        self.run_name = self.config.get('run_name', 'BPTT_Run')
        # åŸºäºrun_nameæ„å»ºå”¯ä¸€çš„æ—¥å¿—ç›®å½•
        self.log_dir = f"logs/{self.run_name}"
        self.num_agents = self.config.get('num_agents', 8)
        self.area_size = self.config.get('area_size', 1.0)
        
        # è®­ç»ƒå‚æ•°
        # ğŸš€ ä¿®å¤ï¼šä»trainingå­éƒ¨åˆ†æ­£ç¡®è¯»å–è®­ç»ƒæ­¥æ•°
        training_config = self.config.get('training', {})
        self.training_steps = training_config.get('training_steps', self.config.get('training_steps', 10000))
        self.eval_interval = training_config.get('eval_interval', self.config.get('eval_interval', 100))
        self.save_interval = training_config.get('save_interval', self.config.get('save_interval', 1000))
        self.horizon_length = training_config.get('horizon_length', self.config.get('horizon_length', 50))
        self.eval_horizon = training_config.get('eval_horizon', self.config.get('eval_horizon', 100))
        self.max_grad_norm = training_config.get('max_grad_norm', self.config.get('max_grad_norm', 1.0))
        
        # æŸå¤±æƒé‡ - æ”¯æŒæ–°çš„æ§åˆ¶æ­£åˆ™åŒ–
        loss_weights = self.config.get('loss_weights', {})
        self.goal_weight = loss_weights.get('goal_weight', self.config.get('goal_weight', 1.0))
        self.safety_weight = loss_weights.get('safety_weight', self.config.get('safety_weight', 10.0))
        self.control_weight = loss_weights.get('control_weight', self.config.get('control_weight', 0.1))
        self.jerk_weight = loss_weights.get('jerk_weight', self.config.get('jerk_weight', 0.05))
        self.alpha_reg_weight = loss_weights.get('alpha_reg_weight', self.config.get('alpha_reg_weight', 0.01))
        self.progress_weight = loss_weights.get('progress_weight', self.config.get('progress_weight', 0.0))
        
        # æ–°å¢æ§åˆ¶æ­£åˆ™åŒ–æƒé‡
        self.acceleration_loss_weight = loss_weights.get('acceleration_loss_weight', 0.01)
        
        self.cbf_alpha = self.config.get('cbf_alpha', 1.0)
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        if not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir)
        self.model_dir = os.path.join(self.log_dir, 'models')
        if not os.path.exists(self.model_dir):
            os.makedirs(self.model_dir)
        
        # å¦‚æœæ²¡æœ‰æä¾›ä¼˜åŒ–å™¨ï¼Œåˆ™åˆå§‹åŒ–
        if optimizer is None:
            params = list(self.policy_network.parameters())
            if self.cbf_network is not None:
                params += list(self.cbf_network.parameters())
                
            self.optimizer = optim.Adam(
                params,
                lr=self.config.get('learning_rate', 0.001)
            )
        else:
            self.optimizer = optimizer
        
        # å¦‚æœæŒ‡å®šäº†å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œåˆ™åˆå§‹åŒ–
        if self.config.get('use_lr_scheduler', False):
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config.get('lr_step_size', 2000),
                gamma=self.config.get('lr_gamma', 0.5)
            )
        else:
            self.scheduler = None
        
        # åˆå§‹åŒ–æ•°æ®è®°å½•å™¨
        self.enable_episode_logging = self.config.get('enable_episode_logging', False)
        if self.enable_episode_logging:
            log_dir = os.path.join(self.log_dir, 'episode_logs')
            self.episode_logger = EpisodeLogger(log_dir=log_dir, prefix="bptt_episode")
            print(f"ğŸ“Š æ•°æ®è®°å½•å·²å¯ç”¨: {log_dir}")
        else:
            self.episode_logger = None
    
    def initialize_scenario(self, batch_size: int = 1) -> MultiAgentState:
        """
        åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„åœºæ™¯ï¼Œå¸¦æœ‰éšæœºåˆå§‹çŠ¶æ€å’Œç›®æ ‡ã€‚
        
        å‚æ•°:
            batch_size: è¦åˆå§‹åŒ–çš„å¹¶è¡Œç¯å¢ƒæ•°é‡
            
        è¿”å›:
            ç¯å¢ƒçŠ¶æ€
        """
        # ä½¿ç”¨ç¯å¢ƒçš„é‡ç½®æ–¹æ³•åˆå§‹åŒ–çŠ¶æ€
        return self.env.reset(batch_size=batch_size, randomize=True)
    
    def train(self) -> None:
        """
        å®ç°BPTTä¼˜åŒ–çš„ä¸»è®­ç»ƒå¾ªç¯ã€‚
        """
        print(f"Starting BPTT training with configuration:")
        print(f"  Run name: {self.run_name}")
        print(f"  Steps: {self.training_steps}")
        print(f"  Horizon: {self.horizon_length}")
        print(f"  Log dir: {self.log_dir}")
        
        # åœ¨ç¦»çº¿æ¨¡å¼ä¸‹åˆå§‹åŒ–wandb
        if WANDB_AVAILABLE:
            wandb.init(name=self.run_name, project='gcbf-bptt', dir=self.log_dir, config=self.config, mode="offline")
        
        start_time = time.time()
        
        # å°†ç¯å¢ƒè®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ä»¥å¯ç”¨æ¢¯åº¦è¡°å‡
        self.env.train()
        
        pbar = tqdm(total=self.training_steps)
        for step in range(self.training_steps):
            # è®­ç»ƒæ¨¡å¼
            self.policy_network.train()
            if self.cbf_network is not None:
                self.cbf_network.train()
            
            # åœ¨æ¯æ¬¡åå‘ä¼ æ’­ä¹‹å‰æ¸…é›¶æ¢¯åº¦
            self.optimizer.zero_grad()
            
            # åˆå§‹åŒ–åœºæ™¯
            state = self.initialize_scenario()
            
            # BPTT Rollout
            trajectory_states = []
            trajectory_actions = []
            trajectory_alphas = []
            trajectory_rewards = []
            trajectory_costs = []
            safety_losses = []
            
            # è¿è¡Œå‰å‘ä»¿çœŸå¹¶æ”¶é›†è½¨è¿¹æ•°æ®
            current_state = state
            for t in range(self.horizon_length):
                # ä¿å­˜å½“å‰çŠ¶æ€
                trajectory_states.append(current_state)
                
                # ä»çŠ¶æ€è·å–è§‚æµ‹å€¼
                observations = self.env.get_observation(current_state)
                
                # åœ¨å°†è§‚æµ‹å€¼ä¼ é€’ç»™ç½‘ç»œä¹‹å‰ï¼Œå°†è§‚æµ‹å€¼ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
                observations = observations.to(self.device)
                actions, alpha, raw_dynamic_margins = self.policy_network(observations)
                
                # ğŸš€ CORE INNOVATION: å¤„ç†åŠ¨æ€å®‰å…¨è£•åº¦
                dynamic_margins = None
                if raw_dynamic_margins is not None and self.config.get("use_adaptive_margin", False):
                    # å°†Sigmoidè¾“å‡º(0,1)æ˜ å°„åˆ°é…ç½®çš„[min_margin, max_margin]èŒƒå›´
                    min_margin = self.config.get("min_safety_margin", 0.15)
                    max_margin = self.config.get("max_safety_margin", 0.4)
                    # å…³é”®ä¿®å¤ï¼šç¡®ä¿dynamic_marginså¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    dynamic_margins = (min_margin + raw_dynamic_margins * (max_margin - min_margin)).to(self.device)
                
                # å¤„ç†alphaä¸ºNoneçš„æƒ…å†µï¼ˆå›ºå®šalphaé…ç½®ï¼‰
                if alpha is None:
                    # ä½¿ç”¨ç¯å¢ƒé»˜è®¤çš„alphaå€¼
                    batch_size, num_agents = actions.shape[:2]
                    alpha = torch.full((batch_size, num_agents, 1), 
                                     self.env.cbf_alpha, 
                                     device=self.device, 
                                     dtype=actions.dtype)
                
                # å­˜å‚¨ç”¨äºåå‘ä¼ æ’­çš„åˆ†ç¦»å‰¯æœ¬
                trajectory_actions.append(actions.clone())
                trajectory_alphas.append(alpha.clone())
                # ğŸš€ CORE INNOVATION: å­˜å‚¨åŠ¨æ€å®‰å…¨è£•åº¦
                if dynamic_margins is not None:
                    if not hasattr(self, 'trajectory_margins'):
                        self.trajectory_margins = []
                    self.trajectory_margins.append(dynamic_margins.clone())
                
                # å¦‚æœæä¾›äº†CBFç½‘ç»œï¼Œåˆ™è®¡ç®—å±éšœå‡½æ•°å€¼ç”¨äºæŸå¤±è®¡ç®—
                if self.cbf_network is not None:
                    # ğŸš€ CORE INNOVATION: CBFç½‘ç»œä½¿ç”¨åŠ¨æ€å®‰å…¨è£•åº¦
                    cbf_values = self.cbf_network.barrier_function(current_state, dynamic_margins)
                    
                    # æ ¹æ®CBFå€¼è®¡ç®—å®‰å…¨æŸå¤±
                    # è´Ÿå€¼è¡¨ç¤ºä¸å®‰å…¨çŠ¶æ€
                    safety_loss = torch.mean(torch.relu(-cbf_values))
                    safety_losses.append(safety_loss)
                
                # ğŸ›¡ï¸ PROBABILISTIC SAFETY SHIELD: åœ¨ç¯å¢ƒä¸­ä½¿ç”¨åŠ¨æ€alphaè¿›è¡Œä¸€æ­¥
                step_result = self.env.step(current_state, actions, alpha)
                next_state = step_result.next_state
                rewards = step_result.reward
                costs = step_result.cost
                
                # ğŸ›¡ï¸ è®¡ç®—å®‰å…¨ä¿¡å¿ƒåˆ†æ•°ç”¨äºæ–°çš„é£é™©è¯„ä¼°æŸå¤±
                if hasattr(self.env, 'safety_layer') and self.env.safety_layer is not None:
                    alpha_safety = self.env.safety_layer.compute_safety_confidence(current_state, dynamic_margins)
                    # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿç¢°æ’ï¼ˆç”¨äºæ–°çš„CBFæŸå¤±è®¡ç®—ï¼‰
                    is_collision = costs > 0  # å‡è®¾cost > 0è¡¨ç¤ºç¢°æ’
                else:
                    alpha_safety = None
                    is_collision = costs > 0
                
                # ğŸ›¡ï¸ å­˜å‚¨å®‰å…¨ä¿¡å¿ƒåˆ†æ•°å’Œç¢°æ’æ ‡å¿—ç”¨äºæ–°çš„é£é™©è¯„ä¼°æŸå¤±
                if alpha_safety is not None:
                    if not hasattr(self, 'trajectory_alpha_safety'):
                        self.trajectory_alpha_safety = []
                    if not hasattr(self, 'trajectory_collisions'):
                        self.trajectory_collisions = []
                    self.trajectory_alpha_safety.append(alpha_safety.clone())
                    self.trajectory_collisions.append(is_collision.clone())
                
                # ä¿å­˜å¥–åŠ±å’Œæˆæœ¬ï¼ˆåˆ†ç¦»ä»¥é˜²æ­¢åœ¨åå‘ä¼ æ’­æœŸé—´ä¿®æ”¹ï¼‰
                trajectory_rewards.append(rewards.clone())
                trajectory_costs.append(costs.clone())
                
                # æ›´æ–°å½“å‰çŠ¶æ€ä»¥è¿›è¡Œä¸‹ä¸€æ¬¡è¿­ä»£ï¼ˆåˆ†ç¦»ä»¥é˜²æ­¢å°±åœ°ä¿®æ”¹ï¼‰
                current_state = next_state
            
            # è®¡ç®—æŸå¤±
            
            # ç›®æ ‡åˆ°è¾¾æŸå¤±ï¼ˆä½¿ç”¨å¥–åŠ±ï¼‰
            if trajectory_rewards:
                stacked_rewards = torch.stack(trajectory_rewards)
                goal_loss = -torch.mean(stacked_rewards)
            else:
                # å›é€€ï¼šä½¿ç”¨åˆ°ç›®æ ‡çš„è·ç¦»
                goal_distances = self.env.get_goal_distance(current_state)
                goal_loss = torch.mean(goal_distances)
            
            # æ§åˆ¶åŠªåŠ›æŸå¤±
            stacked_actions = torch.stack(trajectory_actions)
            control_effort = torch.mean(stacked_actions ** 2)
            
            # åŠ é€Ÿåº¦æŸå¤± - L2èŒƒæ•°çš„åŠ¨ä½œï¼ˆæ§åˆ¶è¾“å…¥çš„å¤§å°ï¼‰
            acceleration_loss = torch.mean(torch.square(stacked_actions))
            
            # æŠ–åŠ¨æŸå¤±ï¼ˆJerk Lossï¼‰- è¿ç»­åŠ¨ä½œä¹‹é—´å·®å¼‚çš„L2èŒƒæ•°
            jerk_loss = 0.0
            if len(trajectory_actions) > 1:
                action_diffs = []
                for i in range(1, len(trajectory_actions)):
                    action_diff = trajectory_actions[i] - trajectory_actions[i-1]
                    action_diffs.append(action_diff)
                if action_diffs:
                    stacked_diffs = torch.stack(action_diffs)
                    jerk_loss = torch.mean(torch.square(stacked_diffs))
            
            # ğŸ›¡ï¸ PROBABILISTIC SAFETY SHIELD: æ–°çš„é£é™©è¯„ä¼°å™¨æŸå¤±å‡½æ•°
            # CBFçš„ç›®çš„ä¸å†æ˜¯ç®€å•åœ°å¼ºåˆ¶h(x) > 0ï¼Œè€Œæ˜¯è®­ç»ƒGCBFæ¨¡å—æˆä¸ºå‡†ç¡®çš„"é£é™©è¯„ä¼°å™¨"
            if hasattr(self, 'trajectory_alpha_safety') and self.trajectory_alpha_safety:
                # å®ç°æ–°çš„loss_cbfï¼šå¦‚æœæ¨¡å‹åœ¨ç¢°æ’å‰"è¿‡åº¦è‡ªä¿¡"ï¼ˆé«˜alpha_safetyï¼‰ï¼Œåˆ™ä¸¥é‡æƒ©ç½š
                stacked_alpha_safety = torch.stack(self.trajectory_alpha_safety)
                stacked_collisions = torch.stack(self.trajectory_collisions)
                
                # è®¡ç®—é£é™©è¯„ä¼°æŸå¤±ï¼šä»…åœ¨å‘ç”Ÿç¢°æ’æ—¶æƒ©ç½šé«˜confidence
                # loss_cbf = alpha_safety if collision else 0.0
                collision_mask = stacked_collisions.float()  # è½¬æ¢å¸ƒå°”å€¼ä¸ºæµ®ç‚¹æ•°
                # Debug shapes to ensure alignment
                print(f"Shape of collision_mask: {collision_mask.shape}")
                print(f"Shape of stacked_alpha_safety: {stacked_alpha_safety.shape}")
                # Align alpha_safety shape to match collision mask if needed
                if stacked_alpha_safety.dim() < collision_mask.dim():
                    # Assume [T] or [T,1] => expand along agent dimension
                    num_agents = collision_mask.shape[1]
                    alpha_expanded = stacked_alpha_safety.unsqueeze(1).expand(-1, num_agents)
                else:
                    alpha_expanded = stacked_alpha_safety
                risk_assessment_loss = torch.mean(collision_mask * alpha_expanded)
                total_safety_loss = risk_assessment_loss
                
            elif safety_losses:
                # å›é€€åˆ°ä¼ ç»ŸCBFæŸå¤±ï¼ˆå¦‚æœæ²¡æœ‰ä½¿ç”¨æ¦‚ç‡é˜²æŠ¤ç½©ï¼‰
                stacked_safety = torch.stack(safety_losses)
                total_safety_loss = torch.mean(stacked_safety)
            else:
                # å¦‚æœæ²¡æœ‰CBFç½‘ç»œï¼Œåˆ™ä½¿ç”¨ç¯å¢ƒæˆæœ¬
                stacked_costs = torch.stack(trajectory_costs)
                total_safety_loss = torch.mean(stacked_costs)
            
            # Alphaæ­£åˆ™åŒ–æŸå¤±ï¼ˆé¼“åŠ±æ›´å°çš„alphaå€¼ä»¥æé«˜æ•ˆç‡ï¼‰
            stacked_alphas = torch.stack(trajectory_alphas)
            alpha_regularization_loss = torch.mean(stacked_alphas)
            
            # è¿›åº¦å¥–åŠ±æŸå¤±ï¼ˆåŸºäºæ½œåŠ›çš„å¥–åŠ±å¡‘å½¢ï¼‰
            progress_loss = 0.0
            if self.progress_weight > 0.0 and len(trajectory_states) > 1:
                progress_loss = self._calculate_progress_loss(trajectory_states)
            
            # è®¡ç®—åŸºç¡€æ€»æŸå¤± - åŒ…å«æ–°çš„æ§åˆ¶æ­£åˆ™åŒ–é¡¹
            total_loss = (
                self.goal_weight * goal_loss +
                self.safety_weight * total_safety_loss +
                self.control_weight * control_effort +
                self.acceleration_loss_weight * acceleration_loss +  # æ–°å¢ï¼šåŠ é€Ÿåº¦æŸå¤±
                self.jerk_weight * jerk_loss +  # æŠ–åŠ¨æŸå¤±ï¼ˆè¿ç»­åŠ¨ä½œå·®å¼‚ï¼‰
                self.progress_weight * progress_loss
            )
            
            # ğŸš€ CORE INNOVATION: å®‰å…¨é—¨æ§Alphaæ­£åˆ™åŒ–
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨æˆ‘ä»¬çš„åˆ›æ–°é€»è¾‘
            alpha_reg_applied = False
            if self.config.get("use_safety_gated_alpha_reg", False):
                # å¦‚æœå¯ç”¨äº†é—¨æ§æœºåˆ¶
                # åªæœ‰å½“å¹³å‡å®‰å…¨æŸå¤±ä½äºæˆ‘ä»¬è®¾å®šçš„é˜ˆå€¼æ—¶ï¼Œæ‰æ·»åŠ alphaæ­£åˆ™åŒ–æƒ©ç½š
                safety_threshold = self.config.get("safety_loss_threshold", 0.01)
                if total_safety_loss.item() < safety_threshold:
                    total_loss += self.alpha_reg_weight * alpha_regularization_loss
                    alpha_reg_applied = True
                    # è®°å½•æƒ©ç½šè¢«æ¿€æ´»äº†
                    if hasattr(self, 'writer') and self.writer:
                        self.writer.add_scalar('innovation/alpha_reg_activated', 1.0, self.global_step)
                        self.writer.add_scalar('innovation/safety_loss_vs_threshold', total_safety_loss.item(), self.global_step)
                else:
                    # å¦‚æœå®‰å…¨æŸå¤±è¾ƒé«˜ï¼Œåˆ™ä¸æ·»åŠ æƒ©ç½šï¼Œè®©alphaè‡ªç”±å¢å¤§ä»¥ç¡®ä¿å®‰å…¨
                    if hasattr(self, 'writer') and self.writer:
                        self.writer.add_scalar('innovation/alpha_reg_activated', 0.0, self.global_step)
                        self.writer.add_scalar('innovation/safety_loss_vs_threshold', total_safety_loss.item(), self.global_step)
            else:
                # å¦‚æœæ²¡æœ‰å¯ç”¨ï¼Œåˆ™ä½¿ç”¨å¸¸è§„æ–¹å¼ï¼ˆæ€»æ˜¯æ·»åŠ æƒ©ç½šï¼‰
                total_loss += self.alpha_reg_weight * alpha_regularization_loss
                alpha_reg_applied = True
                
            # ğŸš€ CORE INNOVATION: åŠ¨æ€å®‰å…¨è£•åº¦æ­£åˆ™åŒ–æŸå¤±
            margin_regularization_loss = torch.tensor(0.0, device=self.device)
            if self.config.get("use_adaptive_margin", False) and hasattr(self, 'trajectory_margins') and self.trajectory_margins:
                # å®ç°æ ¸å¿ƒçº¦æŸé€»è¾‘ï¼š
                # 1. åŸºç¡€æƒ©ç½šï¼šæˆ‘ä»¬ä¸å¸Œæœ›è£•åº¦å¤ªå°ï¼Œæ‰€ä»¥æƒ©ç½š (æœ€å¤§è£•åº¦ - å½“å‰è£•åº¦)ï¼Œé¼“åŠ±å®ƒå˜å¤§ã€‚
                # 2. å®‰å…¨åŠ æƒï¼šå½“å®‰å…¨æŸå¤±å¾ˆé«˜æ—¶ï¼Œè¿™ç§æƒ©ç½šåº”è¯¥è¢«æ”¾å¤§ã€‚
                
                # å †å æ‰€æœ‰è½¨è¿¹çš„åŠ¨æ€è£•åº¦
                stacked_margins = torch.stack(self.trajectory_margins)
                
                # ä» detached çš„ safety_loss è·å–æƒé‡
                safety_weighting = total_safety_loss.detach() + 0.1
                
                # è®¡ç®—è£•åº¦æŸå¤±ï¼šé¼“åŠ±æ›´å¤§çš„è£•åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸å®‰å…¨æ—¶
                max_margin = self.config.get("max_safety_margin", 0.4)
                margin_cost = max_margin - stacked_margins
                
                margin_regularization_loss = torch.mean(safety_weighting * torch.mean(margin_cost))
                
                # å°†å…¶åŠ å…¥æ€»æŸå¤±
                margin_reg_weight = self.config.get("margin_reg_weight", 0.0)
                total_loss += margin_reg_weight * margin_regularization_loss
            
            # é€šè¿‡æ•´ä¸ªè®¡ç®—å›¾åå‘ä¼ æ’­æŸå¤±
            # å§‹ç»ˆä½¿ç”¨retain_graph=Trueè¿›è¡ŒBPTTä»¥é˜²æ­¢è®¡ç®—å›¾é—®é¢˜
            total_loss.backward(retain_graph=True)
            
            # å‰ªè¾‘æ¢¯åº¦ä»¥é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            parameters = list(self.policy_network.parameters())
            if self.cbf_network is not None:
                parameters += list(self.cbf_network.parameters())
                
            torch.nn.utils.clip_grad_norm_(parameters, self.max_grad_norm)
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            
            # å¦‚æœå¯ç”¨äº†è°ƒåº¦å™¨ï¼Œåˆ™æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler is not None:
                self.scheduler.step()
            
            # è®¡ç®—æ—¥å¿—æŒ‡æ ‡ - åŒ…å«æ–°çš„åŠ é€Ÿåº¦æŸå¤±
            metrics = {
                "train/total_loss": total_loss.item(),
                "train/goal_loss": goal_loss.item(),
                "train/safety_loss": total_safety_loss.item(),
                "train/control_loss": control_effort.item(),
                "train/acceleration_loss": acceleration_loss.item(),  # æ–°å¢ï¼šåŠ é€Ÿåº¦æŸå¤±
                "train/jerk_loss": jerk_loss if isinstance(jerk_loss, float) else jerk_loss.item(),
                "train/alpha_reg_loss": alpha_regularization_loss.item(),
                "train/alpha_reg_applied": float(alpha_reg_applied),  # æ–°å¢ï¼šé—¨æ§çŠ¶æ€
                "train/margin_reg_loss": margin_regularization_loss.item(),  # ğŸš€ NEW: è£•åº¦æ­£åˆ™åŒ–æŸå¤±
                "train/progress_loss": progress_loss if isinstance(progress_loss, float) else progress_loss.item(),
                "train/avg_alpha": torch.mean(stacked_alphas).item(),
                "train/lr": self.optimizer.param_groups[0]['lr'],
                "step": step,
            }
            
            # ğŸš€ CORE INNOVATION: æ·»åŠ åŠ¨æ€è£•åº¦ç›¸å…³æŒ‡æ ‡
            if hasattr(self, 'trajectory_margins') and self.trajectory_margins:
                avg_margin = torch.mean(torch.stack(self.trajectory_margins)).item()
                metrics["train/avg_dynamic_margin"] = avg_margin
            
            # è®°å½•æŒ‡æ ‡
            if WANDB_AVAILABLE:
                wandb.log(metrics)
            
            # è¯„ä¼°å’Œæ¨¡å‹ä¿å­˜
            if (step + 1) % self.eval_interval == 0:
                eval_metrics = self.evaluate()
                if WANDB_AVAILABLE:
                    wandb.log(eval_metrics)
                
                # æ‰“å°è¿›åº¦
                time_elapsed = time.time() - start_time
                print(f"\nStep {step+1}/{self.training_steps}, Time: {time_elapsed:.2f}s")
                print(f"  Total Loss: {total_loss.item():.4f}")
                print(f"  Goal Loss: {goal_loss.item():.4f}")
                print(f"  Safety Loss: {total_safety_loss.item():.4f}")
                print(f"  Control Loss: {control_effort.item():.4f}")
                print(f"  Acceleration Loss: {acceleration_loss.item():.4f}")  # æ–°å¢
                print(f"  Jerk Loss: {jerk_loss if isinstance(jerk_loss, float) else jerk_loss.item():.4f}")
                print(f"  Alpha Reg Loss: {alpha_regularization_loss.item():.4f}")
                print(f"  Alpha Reg Applied: {'Yes' if alpha_reg_applied else 'No'}")  # æ–°å¢ï¼šé—¨æ§çŠ¶æ€
                print(f"  Margin Reg Loss: {margin_regularization_loss.item():.4f}")  # ğŸš€ NEW: è£•åº¦æ­£åˆ™åŒ–æŸå¤±
                print(f"  Progress Loss: {progress_loss if isinstance(progress_loss, float) else progress_loss.item():.4f}")
                print(f"  Avg Alpha: {torch.mean(stacked_alphas).item():.4f}")
                # ğŸš€ CORE INNOVATION: æ˜¾ç¤ºåŠ¨æ€è£•åº¦ä¿¡æ¯
                if hasattr(self, 'trajectory_margins') and self.trajectory_margins:
                    avg_margin = torch.mean(torch.stack(self.trajectory_margins)).item()
                    print(f"  Avg Dynamic Margin: {avg_margin:.4f}")
                print(f"  Evaluation Success Rate: {eval_metrics['eval/success_rate']:.2f}")
                print(f"  Evaluation Collision Rate: {eval_metrics['eval/collision_rate']:.2f}")
            
            # ä¿å­˜æ¨¡å‹
            if (step + 1) % self.save_interval == 0:
                self.save_models(step + 1)
            
            pbar.update(1)
            
            # ğŸš€ CORE INNOVATION: æ¸…ç†è½¨è¿¹è£•åº¦åˆ—è¡¨ä»¥å‡†å¤‡ä¸‹ä¸€ä¸ªè®­ç»ƒæ­¥éª¤
            if hasattr(self, 'trajectory_margins'):
                self.trajectory_margins = []
            # ğŸ›¡ï¸ PROBABILISTIC SAFETY SHIELD: æ¸…ç†å®‰å…¨é˜²æŠ¤ç½©ç›¸å…³æ•°æ®
            if hasattr(self, 'trajectory_alpha_safety'):
                self.trajectory_alpha_safety = []
            if hasattr(self, 'trajectory_collisions'):
                self.trajectory_collisions = []
        
        pbar.close()
        print("Training completed.")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_models(self.training_steps)
        
        # æœ€ç»ˆè¯„ä¼°
        final_metrics = self.evaluate(num_episodes=20)
        print("\nFinal Evaluation Results:")
        print(f"  Success Rate: {final_metrics['eval/success_rate']:.2f}")
        print(f"  Collision Rate: {final_metrics['eval/collision_rate']:.2f}")
        print(f"  Avg Goal Distance: {final_metrics['eval/avg_goal_distance']:.4f}")
        
        return final_metrics
    
    def evaluate(self, num_episodes: int = 10) -> Dict[str, float]:
        """
        è¯„ä¼°å½“å‰ç­–ç•¥å’ŒCBFç½‘ç»œã€‚
        
        å‚æ•°:
            num_episodes: è¯„ä¼°çš„å‰§é›†æ•°é‡
            
        è¿”å›:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        success_count = 0
        collision_count = 0
        avg_goal_distance = 0
        avg_min_cbf = float('inf')
        
        # å°†ç½‘ç»œå’Œç¯å¢ƒè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.policy_network.eval()
        if self.cbf_network is not None:
            self.cbf_network.eval()
        
        # å°†ç¯å¢ƒè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨æ¢¯åº¦è¡°å‡ï¼‰
        if hasattr(self.env, 'eval'):
            self.env.eval()
        
        for _ in range(num_episodes):
            # åˆå§‹åŒ–åœºæ™¯
            state = self.initialize_scenario()
            
            # è¿è¡Œå‰§é›†ï¼Œä¸è·Ÿè¸ªæ¢¯åº¦
            with torch.no_grad():
                # é‡ç½®ç¯å¢ƒ
                current_state = state
                
                # è¿è¡Œå‰å‘ä»¿çœŸ
                for _ in range(self.eval_horizon):
                    # è·å–è§‚æµ‹å€¼
                    observations = self.env.get_observation(current_state)
                    
                    # åœ¨å°†è§‚æµ‹å€¼ä¼ é€’ç»™ç½‘ç»œä¹‹å‰ï¼Œå°†è§‚æµ‹å€¼ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
                    observations = observations.to(self.device)
                    
                    # å¦‚æœæä¾›äº†CBFç½‘ç»œï¼Œåˆ™è·å–CBFå€¼
                    if self.cbf_network is not None:
                        cbf_values = self.cbf_network.barrier_function(current_state)
                        min_cbf_val = cbf_values.min().item()
                        avg_min_cbf = min(avg_min_cbf, min_cbf_val)
                    
                    # ä»ç­–ç•¥ç½‘ç»œè·å–åŠ¨ä½œã€alphaå’ŒåŠ¨æ€è£•åº¦
                    actions, alpha, raw_dynamic_margins = self.policy_network(observations)
                    
                    # ğŸš€ CORE INNOVATION: å¤„ç†åŠ¨æ€å®‰å…¨è£•åº¦ï¼ˆè¯„ä¼°æ—¶ï¼‰
                    dynamic_margins = None
                    if raw_dynamic_margins is not None and self.config.get("use_adaptive_margin", False):
                        min_margin = self.config.get("min_safety_margin", 0.15)
                        max_margin = self.config.get("max_safety_margin", 0.4)
                        # å…³é”®ä¿®å¤ï¼šç¡®ä¿dynamic_marginså¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆè¯„ä¼°æ—¶ï¼‰
                        dynamic_margins = (min_margin + raw_dynamic_margins * (max_margin - min_margin)).to(self.device)
                    
                    # å¤„ç†alphaä¸ºNoneçš„æƒ…å†µï¼ˆå›ºå®šalphaé…ç½®ï¼‰
                    if alpha is None:
                        # ä½¿ç”¨ç¯å¢ƒé»˜è®¤çš„alphaå€¼
                        batch_size, num_agents = actions.shape[:2]
                        alpha = torch.full((batch_size, num_agents, 1), 
                                         self.env.cbf_alpha, 
                                         device=self.device, 
                                         dtype=actions.dtype)
                    
                    # åœ¨ç¯å¢ƒä¸­ä½¿ç”¨åŠ¨æ€alphaè¿›è¡Œä¸€æ­¥
                    step_result = self.env.step(current_state, actions, alpha)
                    next_state = step_result.next_state
                    
                    # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿç¢°æ’
                    if torch.any(step_result.cost > 0):
                        collision_count += 1
                        break
                    
                    # æ›´æ–°çŠ¶æ€
                    current_state = next_state
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡ï¼ˆä½¿ç”¨ç¯å¢ƒçš„ç›®æ ‡è·ç¦»ï¼‰
                goal_distances = self.env.get_goal_distance(current_state)
                avg_distance = goal_distances.mean().item()
                avg_goal_distance += avg_distance
                
                # å¦‚æœæ‰€æœ‰ä»£ç†éƒ½æ¥è¿‘å…¶ç›®æ ‡ï¼Œåˆ™è®¡ä¸ºæˆåŠŸ
                if torch.all(goal_distances < self.env.agent_radius * 2):
                    success_count += 1
        
        # å°†ç½‘ç»œå’Œç¯å¢ƒè®¾ç½®å›è®­ç»ƒæ¨¡å¼
        self.policy_network.train()
        if self.cbf_network is not None:
            self.cbf_network.train()
            
        # å°†ç¯å¢ƒè®¾ç½®å›è®­ç»ƒæ¨¡å¼
        if hasattr(self.env, 'train'):
            self.env.train()
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        success_rate = success_count / num_episodes
        collision_rate = collision_count / num_episodes
        avg_goal_distance /= num_episodes
        
        # å‡†å¤‡è¯„ä¼°æŒ‡æ ‡
        metrics = {
            "eval/success_rate": success_rate,
            "eval/collision_rate": collision_rate,
            "eval/avg_goal_distance": avg_goal_distance,
        }
        
        # å¦‚æœæä¾›äº†CBFç½‘ç»œï¼Œåˆ™æ·»åŠ CBFæŒ‡æ ‡
        if self.cbf_network is not None and avg_min_cbf != float('inf'):
            metrics["eval/avg_min_cbf"] = avg_min_cbf
        
        return metrics
    
    def evaluate_with_logging(self, num_episodes: int = 1, log_episodes: bool = True) -> Dict[str, float]:
        """
        è¯„ä¼°å½“å‰ç­–ç•¥å¹¶è®°å½•è¯¦ç»†çš„episodeæ•°æ®ï¼Œè®¡ç®—å…¨é¢çš„KPIsã€‚
        
        å‚æ•°:
            num_episodes: è¯„ä¼°çš„å‰§é›†æ•°é‡
            log_episodes: æ˜¯å¦è®°å½•episodeæ•°æ®åˆ°æ–‡ä»¶
            
        è¿”å›:
            åŒ…å«è¯¦ç»†KPIsçš„è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        success_count = 0
        collision_count = 0
        timeout_count = 0
        avg_goal_distance = 0
        avg_min_cbf = float('inf')
        episode_files = []
        
        # ğŸ† **NEW: å† å†›è¯„ä¼°ä½“ç³» - KPIç»Ÿè®¡èšåˆå™¨**
        stats_aggregator = {
            'success_episodes': [],      # æˆåŠŸçš„episodeè¯¦ç»†æ•°æ®
            'collision_episodes': [],    # ç¢°æ’çš„episodeè¯¦ç»†æ•°æ®  
            'timeout_episodes': [],      # è¶…æ—¶çš„episodeè¯¦ç»†æ•°æ®
            'all_episodes': []           # æ‰€æœ‰episodeçš„åŸºç¡€ç»Ÿè®¡
        }
        
        # å°†ç½‘ç»œå’Œç¯å¢ƒè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.policy_network.eval()
        if self.cbf_network is not None:
            self.cbf_network.eval()
        
        # å°†ç¯å¢ƒè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨æ¢¯åº¦è¡°å‡ï¼‰
        if hasattr(self.env, 'eval'):
            self.env.eval()
        
        for episode_idx in range(num_episodes):
            print(f"\nğŸ¯ è¿è¡Œè¯„ä¼° Episode {episode_idx + 1}/{num_episodes}")
            
            # åˆå§‹åŒ–åœºæ™¯
            state = self.initialize_scenario()
            
            # åˆå§‹åŒ–æ•°æ®è®°å½•
            episode_logger = None
            if log_episodes and (self.episode_logger is not None or num_episodes == 1):
                if self.episode_logger is not None:
                    episode_logger = self.episode_logger
                else:
                    # ä¸ºå•æ¬¡è¯„ä¼°åˆ›å»ºä¸´æ—¶è®°å½•å™¨
                    log_dir = os.path.join(self.log_dir, 'eval_logs')
                    episode_logger = EpisodeLogger(log_dir=log_dir, prefix="eval_episode")
                
                # å¼€å§‹è®°å½•episode
                episode_id = episode_logger.start_episode(
                    batch_size=state.batch_size,
                    n_agents=state.n_agents,
                    obstacles=state.obstacles,
                    goals=state.goals,
                    safety_radius=getattr(self.env, 'agent_radius', 0.2),
                    area_size=getattr(self.env, 'area_size', 2.0)
                )
            
            # è¿è¡Œå‰§é›†ï¼Œä¸è·Ÿè¸ªæ¢¯åº¦
            episode_status = "TIMEOUT"
            step_count = 0
            
            with torch.no_grad():
                # é‡ç½®ç¯å¢ƒ
                current_state = state
                
                # è¿è¡Œå‰å‘ä»¿çœŸ
                for step in range(self.eval_horizon):
                    step_count = step + 1
                    
                    # è·å–è§‚æµ‹å€¼
                    observations = self.env.get_observation(current_state)
                    
                    # ç¡®ä¿è§‚æµ‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                    if hasattr(observations, 'to'):
                        observations = observations.to(self.get_device())
                    
                    # ä»ç­–ç•¥ç½‘ç»œè·å–åŠ¨ä½œã€alphaå’ŒåŠ¨æ€å®‰å…¨è£•åº¦
                    # ğŸš€ ä¿®å¤ï¼šç­–ç•¥ç½‘ç»œç°åœ¨è¿”å›ä¸‰ä¸ªå€¼ (actions, alpha, dynamic_margins)
                    policy_output = self.policy_network(observations)
                    if len(policy_output) == 3:
                        # æ–°çš„è‡ªé€‚åº”è£•åº¦æ¨¡å‹ï¼šè¿”å› (actions, alpha, dynamic_margins)
                        actions, alpha, dynamic_margins = policy_output
                    else:
                        # æ—§æ¨¡å‹ï¼šåªè¿”å› (actions, alpha)
                        actions, alpha = policy_output
                        dynamic_margins = None
                    
                    # å¦‚æœæä¾›äº†CBFç½‘ç»œï¼Œåˆ™è·å–CBFå€¼
                    h_values = None
                    if self.cbf_network is not None:
                        # CBFç½‘ç»œéœ€è¦stateå’Œå¯é€‰çš„dynamic_marginsä½œä¸ºè¾“å…¥
                        # ğŸš€ ä¿®å¤ï¼šä¼ é€’dynamic_marginsä»¥æ”¯æŒè‡ªé€‚åº”å®‰å…¨è£•åº¦
                        if dynamic_margins is not None:
                            # ç¡®ä¿dynamic_marginsåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                            dynamic_margins = dynamic_margins.to(self.get_device())
                        h_values = self.cbf_network.barrier_function(current_state, dynamic_margins)
                        min_cbf_val = h_values.min().item()
                        avg_min_cbf = min(avg_min_cbf, min_cbf_val)
                    
                    # å¤„ç†alphaä¸ºNoneçš„æƒ…å†µï¼ˆå›ºå®šalphaé…ç½®ï¼‰
                    if alpha is None:
                        # ä½¿ç”¨ç¯å¢ƒé»˜è®¤çš„alphaå€¼
                        batch_size, num_agents = actions.shape[:2]
                        alpha = torch.full((batch_size, num_agents, 1), 
                                         self.cbf_alpha, 
                                         device=self.get_device(), 
                                         dtype=actions.dtype)
                    
                    # åœ¨ç¯å¢ƒä¸­ä½¿ç”¨åŠ¨æ€alphaè¿›è¡Œä¸€æ­¥
                    step_result = self.env.step(current_state, actions, alpha)
                    next_state = step_result.next_state
                    
                    # è®°å½•stepæ•°æ®
                    if episode_logger is not None:
                        # è®¡ç®—æœ€å°è·ç¦»å’Œç›®æ ‡è·ç¦»
                        min_distances = None
                        goal_distances = None
                        
                        if hasattr(current_state, 'obstacles') and current_state.obstacles is not None:
                            obstacles_np = current_state.obstacles.detach().cpu().numpy()
                            positions_np = current_state.positions.detach().cpu().numpy()
                            min_distances = torch.from_numpy(
                                compute_min_distances_to_obstacles(positions_np, obstacles_np)
                            )
                        
                        if hasattr(current_state, 'goals'):
                            positions_np = current_state.positions.detach().cpu().numpy()
                            goals_np = current_state.goals.detach().cpu().numpy()
                            goal_distances = torch.from_numpy(
                                compute_goal_distances(positions_np, goals_np)
                            )
                        
                        # è®°å½•æ•°æ®
                        episode_logger.log_step(
                            positions=current_state.positions,
                            velocities=current_state.velocities,
                            actions=step_result.info.get('action', actions),
                            raw_actions=step_result.info.get('raw_action', actions),
                            alpha_values=alpha,
                            h_values=h_values,
                            min_distances=min_distances,
                            goal_distances=goal_distances,
                            rewards=step_result.reward,
                            costs=step_result.cost
                        )
                    
                    # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿç¢°æ’
                    if torch.any(step_result.cost > 0):
                        collision_count += 1
                        episode_status = "COLLISION"
                        print(f"   âŒ ç¢°æ’å‘ç”Ÿåœ¨ç¬¬ {step + 1} æ­¥")
                        break
                    
                    # æ›´æ–°çŠ¶æ€
                    current_state = next_state
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                    goal_distances = self.env.get_goal_distance(current_state)
                    if torch.all(goal_distances < getattr(self.env, 'agent_radius', 0.2) * 2):
                        success_count += 1
                        episode_status = "SUCCESS"
                        print(f"   âœ… æˆåŠŸå®Œæˆä»»åŠ¡åœ¨ç¬¬ {step + 1} æ­¥")
                        break
                
                # è®¡ç®—æœ€ç»ˆç›®æ ‡è·ç¦»
                final_goal_distances = self.env.get_goal_distance(current_state)
                avg_distance = final_goal_distances.mean().item()
                avg_goal_distance += avg_distance
                
                if episode_status == "TIMEOUT":
                    timeout_count += 1
                    print(f"   â° Episodeè¶…æ—¶ ({self.eval_horizon} æ­¥)")
            
            # ç»“æŸepisodeè®°å½•
            if episode_logger is not None:
                filename = episode_logger.end_episode(episode_status)
                episode_files.append(filename)
                print(f"   ğŸ’¾ Episodeæ•°æ®å·²ä¿å­˜: {filename}")
            
            # ğŸ† **NEW: æ”¶é›†episodeç»Ÿè®¡æ•°æ®åˆ°KPIèšåˆå™¨**
            episode_stats = self._collect_episode_kpis(
                episode_status=episode_status,
                step_count=step_count,
                final_goal_distance=avg_distance,
                min_cbf_value=avg_min_cbf if avg_min_cbf != float('inf') else None,
                episode_file=episode_files[-1] if episode_files else None
            )
            
            # æŒ‰çŠ¶æ€åˆ†ç±»å­˜å‚¨episodeæ•°æ®  
            stats_aggregator['all_episodes'].append(episode_stats)
            if episode_status == "SUCCESS":
                stats_aggregator['success_episodes'].append(episode_stats)
            elif episode_status == "COLLISION":
                stats_aggregator['collision_episodes'].append(episode_stats)
            elif episode_status == "TIMEOUT":
                stats_aggregator['timeout_episodes'].append(episode_stats)
        
        # å°†ç½‘ç»œå’Œç¯å¢ƒè®¾ç½®å›è®­ç»ƒæ¨¡å¼
        self.policy_network.train()
        if self.cbf_network is not None:
            self.cbf_network.train()
            
        # å°†ç¯å¢ƒè®¾ç½®å›è®­ç»ƒæ¨¡å¼
        if hasattr(self.env, 'train'):
            self.env.train()
        
        # ğŸ† **NEW: è®¡ç®—è¯¦ç»†çš„KPIæŒ‡æ ‡**
        kpi_metrics = self._compute_champion_kpis(stats_aggregator, num_episodes)
        
        # ä¼ ç»ŸæŒ‡æ ‡ (å‘åå…¼å®¹)
        success_rate = success_count / num_episodes
        collision_rate = collision_count / num_episodes
        timeout_rate = timeout_count / num_episodes
        avg_goal_distance /= num_episodes
        
        # å‡†å¤‡è¯„ä¼°æŒ‡æ ‡ (ç»“åˆä¼ ç»Ÿå’Œæ–°KPI)
        metrics = {
            "eval/success_rate": success_rate,
            "eval/collision_rate": collision_rate,
            "eval/timeout_rate": timeout_rate,
            "eval/avg_goal_distance": avg_goal_distance,
            "eval/total_episodes": num_episodes,
            "eval/avg_episode_length": step_count,
        }
        
        if self.cbf_network is not None and avg_min_cbf != float('inf'):
            metrics["eval/avg_min_cbf"] = avg_min_cbf
        
        # ğŸ† **NEW: æ·»åŠ è¯¦ç»†KPIåˆ°è¿”å›ç»“æœ**
        metrics.update(kpi_metrics)
        
        # æ·»åŠ episodeæ–‡ä»¶è·¯å¾„ä¿¡æ¯
        if episode_files:
            metrics["eval/episode_files"] = episode_files
            
            # ğŸ† **NEW: æ˜¾ç¤ºå† å†›çº§åˆ«çš„KPIæ€»ç»“**
            self._print_champion_summary(kpi_metrics, episode_files)
        
        return metrics
    
    def _calculate_progress_loss(self, trajectory_states) -> torch.Tensor:
        """
        è¨ˆç®—é€²åº¦æå¤±ï¼ˆåŸºæ–¼æ½œåŠ›çš„å¥–åŠ±å¡‘å½¢ï¼‰ã€‚
        
        åƒæ•¸:
            trajectory_states: è»Œè·¡ç‹€æ…‹åˆ—è¡¨
            
        è¿”å›:
            é€²åº¦æå¤±å¼µé‡
        """
        if len(trajectory_states) < 2:
            return torch.tensor(0.0, device=self.get_device())
        
        # è¨ˆç®—åˆå§‹å’Œæœ€çµ‚ç›®æ¨™è·é›¢
        initial_state = trajectory_states[0]
        final_state = trajectory_states[-1]
        
        initial_distances = self.env.get_goal_distance(initial_state)
        final_distances = self.env.get_goal_distance(final_state)
        
        # é€²åº¦ = åˆå§‹è·é›¢ - æœ€çµ‚è·é›¢ï¼ˆæ­£å€¼è¡¨ç¤ºæœç›®æ¨™å‰é€²ï¼‰
        progress = initial_distances - final_distances
        
        # è² é€²åº¦è¡¨ç¤ºé é›¢ç›®æ¨™ï¼Œæ‡‰è©²è¢«æ‡²ç½°
        progress_loss = -torch.mean(progress)
        
        return progress_loss
    
    def get_device(self) -> torch.device:
        """è·å–è®¾å¤‡ä¿¡æ¯ï¼ˆCPUæˆ–CUDAï¼‰ã€‚"""
        if hasattr(self.policy_network, 'parameters'):
            params = list(self.policy_network.parameters())
            if params:
                return params[0].device
        return torch.device('cpu')
    
    def save_models(self, step: int) -> None:
        """
        ä¿å­˜ç­–ç•¥å’ŒCBFç½‘ç»œæ¨¡å‹ã€‚
        
        å‚æ•°:
            step: å½“å‰è®­ç»ƒæ­¥æ•°
        """
        step_dir = os.path.join(self.model_dir, str(step))
        if not os.path.exists(step_dir):
            os.makedirs(step_dir)
        
        # ä¿å­˜ç­–ç•¥ç½‘ç»œ
        policy_path = os.path.join(step_dir, "policy.pt")
        torch.save(self.policy_network.state_dict(), policy_path)
        
        # å¦‚æœæä¾›äº†CBFç½‘ç»œï¼Œåˆ™ä¿å­˜
        if self.cbf_network is not None:
            cbf_path = os.path.join(step_dir, "cbf.pt")
            torch.save(self.cbf_network.state_dict(), cbf_path)
        
        # ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
        optim_path = os.path.join(step_dir, "optimizer.pt")
        torch.save(self.optimizer.state_dict(), optim_path)
        
        # ä¿å­˜é…ç½®
        config_path = os.path.join(step_dir, "config.pt")
        torch.save(self.config, config_path)
        
        print(f"Models saved at step {step}")
    
    def load_models(self, step: int) -> None:
        """
        åŠ è½½ç­–ç•¥å’ŒCBFç½‘ç»œæ¨¡å‹ã€‚
        
        å‚æ•°:
            step: è¦åŠ è½½çš„è®­ç»ƒæ­¥æ•°
        """
        step_dir = os.path.join(self.model_dir, str(step))
        
        if not os.path.exists(step_dir):
            raise FileNotFoundError(f"No saved models found at step {step}")
        
        # åŠ è½½ç­–ç•¥ç½‘ç»œ
        policy_path = os.path.join(step_dir, "policy.pt")
        if os.path.exists(policy_path):
            self.policy_network.load_state_dict(torch.load(policy_path))
            print(f"Policy network loaded from {policy_path}")
        
        # å¦‚æœæä¾›äº†CBFç½‘ç»œï¼Œåˆ™åŠ è½½
        if self.cbf_network is not None:
            cbf_path = os.path.join(step_dir, "cbf.pt")
            if os.path.exists(cbf_path):
                self.cbf_network.load_state_dict(torch.load(cbf_path))
                print(f"CBF network loaded from {cbf_path}")
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        optim_path = os.path.join(step_dir, "optimizer.pt")
        if os.path.exists(optim_path):
            self.optimizer.load_state_dict(torch.load(optim_path))
            print(f"Optimizer state loaded from {optim_path}")
    
    # ğŸ† **NEW: å† å†›è¯„ä¼°ä½“ç³»çš„KPIæ”¯æŒæ–¹æ³•**
    
    def _collect_episode_kpis(self, episode_status: str, step_count: int, 
                            final_goal_distance: float, min_cbf_value: float = None,
                            episode_file: str = None) -> Dict[str, float]:
        """
        ä¸ºå•ä¸ªepisodeæ”¶é›†è¯¦ç»†çš„KPIç»Ÿè®¡æ•°æ®ã€‚
        
        å‚æ•°:
            episode_status: episodeç»“æŸçŠ¶æ€ ("SUCCESS", "COLLISION", "TIMEOUT")
            step_count: episodeæ€»æ­¥æ•°
            final_goal_distance: æœ€ç»ˆç›®æ ‡è·ç¦»
            min_cbf_value: æœ€å°CBFå€¼
            episode_file: episodeæ•°æ®æ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            åŒ…å«è¯¥episodeæ‰€æœ‰KPIçš„å­—å…¸
        """
        import numpy as np
        
        stats = {
            'status': episode_status,
            'completion_time': step_count,
            'final_goal_distance': final_goal_distance,
            'success': 1 if episode_status == "SUCCESS" else 0,
            'collision': 1 if episode_status == "COLLISION" else 0,
            'timeout': 1 if episode_status == "TIMEOUT" else 0,
            'episode_file': episode_file
        }
        
        if min_cbf_value is not None:
            stats['min_safety_distance'] = min_cbf_value
            
        # å¦‚æœæœ‰episodeæ–‡ä»¶ï¼Œå°è¯•åŠ è½½æ›´è¯¦ç»†çš„ç»Ÿè®¡
        if episode_file and os.path.exists(episode_file):
            try:
                from gcbfplus.utils.episode_logger import load_episode_data
                episode_data = load_episode_data(episode_file)
                
                # è®¡ç®—å¹³å‡åŠ é€Ÿåº¦å’ŒæŠ–åŠ¨ (jerk)
                if 'actions' in episode_data and episode_data['actions'] is not None:
                    actions = episode_data['actions']
                    if len(actions) > 1:
                        # è®¡ç®—åŠ é€Ÿåº¦ (action differences)
                        accelerations = np.diff(actions, axis=0)
                        avg_acceleration = np.mean(np.linalg.norm(accelerations, axis=-1))
                        stats['avg_acceleration'] = float(avg_acceleration)
                        
                        # è®¡ç®—æŠ–åŠ¨ (acceleration differences)
                        if len(accelerations) > 1:
                            jerks = np.diff(accelerations, axis=0)
                            avg_jerk = np.mean(np.linalg.norm(jerks, axis=-1))
                            stats['avg_jerk'] = float(avg_jerk)
                
                # è®¡ç®—æœ€å°å®‰å…¨è·ç¦»ç»Ÿè®¡
                if 'min_distances' in episode_data and episode_data['min_distances'] is not None:
                    min_distances = episode_data['min_distances']
                    if len(min_distances) > 0:
                        stats['min_safety_distance'] = float(np.min(min_distances))
                        stats['avg_safety_distance'] = float(np.mean(min_distances))
                        stats['safety_violations'] = int(np.sum(min_distances < 0.1))  # è¿è§„æ¬¡æ•°
                        
            except Exception as e:
                print(f"Warning: Failed to extract detailed stats from {episode_file}: {e}")
        
        return stats
    
    def _compute_champion_kpis(self, stats_aggregator: Dict, num_episodes: int) -> Dict[str, float]:
        """
        åŸºäºèšåˆçš„ç»Ÿè®¡æ•°æ®è®¡ç®—æœ€ç»ˆçš„å† å†›çº§åˆ«KPIsã€‚
        
        å‚æ•°:
            stats_aggregator: åŒ…å«æ‰€æœ‰episodeç»Ÿè®¡çš„èšåˆå™¨
            num_episodes: æ€»episodeæ•°
            
        è¿”å›:
            åŒ…å«æ‰€æœ‰å† å†›KPIçš„å­—å…¸
        """
        import numpy as np
        
        kpis = {}
        
        # åŸºç¡€æˆåŠŸç‡ç»Ÿè®¡
        success_episodes = stats_aggregator['success_episodes']
        collision_episodes = stats_aggregator['collision_episodes']
        timeout_episodes = stats_aggregator['timeout_episodes']
        all_episodes = stats_aggregator['all_episodes']
        
        kpis['champion/success_rate'] = len(success_episodes) / num_episodes
        kpis['champion/collision_rate'] = len(collision_episodes) / num_episodes
        kpis['champion/timeout_rate'] = len(timeout_episodes) / num_episodes
        
        # ğŸ† æˆåŠŸæ¡ˆä¾‹çš„è¯¦ç»†KPIs
        if success_episodes:
            completion_times = [ep['completion_time'] for ep in success_episodes]
            kpis['champion/avg_completion_time_success'] = np.mean(completion_times)
            kpis['champion/std_completion_time_success'] = np.std(completion_times)
            kpis['champion/min_completion_time'] = np.min(completion_times)
            kpis['champion/max_completion_time'] = np.max(completion_times)
            
            # æŠ–åŠ¨ç»Ÿè®¡ (ä»…æˆåŠŸæ¡ˆä¾‹)
            jerks = [ep.get('avg_jerk', 0) for ep in success_episodes if 'avg_jerk' in ep]
            if jerks:
                kpis['champion/avg_jerk_success'] = np.mean(jerks)
                kpis['champion/std_jerk_success'] = np.std(jerks)
            
            # å®‰å…¨è·ç¦»ç»Ÿè®¡ (ä»…æˆåŠŸæ¡ˆä¾‹)
            safety_dists = [ep.get('min_safety_distance', float('inf')) for ep in success_episodes if 'min_safety_distance' in ep]
            if safety_dists:
                kpis['champion/avg_min_safety_distance_success'] = np.mean(safety_dists)
                kpis['champion/std_min_safety_distance_success'] = np.std(safety_dists)
                
            # å®‰å…¨è¿è§„ç»Ÿè®¡
            violations = [ep.get('safety_violations', 0) for ep in success_episodes if 'safety_violations' in ep]
            if violations:
                kpis['champion/avg_safety_violations_success'] = np.mean(violations)
        
        # ğŸ¥‡ å¯»æ‰¾æœ€ä½³episode (æˆåŠŸæ¡ˆä¾‹ä¸­æœ€çŸ­æ—¶é—´)
        if success_episodes:
            best_episode = min(success_episodes, key=lambda x: x['completion_time'])
            kpis['champion/best_episode_file'] = best_episode.get('episode_file', 'unknown')
            kpis['champion/best_completion_time'] = best_episode['completion_time']
            kpis['champion/best_episode_jerk'] = best_episode.get('avg_jerk', 0)
            kpis['champion/best_episode_safety'] = best_episode.get('min_safety_distance', float('inf'))
        
        # ğŸ”¥ æ•´ä½“é²æ£’æ€§æŒ‡æ ‡
        all_completion_times = [ep['completion_time'] for ep in all_episodes]
        kpis['champion/avg_episode_length'] = np.mean(all_completion_times)
        kpis['champion/robustness_score'] = kpis['champion/success_rate'] * (1 - kpis['champion/collision_rate'])
        
        return kpis
    
    def _print_champion_summary(self, kpi_metrics: Dict[str, float], episode_files: list) -> None:
        """
        æ‰“å°å† å†›çº§åˆ«çš„KPIæ€»ç»“ã€‚
        
        å‚æ•°:
            kpi_metrics: è®¡ç®—å¥½çš„KPIæŒ‡æ ‡
            episode_files: episodeæ–‡ä»¶åˆ—è¡¨
        """
        print(f"\nğŸ† â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print(f"ğŸ†           å† å†›è¯„ä¼°ä½“ç³» - KPIæ€»ç»“æŠ¥å‘Š           ")  
        print(f"ğŸ† â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        # åŸºç¡€æ€§èƒ½æŒ‡æ ‡
        print(f"ğŸ“Š åŸºç¡€æ€§èƒ½:")
        print(f"   âœ… æˆåŠŸç‡: {kpi_metrics.get('champion/success_rate', 0):.1%}")
        print(f"   âŒ ç¢°æ’ç‡: {kpi_metrics.get('champion/collision_rate', 0):.1%}")
        print(f"   â° è¶…æ—¶ç‡: {kpi_metrics.get('champion/timeout_rate', 0):.1%}")
        print(f"   ğŸ›¡ï¸ é²æ£’æ€§å¾—åˆ†: {kpi_metrics.get('champion/robustness_score', 0):.3f}")
        
        # æˆåŠŸæ¡ˆä¾‹åˆ†æ
        if 'champion/avg_completion_time_success' in kpi_metrics:
            print(f"\nğŸ¯ æˆåŠŸæ¡ˆä¾‹åˆ†æ:")
            print(f"   â±ï¸ å¹³å‡å®Œæˆæ—¶é—´: {kpi_metrics['champion/avg_completion_time_success']:.1f} Â± {kpi_metrics.get('champion/std_completion_time_success', 0):.1f} æ­¥")
            print(f"   ğŸš€ æœ€ä½³å®Œæˆæ—¶é—´: {kpi_metrics.get('champion/min_completion_time', 0):.0f} æ­¥")
            print(f"   ğŸ­ æœ€å·®å®Œæˆæ—¶é—´: {kpi_metrics.get('champion/max_completion_time', 0):.0f} æ­¥")
            
            if 'champion/avg_jerk_success' in kpi_metrics:
                print(f"   ğŸ“ˆ å¹³å‡æŠ–åŠ¨: {kpi_metrics['champion/avg_jerk_success']:.4f} Â± {kpi_metrics.get('champion/std_jerk_success', 0):.4f}")
                
            if 'champion/avg_min_safety_distance_success' in kpi_metrics:
                print(f"   ğŸ›¡ï¸ å¹³å‡å®‰å…¨è·ç¦»: {kpi_metrics['champion/avg_min_safety_distance_success']:.3f} Â± {kpi_metrics.get('champion/std_min_safety_distance_success', 0):.3f}")
        
        # æœ€ä½³episodeä¿¡æ¯
        if 'champion/best_episode_file' in kpi_metrics:
            print(f"\nğŸ¥‡ å† å†›Episode:")
            print(f"   ğŸ“ æ–‡ä»¶: {os.path.basename(kpi_metrics['champion/best_episode_file'])}")
            print(f"   â±ï¸ å®Œæˆæ—¶é—´: {kpi_metrics.get('champion/best_completion_time', 0):.0f} æ­¥")
            print(f"   ğŸ“ˆ æŠ–åŠ¨å€¼: {kpi_metrics.get('champion/best_episode_jerk', 0):.4f}")
            print(f"   ğŸ›¡ï¸ å®‰å…¨è·ç¦»: {kpi_metrics.get('champion/best_episode_safety', float('inf')):.3f}")
        
        print(f"\nğŸ’¾ æ•°æ®æ–‡ä»¶: {len(episode_files)} ä¸ªepisodeå·²ä¿å­˜")
        print(f"ğŸ† â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        