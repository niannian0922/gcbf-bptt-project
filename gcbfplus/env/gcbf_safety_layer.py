import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Tuple, Optional, Callable, Any, List, Union
from dataclasses import dataclass

from .multi_agent_env import MultiAgentState


class GCBFSafetyLayer(nn.Module):
    """
    ðŸ›¡ï¸ PROBABILISTIC SAFETY SHIELD æ¦‚çŽ‡å®‰å…¨é˜²æŠ¤ç½©
    
    é‡æž„ä¸ºæ¦‚çŽ‡å®‰å…¨é˜²æŠ¤ç½©ï¼Œè¾“å‡ºå®‰å…¨ä¿¡å¿ƒåˆ†æ•° alpha_safety (0-1èŒƒå›´)ã€‚
    ä¸å†ç›´æŽ¥è¿‡æ»¤åŠ¨ä½œï¼Œè€Œæ˜¯ä½œä¸º"æ˜Žæ™ºé¡¾é—®"ï¼ŒåŸºäºŽCBFå€¼è¯„ä¼°å®‰å…¨çŠ¶å†µã€‚
    æ”¯æŒè‡ªé€‚åº”å®‰å…¨è¾¹è·å’ŒåŠ¨æ€Alphaæœºåˆ¶ã€‚
    
    æ ¸å¿ƒåˆ›æ–°ï¼šè§£è€¦"å®‰å…¨"å’Œ"æ•ˆçŽ‡"ç›®æ ‡ï¼Œå…è®¸ç­–ç•¥ç½‘ç»œåœ¨å®‰å…¨åŒºåŸŸè‡ªç”±æŽ¢ç´¢ï¼Œ
    åŒæ—¶åœ¨å±é™©æƒ…å†µä¸‹æä¾›å®‰å…¨å›žé€€ä¿è¯ã€‚
    """
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–å®‰å…¨å±‚ã€‚
        
        å‚æ•°:
            config: åŒ…å«å®‰å…¨å±‚å‚æ•°çš„å­—å…¸
                å¿…éœ€é”®å€¼:
                - 'alpha': CBFå‚æ•°alpha (h_dot + alpha * h >= 0)
                - 'eps': æ•°å€¼ç¨³å®šæ€§çš„å°æ­£å‚æ•°
                - 'safety_margin': ç¢°æ’žé¿å…çš„å®‰å…¨è·ç¦»è¾¹è·
                å¯é€‰é”®å€¼:
                - 'use_qp': æ˜¯å¦ä½¿ç”¨QPæ±‚è§£å™¨ï¼ˆå¦åˆ™ä½¿ç”¨ç®€å•æŠ•å½±ï¼‰
                - 'qp_relaxation_weight': çº¦æŸæ¾å¼›æƒé‡
                - 'max_iterations': æ±‚è§£å™¨çš„æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        super(GCBFSafetyLayer, self).__init__()
        
        # CBFå‚æ•°
        self.alpha = config.get('alpha', 1.0)
        self.eps = config.get('eps', 0.02)
        self.safety_margin = config.get('safety_margin', 0.05)
        
        # QPå‚æ•°
        self.use_qp = config.get('use_qp', True)
        self.qp_relaxation_weight = config.get('qp_relaxation_weight', 10.0)
        self.max_iterations = config.get('max_iterations', 10)
        
        # æ³¨å†Œå‚æ•°
        self.register_buffer('alpha_tensor', torch.tensor([self.alpha], dtype=torch.float32))
        
        # ðŸ›¡ï¸ PROBABILISTIC SAFETY SHIELD: å®‰å…¨ä¿¡å¿ƒåˆ†æ•°çš„è¶…å‚æ•°
        self.k = config.get('safety_sharpness', 1.0)  # æŽ§åˆ¶è¿‡æ¸¡çš„é”åˆ©åº¦
        
    def barrier_function(self, state: MultiAgentState, dynamic_margins: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—æ™ºèƒ½ä½“é—´å’Œæ™ºèƒ½ä½“-éšœç¢ç‰©é—´çš„å±éšœå‡½æ•°å€¼ã€‚
        
        å‚æ•°:
            state: å¤šæ™ºèƒ½ä½“çŠ¶æ€ï¼ŒåŒ…å«ä½ç½®ã€é€Ÿåº¦å’Œéšœç¢ç‰©ä¿¡æ¯
            
        è¿”å›ž:
            å±éšœå‡½æ•°å€¼å¼ é‡ [batch, n_agents, n_constraints]
        """
        batch_size, n_agents, pos_dim = state.positions.shape
        
        # è®¡ç®—æ™ºèƒ½ä½“é—´å±éšœå‡½æ•°ï¼ˆç¢°æ’žé¿å…ï¼‰
        # å¯¹äºŽæ¯å¯¹æ™ºèƒ½ä½“ï¼Œh(x) = ||p_i - p_j||^2 - (2r)^2ï¼Œå…¶ä¸­ræ˜¯æ™ºèƒ½ä½“åŠå¾„
        
        # è®¡ç®—ä½ç½®é—´çš„æˆå¯¹å·®å¼‚
        # å½¢çŠ¶: [batch, n_agents, n_agents, pos_dim]
        pos_diff = state.positions.unsqueeze(2) - state.positions.unsqueeze(1)
        
        # è®¡ç®—å¹³æ–¹è·ç¦»
        # å½¢çŠ¶: [batch, n_agents, n_agents]
        dist_squared = torch.sum(pos_diff**2, dim=-1)
        
        # ðŸš€ CORE INNOVATION: ä½¿ç”¨åŠ¨æ€å®‰å…¨è£•åº¦ï¼ˆå¦‚æžœæä¾›ï¼‰
        agent_radius = getattr(state, 'agent_radius', 0.05)  # é»˜è®¤åŠå¾„
        
        # ä»Žå·²çŸ¥åœ¨GPUä¸Šçš„å¼ é‡èŽ·å–æ­£ç¡®çš„è®¾å¤‡ä¿¡æ¯
        device = dist_squared.device
        
        if dynamic_margins is not None:
            # ä½¿ç”¨åŠ¨æ€å®‰å…¨è£•åº¦ï¼š[batch_size, n_agents, 1] -> [batch_size, n_agents, n_agents]
            # å¯¹äºŽæ™ºèƒ½ä½“iå’Œjçš„äº¤äº’ï¼Œä½¿ç”¨ä¸¤è€…è£•åº¦çš„å¹³å‡å€¼
            margins_i = dynamic_margins.unsqueeze(2)  # [batch, n_agents, 1, 1]
            margins_j = dynamic_margins.unsqueeze(1)  # [batch, 1, n_agents, 1]
            avg_margins = (margins_i + margins_j) / 2.0  # [batch, n_agents, n_agents, 1]
            avg_margins = avg_margins.squeeze(-1)  # [batch, n_agents, n_agents]
            
            # è®¡ç®—åŠ¨æ€é˜ˆå€¼ (2 * radius + dynamic_margin)^2
            # å…³é”®ä¿®å¤ï¼šç¡®ä¿thresholdå¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            threshold = ((2 * agent_radius + avg_margins)**2).to(device)
        else:
            # ä½¿ç”¨å›ºå®šå®‰å…¨è£•åº¦
            # å…³é”®ä¿®å¤ï¼šç¡®ä¿thresholdå¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            threshold = torch.tensor((2 * agent_radius + self.safety_margin)**2, device=device)
        
        # åˆ›å»ºå±éšœå€¼: h(x) = dist_squared - threshold
        # å½¢çŠ¶: [batch, n_agents, n_agents]
        h_agent = dist_squared - threshold
        
        # å°†å¯¹è§’çº¿è®¾ç½®ä¸ºå¤§å€¼ï¼ˆæ— è‡ªç¢°æ’žï¼‰
        mask = torch.eye(n_agents, device=h_agent.device, dtype=torch.bool)
        h_agent = h_agent.masked_fill(mask.unsqueeze(0), float('inf'))
        
        # å¦‚æžœå­˜åœ¨éšœç¢ç‰©ï¼Œè®¡ç®—æ™ºèƒ½ä½“-éšœç¢ç‰©å±éšœå‡½æ•°
        if hasattr(state, 'obstacles') and state.obstacles is not None:
            # æå–éšœç¢ç‰©ä½ç½®å’ŒåŠå¾„
            obstacle_positions = state.obstacles[..., :-1]  # [batch, n_obs, pos_dim]
            obstacle_radii = state.obstacles[..., -1:]     # [batch, n_obs, 1]
            
            # è®¡ç®—æ™ºèƒ½ä½“ä¸Žéšœç¢ç‰©ä½ç½®ä¹‹é—´çš„å·®å¼‚
            # å½¢çŠ¶: [batch, n_agents, n_obs, pos_dim]
            agent_obs_diff = state.positions.unsqueeze(2) - obstacle_positions.unsqueeze(1)
            
            # è®¡ç®—å¹³æ–¹è·ç¦»
            # å½¢çŠ¶: [batch, n_agents, n_obs]
            agent_obs_dist_squared = torch.sum(agent_obs_diff**2, dim=-1)
            
            # è®¡ç®—é˜ˆå€¼: (agent_radius + obstacle_radius + margin)^2
            # å½¢çŠ¶: [batch, 1, n_obs]
            # å…³é”®ä¿®å¤ï¼šç¡®ä¿obs_thresholdå¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            obs_threshold = ((agent_radius + obstacle_radii.squeeze(-1).unsqueeze(1) + self.safety_margin)**2).to(device)
            
            # åˆ›å»ºå±éšœå€¼: h(x) = dist_squared - threshold
            # å½¢çŠ¶: [batch, n_agents, n_obs]
            h_obstacle = agent_obs_dist_squared - obs_threshold
            
            # ç»„åˆæ™ºèƒ½ä½“-æ™ºèƒ½ä½“å’Œæ™ºèƒ½ä½“-éšœç¢ç‰©çº¦æŸ
            # å½¢çŠ¶: [batch, n_agents, n_agents + n_obs]
            h = torch.cat([h_agent, h_obstacle], dim=-1)
        else:
            # ä»…æ™ºèƒ½ä½“-æ™ºèƒ½ä½“çº¦æŸ
            h = h_agent
            
        return h
    
    def barrier_jacobian(self, state: MultiAgentState) -> torch.Tensor:
        """
        Compute the Jacobian (gradient) of the barrier function with respect to states.
        
        Args:
            state: Current environment state
            
        Returns:
            Jacobian tensor [batch_size, n_agents, n_constraints, state_dim]
        """
        # æˆ‘ä»¬éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œå› æ­¤å¯ç”¨è‡ªåŠ¨å¾®åˆ†
        batch_size = state.batch_size
        n_agents = state.positions.shape[1]
        pos_dim = state.positions.shape[2]
        device = state.positions.device
        
        # ä¸ºè‡ªåŠ¨å¾®åˆ†åˆ›å»ºè®¡ç®—å›¾
        positions = state.positions.clone().requires_grad_(True)
        
        # è®¡ç®—ä½ç½®é—´çš„æˆå¯¹å·®å¼‚
        pos_diff = positions.unsqueeze(2) - positions.unsqueeze(1)
        
        # è®¡ç®—å¹³æ–¹è·ç¦»
        dist_squared = torch.sum(pos_diff**2, dim=3)
        
        # è®¡ç®—é˜ˆå€¼
        threshold = (2 * (self.safety_margin + 0.05))**2
        
        # åˆ›å»ºå±éšœå€¼: h(x) = dist_squared - threshold
        h_agent_agent = dist_squared - threshold
        
        # å°†å¯¹è§’çº¿è®¾ç½®ä¸ºå¤§å€¼ï¼ˆæ— è‡ªç¢°æ’žï¼‰
        mask = torch.eye(n_agents, device=device).unsqueeze(0).expand(batch_size, -1, -1)
        h_agent_agent = h_agent_agent.masked_fill(mask == 1, 1000.0)
        
        # èŽ·å–çº¦æŸæ•°é‡
        if state.obstacles is not None:
            n_obs = state.obstacles.shape[1]
            n_constraints = n_agents + n_obs
        else:
            n_constraints = n_agents
        
        # åˆå§‹åŒ–é›…å¯æ¯”çŸ©é˜µå¼ é‡
        # ä¸ºç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬åªè®¡ç®—ç›¸å¯¹äºŽä½ç½®çš„æ¢¯åº¦
        # å®Œæ•´ç‰ˆæœ¬åº”åŒ…å«é€Ÿåº¦
        jacobian = torch.zeros(batch_size, n_agents, n_constraints, pos_dim*2, device=device)
        
        # å¯¹äºŽæ¯ä¸ªæ™ºèƒ½ä½“å’Œæ¯ä¸ªçº¦æŸï¼Œè®¡ç®—æ¢¯åº¦
        for i in range(n_agents):
            for j in range(n_agents):
                if i != j:
                    # è®¡ç®—ç›¸å¯¹äºŽä½ç½®çš„h_ijæ¢¯åº¦
                    # æˆ‘ä»¬æ­£åœ¨è®¡ç®—âˆ‚h_ij/âˆ‚p_iå’Œâˆ‚h_ij/âˆ‚p_j
                    grad_outputs = torch.zeros_like(h_agent_agent)
                    grad_outputs[:, i, j] = 1.0
                    
                    # ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†èŽ·å–æ¢¯åº¦
                    grads = torch.autograd.grad(
                        outputs=h_agent_agent,
                        inputs=positions,
                        grad_outputs=grad_outputs,
                        retain_graph=True,
                        create_graph=False,
                        allow_unused=True
                    )[0]
                    
                    # åœ¨é›…å¯æ¯”çŸ©é˜µå¼ é‡ä¸­å­˜å‚¨æ¢¯åº¦
                    # å¯¹äºŽæ™ºèƒ½ä½“iï¼Œçº¦æŸjï¼ˆæ¥è‡ªæ™ºèƒ½ä½“jï¼‰
                    jacobian[:, i, j, :pos_dim] = grads[:, i]
        
        # å¦‚æžœæœ‰éšœç¢ç‰©ï¼Œè®¡ç®—æ™ºèƒ½ä½“-éšœç¢ç‰©çº¦æŸçš„æ¢¯åº¦
        if state.obstacles is not None:
            # æå–éšœç¢ç‰©ä½ç½®å’ŒåŠå¾„
            obstacle_positions = state.obstacles[..., :-1]  # [batch, n_obs, pos_dim]
            obstacle_radii = state.obstacles[..., -1:]     # [batch, n_obs, 1]
            
            # è®¡ç®—æ™ºèƒ½ä½“ä¸Žéšœç¢ç‰©ä½ç½®ä¹‹é—´çš„å·®å¼‚
            obs_diff = positions.unsqueeze(2) - obstacle_positions.unsqueeze(1)
            
            # è®¡ç®—å¹³æ–¹è·ç¦»
            obs_dist_squared = torch.sum(obs_diff**2, dim=3)
            
            # è®¡ç®—é˜ˆå€¼
            obs_threshold = (0.05 + obstacle_radii.squeeze(-1).unsqueeze(1) + self.safety_margin)**2
            
            # åˆ›å»ºå±éšœå€¼: h(x) = dist_squared - threshold
            h_agent_obs = obs_dist_squared - obs_threshold
            
            # å¯¹äºŽæ¯ä¸ªæ™ºèƒ½ä½“å’Œæ¯ä¸ªéšœç¢ç‰©ï¼Œè®¡ç®—æ¢¯åº¦
            for i in range(n_agents):
                for j in range(n_obs):
                    # è®¡ç®—ç›¸å¯¹äºŽä½ç½®çš„h_ijæ¢¯åº¦
                    grad_outputs = torch.zeros_like(h_agent_obs)
                    grad_outputs[:, i, j] = 1.0
                    
                    # ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†èŽ·å–æ¢¯åº¦
                    grads = torch.autograd.grad(
                        outputs=h_agent_obs,
                        inputs=positions,
                        grad_outputs=grad_outputs,
                        retain_graph=True,
                        create_graph=False,
                        allow_unused=True
                    )[0]
                    
                    # åœ¨é›…å¯æ¯”çŸ©é˜µå¼ é‡ä¸­å­˜å‚¨æ¢¯åº¦
                    # å¯¹äºŽæ™ºèƒ½ä½“iï¼Œçº¦æŸn_agents+jï¼ˆæ¥è‡ªéšœç¢ç‰©jï¼‰
                    jacobian[:, i, n_agents+j, :pos_dim] = grads[:, i]
        
        return jacobian
    
    def control_affine_dynamics(self, state: MultiAgentState) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è®¡ç®—æŽ§åˆ¶ä»¿å°„åŠ¨åŠ›å­¦çŸ©é˜µf(x)å’Œg(x)ã€‚
        
        å¯¹äºŽåŒç§¯åˆ†å™¨ç³»ç»Ÿ:
        dx/dt = f(x) + g(x)u
        
        å…¶ä¸­f(x) = [vx, vy, 0, 0]^T
        ä¸”g(x) = [0, 0; 0, 0; 1/m, 0; 0, 1/m]
        
        å‚æ•°:
            state: å½“å‰çŽ¯å¢ƒçŠ¶æ€
            
        è¿”å›ž:
            (f, g)çš„å…ƒç»„ï¼Œå…¶ä¸­:
            - f: æ¼‚ç§»é¡¹ [batch_size, n_agents, state_dim]
            - g: æŽ§åˆ¶è¾“å…¥é¡¹ [batch_size, n_agents, state_dim, action_dim]
        """
        batch_size = state.batch_size
        n_agents = state.positions.shape[1]
        pos_dim = state.positions.shape[2]
        device = state.positions.device
        
        # å¯¹äºŽçŠ¶æ€ä¸º[x, y, vx, vy]çš„åŒç§¯åˆ†å™¨ï¼Œæˆ‘ä»¬æœ‰:
        # dx/dt = vx
        # dy/dt = vy
        # dvx/dt = 1/m * fx
        # dvy/dt = 1/m * fy
        
        # æ¼‚ç§»é¡¹f(x) = [vx, vy, 0, 0]^T
        f = torch.zeros(batch_size, n_agents, 2*pos_dim, device=device)
        f[:, :, :pos_dim] = state.velocities  # ä½ç½®å¯¼æ•° = é€Ÿåº¦
        
        # æŽ§åˆ¶è¾“å…¥é¡¹g(x) = [0, 0; 0, 0; 1/m, 0; 0, 1/m]
        g = torch.zeros(batch_size, n_agents, 2*pos_dim, pos_dim, device=device)
        
        # é»˜è®¤è´¨é‡ = 1.0 å¦‚æžœæœªæŒ‡å®š
        m = 0.1  # é»˜è®¤è´¨é‡
        
        # è®¾ç½®æŽ§åˆ¶çŸ©é˜µ - æ¯ä¸ªåŠ›åˆ†é‡åªå½±å“å…¶å¯¹åº”çš„é€Ÿåº¦
        for i in range(pos_dim):
            g[:, :, pos_dim+i, i] = 1.0 / m
        
        return f, g
    
    def compute_safety_confidence(
        self, 
        state: MultiAgentState, 
        dynamic_margins: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        ðŸ›¡ï¸ PROBABILISTIC SAFETY SHIELD: è¨ˆç®—å®‰å…¨ä¿¡å¿ƒåˆ†æ•¸
        
        åŸºæ–¼å±éšœå‡½æ•¸å€¼è¨ˆç®—alpha_safetyåˆ†æ•¸ï¼Œä½¿ç”¨sigmoidå‡½æ•¸å°‡h(x)æ˜ å°„åˆ°[0,1]ç¯„åœã€‚
        
        Args:
            state: ç•¶å‰ç’°å¢ƒç‹€æ…‹
            dynamic_margins: å‹•æ…‹å®‰å…¨è£•åº¦ [batch_size, n_agents, 1] (å¯é¸)
            
        Returns:
            alpha_safety: å®‰å…¨ä¿¡å¿ƒåˆ†æ•¸ [batch_size, n_agents, 1]
                         0 = æ¥µåº¦å±éšªï¼Œ1 = å®Œå…¨å®‰å…¨
        """
        # è¨ˆç®—å±éšœå‡½æ•¸å€¼
        h_val = self.barrier_function(state, dynamic_margins)
        
        # å–æœ€å°å€¼ï¼ˆæœ€å±éšªçš„ç´„æŸï¼‰ä¾†ä»£è¡¨æ•´é«”å®‰å…¨ç‹€æ³
        # h_val: [batch_size, n_agents, n_constraints]
        min_h_val, _ = torch.min(h_val, dim=2)  # [batch_size, n_agents]
        
        # ä½¿ç”¨sigmoidå‡½æ•¸å°‡hå€¼æ˜ å°„åˆ°[0,1]ç¯„åœ
        # h > 0 å°æ‡‰å®‰å…¨ç‹€æ…‹ï¼ˆæ¨™æº–CBFç´„å®šï¼‰
        # kæŽ§åˆ¶éŽæ¸¡çš„éŠ³åˆ©åº¦
        alpha_safety = torch.sigmoid(self.k * min_h_val)
        
        # ç¢ºä¿è¼¸å‡ºå½¢ç‹€ç‚º [batch_size, n_agents, 1]
        alpha_safety = alpha_safety.unsqueeze(-1)
        
        return alpha_safety
    
    def forward(
        self, 
        state: MultiAgentState, 
        raw_action: torch.Tensor, 
        alphas: Optional[torch.Tensor] = None,
        dynamic_margins: Optional[torch.Tensor] = None,
        dynamics_fn: Optional[Callable] = None
    ) -> torch.Tensor:
        """
        Apply CBF-based safety filtering to raw actions.
        
        Args:
            state: Current environment state
            raw_action: Raw actions from policy [batch_size, n_agents, action_dim]
            alphas: Dynamic CBF alpha values [batch_size, n_agents, 1] (optional)
            dynamic_margins: Dynamic safety margins [batch_size, n_agents, 1] (optional)
            dynamics_fn: Optional function to compute control-affine dynamics
            
        Returns:
            Safe actions [batch_size, n_agents, action_dim]
        """
        # ðŸš€ CORE INNOVATION: Compute barrier function values with dynamic margins
        h = self.barrier_function(state, dynamic_margins)
        
        # Compute barrier function Jacobian
        dh_dx = self.barrier_jacobian(state)
        
        # Compute control-affine dynamics
        if dynamics_fn is not None:
            f, g = dynamics_fn(state)
        else:
            f, g = self.control_affine_dynamics(state)
        
        # Compute Lie derivatives
        # L_f h = dh/dx * f(x)
        # L_g h = dh/dx * g(x)
        
        # Compute L_f h (drift term): dh/dx * f(x)
        # [batch, n_agents, n_constraints, state_dim] x [batch, n_agents, state_dim]
        # -> [batch, n_agents, n_constraints]
        L_f_h = torch.sum(dh_dx * f.unsqueeze(2), dim=3)
        
        # Compute L_g h (control term): dh/dx * g(x)
        # [batch, n_agents, n_constraints, state_dim] x [batch, n_agents, state_dim, action_dim]
        # -> [batch, n_agents, n_constraints, action_dim]
        L_g_h = torch.matmul(dh_dx.view(*dh_dx.shape[:-1], 1, -1), 
                            g.view(*g.shape[:-2], -1, g.shape[-1]))
        L_g_h = L_g_h.squeeze(-2)
        
        # CBF constraint: L_f h + L_g h * u + alpha * h >= 0
        # Rearranging: L_g h * u >= -L_f h - alpha * h
        
        # Use dynamic alphas if provided, otherwise use fixed alpha
        if alphas is not None:
            # Ensure alphas are on the same device and have correct shape
            alphas = alphas.to(h.device)
            # Broadcast alphas to match h shape if needed
            # alphas: [batch, n_agents, 1] -> [batch, n_agents, n_constraints]
            alpha_values = alphas.squeeze(-1).unsqueeze(-1).expand_as(h)
        else:
            alpha_values = self.alpha
        
        # Right-hand side of constraint: -L_f h - alpha * h
        rhs = -L_f_h - alpha_values * h
        
        # If using QP solver
        if self.use_qp:
            # Solve quadratic program for each agent
            safe_action = raw_action.clone()
            
            batch_size = state.batch_size
            n_agents = state.positions.shape[1]
            
            # Process each batch and agent separately
            for b in range(batch_size):
                for i in range(n_agents):
                    # Skip if no constraints for this agent
                    if L_g_h[b, i].shape[0] == 0:
                        continue
                    
                    # QP formulation:
                    # min_u 0.5 * (u - u_raw)^T * (u - u_raw)
                    # s.t. L_g_h * u >= rhs
                    
                    # Filter constraints to include only active ones
                    # Active means the constraint is either violated or close to being violated
                    # h < margin or L_f h + L_g h * u_raw + alpha * h < 0
                    active_margin = 0.1
                    constraint_values = h[b, i]
                    constraint_derivatives = L_f_h[b, i] + torch.bmm(L_g_h[b, i].unsqueeze(1), 
                                                                  raw_action[b, i].unsqueeze(-1)).squeeze(-1)
                    
                    # Use appropriate alpha value for this agent
                    agent_alpha = alpha_values[b, i, 0] if alphas is not None else self.alpha
                    active_constraints = (constraint_values < active_margin) | \
                                       (constraint_derivatives + agent_alpha * constraint_values < 0)
                    
                    # Skip if no active constraints
                    if not torch.any(active_constraints):
                        continue
                    
                    # Extract active constraints
                    A = L_g_h[b, i][active_constraints]
                    b_qp = rhs[b, i][active_constraints]
                    
                    # Solve QP using a simple projection method
                    # This is a simplification; a full QP solver would be more robust
                    u = raw_action[b, i]
                    for _ in range(self.max_iterations):
                        # Check constraint violations
                        violations = torch.mm(A, u.unsqueeze(-1)).squeeze(-1) - b_qp
                        if torch.all(violations >= 0):
                            break
                            
                        # Compute projections for violated constraints
                        violated = violations < 0
                        if torch.any(violated):
                            # Update for each violated constraint
                            for j in torch.nonzero(violated):
                                # Compute projection
                                a_j = A[j]
                                b_j = b_qp[j]
                                
                                # Project onto constraint: u -= (a_j^T u - b_j) * a_j / ||a_j||^2
                                a_j_norm = torch.sum(a_j * a_j)
                                if a_j_norm > 1e-6:  # Avoid division by zero
                                    u = u - (torch.dot(a_j, u) - b_j) * a_j / a_j_norm
                    
                    safe_action[b, i] = u
        else:
            # Simpler safety filtering approach for each constraint
            # Instead of solving a QP, we just project the action if constraints are violated
            
            # Check which constraints are violated: L_f h + L_g h * u_raw + alpha * h < 0
            constraint_values = L_f_h + torch.matmul(L_g_h, raw_action.unsqueeze(-1)).squeeze(-1) + alpha_values * h
            violations = constraint_values < 0
            
            # Initialize safe action as raw action
            safe_action = raw_action.clone()
            
            # Process each batch and agent separately
            batch_size = state.batch_size
            n_agents = state.positions.shape[1]
            
            for b in range(batch_size):
                for i in range(n_agents):
                    # Skip if no violations for this agent
                    if not torch.any(violations[b, i]):
                        continue
                    
                    # For each violated constraint, project the action
                    for j in torch.nonzero(violations[b, i]):
                        # Skip if L_g_h is too small (constraint not controllable)
                        a_j = L_g_h[b, i, j]
                        a_j_norm = torch.sum(a_j * a_j)
                        if a_j_norm < 1e-6:
                            continue
                            
                        # Compute minimum value to satisfy constraint
                        b_j = rhs[b, i, j]
                        u = safe_action[b, i]
                        
                        # Project onto constraint: u -= (a_j^T u - b_j) * a_j / ||a_j||^2 if a_j^T u < b_j
                        if torch.dot(a_j, u) < b_j:
                            safe_action[b, i] = u - (torch.dot(a_j, u) - b_j) * a_j / a_j_norm
        
        return safe_action


class GCBFPlusAgent(nn.Module):
    """
    ç»“åˆç­–ç•¥ç½‘ç»œå’ŒGCBFå®‰å…¨å±‚çš„æ™ºèƒ½ä½“ã€‚
    
    è¯¥æ™ºèƒ½ä½“å°è£…äº†ç­–ç•¥ç½‘ç»œå’Œå®‰å…¨å±‚ï¼Œ
    æä¾›äº†ç”Ÿæˆå®‰å…¨åŠ¨ä½œçš„ç»Ÿä¸€æŽ¥å£ã€‚
    """
    
    def __init__(self, policy_network: nn.Module, safety_layer: GCBFSafetyLayer, cbf_network: Optional[nn.Module] = None):
        """
        åˆå§‹åŒ–GCBF+æ™ºèƒ½ä½“ã€‚
        
        å‚æ•°:
            policy_network: ç¥žç»ç½‘ç»œç­–ç•¥
            safety_layer: ç”¨äºŽåŠ¨ä½œè¿‡æ»¤çš„CBFå®‰å…¨å±‚
            cbf_network: å¯é€‰çš„å­¦ä¹ å±éšœå‡½æ•°ç¥žç»ç½‘ç»œ
        """
        super(GCBFPlusAgent, self).__init__()
        
        self.policy_network = policy_network
        self.safety_layer = safety_layer
        self.cbf_network = cbf_network
        
    def forward(self, state: MultiAgentState) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ä¸ºå½“å‰çŠ¶æ€ç”Ÿæˆå®‰å…¨åŠ¨ä½œã€‚
        
        å‚æ•°:
            state: å½“å‰çŽ¯å¢ƒçŠ¶æ€
            
        è¿”å›ž:
            (safe_action, raw_action)çš„å…ƒç»„:
            - safe_action: å®‰å…¨è¿‡æ»¤åŽçš„åŠ¨ä½œ
            - raw_action: ç­–ç•¥ç½‘ç»œè¾“å‡ºçš„åŽŸå§‹åŠ¨ä½œ
        """
        # ä»ŽçŠ¶æ€èŽ·å–è§‚æµ‹
        observations = self.get_observations(state)
        
        # ä»Žç­–ç•¥ç”ŸæˆåŽŸå§‹åŠ¨ä½œ
        raw_action = self.policy_network(observations)
        
        # åº”ç”¨å®‰å…¨è¿‡æ»¤
        safe_action = self.safety_layer(state, raw_action)
        
        return safe_action, raw_action
    
    def get_observations(self, state: MultiAgentState) -> torch.Tensor:
        """
        Extract observations from environment state.
        
        Args:
            state: Current environment state
            
        Returns:
            Observation tensor for the policy network
        """
        # Default implementation: concatenate positions, velocities, and goals
        # This can be overridden for more complex observation spaces
        observations = torch.cat([
            state.positions,
            state.velocities,
            state.goals
        ], dim=2)
        
        return observations 