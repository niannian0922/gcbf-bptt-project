#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Centralized Inference Utility
çµ±ä¸€çš„æ¨¡å‹æ¨ç†å·¥å…·

è§£æ±ºæ‰€æœ‰é…ç½®åŠ è¼‰å’Œå¼µé‡ç¶­åº¦å•é¡Œ
"""

import os
import torch
import yaml
import numpy as np
from typing import Dict, Tuple, Any
import torch.nn as nn
from pathlib import Path

from ..policy import BPTTPolicy
from ..env.multi_agent_env import MultiAgentState


def load_model_and_config(model_dir: str) -> Tuple[nn.Module, Dict]:
    """
    å¾æ¨¡å‹ç›®éŒ„åŠ è¼‰ç­–ç•¥ç¶²çµ¡å’Œé…ç½®
    
    Args:
        model_dir: æ¨¡å‹ç›®éŒ„è·¯å¾‘ (e.g., logs/bptt/models/9500)
    
    Returns:
        Tuple[nn.Module, Dict]: (ç­–ç•¥ç¶²çµ¡, é…ç½®å­—å…¸)
    """
    print(f"ğŸ” åŠ è¼‰æ¨¡å‹å’Œé…ç½®: {model_dir}")
    
    # æª¢æŸ¥å¿…è¦æ–‡ä»¶
    policy_path = os.path.join(model_dir, "policy.pt")
    config_path = os.path.join(model_dir, "config.pt")
    
    if not os.path.exists(policy_path):
        raise FileNotFoundError(f"ç­–ç•¥æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {policy_path}")
    
    # åŠ è¼‰é…ç½®
    config = None
    if os.path.exists(config_path):
        try:
            config = torch.load(config_path, map_location='cpu', weights_only=False)
            print("âœ… å¾ config.pt åŠ è¼‰é…ç½®æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ config.pt åŠ è¼‰å¤±æ•—: {e}")
    
    # å¦‚æœconfig.ptå¤±æ•—ï¼Œå˜—è©¦å°‹æ‰¾é—œè¯çš„yamlé…ç½®
    if config is None:
        # æŸ¥æ‰¾å¯èƒ½çš„é…ç½®æ–‡ä»¶
        config_candidates = [
            "config/alpha_medium_obs.yaml",
            "config/simple_collaboration.yaml", 
            "config/collaboration_test.yaml"
        ]
        
        for config_file in config_candidates:
            if os.path.exists(config_file):
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = yaml.safe_load(f)
                    print(f"âœ… å¾ {config_file} åŠ è¼‰é…ç½®æˆåŠŸ")
                    break
                except Exception as e:
                    print(f"âš ï¸ {config_file} åŠ è¼‰å¤±æ•—: {e}")
    
    # å¦‚æœé‚„æ˜¯æ²’æœ‰é…ç½®ï¼Œä½¿ç”¨é»˜èªé…ç½®
    if config is None:
        print("âš ï¸ ä½¿ç”¨é»˜èªé…ç½®")
        config = {
            'env': {
                'n_agents': 8,
                'world_size': 4.0,
                'agent_radius': 0.1,
                'goal_radius': 0.2,
                'dt': 0.05,
                'obstacles': {
                    'enabled': True,
                    'num_obstacles': 8,
                    'radius_range': [0.2, 0.4]
                }
            }
        }
    
    # åŠ è¼‰ç­–ç•¥æ¨¡å‹ä¸¦æ¨æ–·æ¶æ§‹
    print("ğŸ” åˆ†æç­–ç•¥æ¨¡å‹æ¶æ§‹...")
    policy_state_dict = torch.load(policy_path, map_location='cpu', weights_only=False)
    
    # å¾state_dictæ¨æ–·ç¶²çµ¡æ¶æ§‹
    perception_weight_key = 'perception.mlp.0.weight'
    if perception_weight_key in policy_state_dict:
        perception_input_dim = policy_state_dict[perception_weight_key].shape[1]
        perception_hidden_dim = policy_state_dict[perception_weight_key].shape[0]
        print(f"ğŸ“ æ¨æ–·çš„æ„ŸçŸ¥æ¨¡å¡Šæ¶æ§‹: input_dim={perception_input_dim}, hidden_dim={perception_hidden_dim}")
    else:
        # é»˜èªæ¶æ§‹
        perception_input_dim = 9  # æœ‰éšœç¤™ç‰©ç’°å¢ƒ
        perception_hidden_dim = 128
        print(f"âš ï¸ ä½¿ç”¨é»˜èªæ„ŸçŸ¥æ¨¡å¡Šæ¶æ§‹: input_dim={perception_input_dim}, hidden_dim={perception_hidden_dim}")
    
    # æ¨æ–·è¨˜æ†¶æ¨¡å¡Šæ¶æ§‹
    memory_weight_key = 'memory.rnn.weight_ih_l0'
    if memory_weight_key in policy_state_dict:
        memory_input_dim = policy_state_dict[memory_weight_key].shape[1]
        memory_hidden_dim = policy_state_dict[memory_weight_key].shape[0] // 4  # LSTMæœ‰4å€‹gate
        print(f"ğŸ“ æ¨æ–·çš„è¨˜æ†¶æ¨¡å¡Šæ¶æ§‹: input_dim={memory_input_dim}, hidden_dim={memory_hidden_dim}")
    else:
        memory_hidden_dim = 128
        print(f"âš ï¸ ä½¿ç”¨é»˜èªè¨˜æ†¶æ¨¡å¡Šæ¶æ§‹: hidden_dim={memory_hidden_dim}")
    
    # æ¨æ–·ç­–ç•¥é ­æ¶æ§‹
    policy_head_weight_key = 'policy_head.mlp.0.weight'
    if policy_head_weight_key in policy_state_dict:
        policy_head_input_dim = policy_state_dict[policy_head_weight_key].shape[1]
        policy_head_output_key = None
        for key in policy_state_dict.keys():
            if key.startswith('policy_head.mlp.') and key.endswith('.weight'):
                policy_head_output_key = key
        if policy_head_output_key:
            policy_head_output_dim = policy_state_dict[policy_head_output_key].shape[0]
        else:
            policy_head_output_dim = 2  # é»˜èª2Då‹•ä½œ
        print(f"ğŸ“ æ¨æ–·çš„ç­–ç•¥é ­æ¶æ§‹: input_dim={policy_head_input_dim}, output_dim={policy_head_output_dim}")
    else:
        policy_head_output_dim = 2
        print(f"âš ï¸ ä½¿ç”¨é»˜èªç­–ç•¥é ­æ¶æ§‹: output_dim={policy_head_output_dim}")
    
    # æ§‹å»ºç­–ç•¥ç¶²çµ¡é…ç½®
    policy_config = {
        'perception': {
            'use_vision': False,
            'input_dim': perception_input_dim,
            'hidden_dim': perception_hidden_dim,
            'activation': 'relu'
        },
        'memory': {
            'hidden_dim': memory_hidden_dim,
            'num_layers': 1
        },
        'policy_head': {
            'output_dim': policy_head_output_dim,
            'hidden_dim': memory_hidden_dim,  # é€šå¸¸èˆ‡memoryä¸€è‡´
            'activation': 'relu',
            'predict_alpha': True
        }
    }
    
    # å‰µå»ºç­–ç•¥ç¶²çµ¡
    print("ğŸ—ï¸ å‰µå»ºç­–ç•¥ç¶²çµ¡...")
    policy_network = BPTTPolicy(policy_config)
    
    # åŠ è¼‰æ¬Šé‡
    try:
        missing_keys, unexpected_keys = policy_network.load_state_dict(policy_state_dict, strict=False)
        if missing_keys:
            print(f"âš ï¸ ç¼ºå¤±çš„æ¬Šé‡: {missing_keys}")
        if unexpected_keys:
            print(f"âš ï¸ æ„å¤–çš„æ¬Šé‡: {unexpected_keys}")
        print("âœ… ç­–ç•¥ç¶²çµ¡æ¬Šé‡åŠ è¼‰æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¬Šé‡åŠ è¼‰å¤±æ•—: {e}")
        raise
    
    # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
    policy_network.eval()
    
    return policy_network, config


def run_simulation_for_visualization(env, policy: nn.Module, steps: int = 300) -> Tuple[list, list]:
    """
    é‹è¡Œä»¿çœŸç”Ÿæˆå¯è¦–åŒ–è»Œè·¡
    
    Args:
        env: ç’°å¢ƒå¯¦ä¾‹
        policy: ç­–ç•¥ç¶²çµ¡
        steps: ä»¿çœŸæ­¥æ•¸
    
    Returns:
        Tuple[list, list]: (è»Œè·¡åˆ—è¡¨, éšœç¤™ç‰©åˆ—è¡¨)
    """
    print(f"ğŸ¬ é–‹å§‹ä»¿çœŸ: {steps} æ­¥")
    
    # é‡ç½®ç’°å¢ƒ
    state = env.reset()
    
    # å­˜å„²è»Œè·¡
    trajectories = []
    
    # ç²å–éšœç¤™ç‰©ä¿¡æ¯
    obstacles = []
    if hasattr(env, 'obstacles') and env.obstacles:
        for obs in env.obstacles:
            if hasattr(obs, 'center') and hasattr(obs, 'radius'):
                obstacles.append({
                    'center': obs.center.copy() if hasattr(obs.center, 'copy') else np.array(obs.center),
                    'radius': obs.radius
                })
    
    print(f"ğŸ“Š ç’°å¢ƒä¿¡æ¯: {env.num_agents} æ™ºèƒ½é«”, {len(obstacles)} éšœç¤™ç‰©")
    
    for step in range(steps):
        # æå–ä½ç½®å’Œé€Ÿåº¦
        if hasattr(state, 'positions'):
            # MultiAgentState æ ¼å¼
            positions = state.positions.numpy() if hasattr(state.positions, 'numpy') else np.array(state.positions)
            velocities = state.velocities.numpy() if hasattr(state.velocities, 'numpy') else np.array(state.velocities)
        elif isinstance(state, torch.Tensor):
            # å¼µé‡æ ¼å¼
            state_np = state.numpy()
            positions = state_np[:, :2]
            velocities = state_np[:, 2:4] if state_np.shape[1] >= 4 else np.zeros_like(positions)
        else:
            # numpyæ•¸çµ„æ ¼å¼
            state_np = np.array(state)
            positions = state_np[:, :2]
            velocities = state_np[:, 2:4] if state_np.shape[1] >= 4 else np.zeros_like(positions)
        
        # è¨˜éŒ„è»Œè·¡
        trajectories.append({
            'positions': positions.copy(),
            'velocities': velocities.copy(),
            'step': step
        })
        
        # ç²å–è§€æ¸¬
        try:
            obs = env.get_observation(state)
            
            # ç¢ºä¿è§€æ¸¬æ˜¯æ­£ç¢ºçš„å¼µé‡æ ¼å¼
            if not isinstance(obs, torch.Tensor):
                obs = torch.FloatTensor(obs)
            
            # æ·»åŠ æ‰¹æ¬¡ç¶­åº¦: [n_agents, obs_dim] -> [1, n_agents, obs_dim]
            if obs.dim() == 2:
                obs_tensor = obs.unsqueeze(0)
            else:
                obs_tensor = obs
            
            print(f"Step {step}: obs shape = {obs_tensor.shape}")
            
            # ç­–ç•¥ç¶²çµ¡é æ¸¬
            with torch.no_grad():
                try:
                    result = policy(obs_tensor)
                    
                    # è™•ç†è¿”å›å€¼
                    if isinstance(result, tuple):
                        if len(result) >= 2:
                            actions, alpha = result[0], result[1]
                        else:
                            actions = result[0]
                            alpha = None
                    else:
                        actions = result
                        alpha = None
                    
                    # ç§»é™¤æ‰¹æ¬¡ç¶­åº¦: [1, n_agents, action_dim] -> [n_agents, action_dim]
                    if actions.dim() == 3:
                        actions = actions.squeeze(0)
                    
                    actions_np = actions.numpy()
                    
                except Exception as policy_error:
                    print(f"âš ï¸ ç­–ç•¥é æ¸¬å¤±æ•— (step {step}): {policy_error}")
                    # ä½¿ç”¨é›¶å‹•ä½œ
                    actions_np = np.zeros((env.num_agents, 2))
            
        except Exception as obs_error:
            print(f"âš ï¸ è§€æ¸¬ç²å–å¤±æ•— (step {step}): {obs_error}")
            # ä½¿ç”¨é›¶å‹•ä½œ
            actions_np = np.zeros((env.num_agents, 2))
        
        # åŸ·è¡Œå‹•ä½œ
        try:
            state = env.step(state, actions_np)
        except Exception as step_error:
            print(f"âš ï¸ ç’°å¢ƒæ­¥é€²å¤±æ•— (step {step}): {step_error}")
            break
        
        # ç°¡å–®çš„ç›®æ¨™æª¢æŸ¥
        if hasattr(env, 'goal_positions') and step % 50 == 0:
            goal_distances = np.linalg.norm(positions - env.goal_positions, axis=1)
            avg_distance = np.mean(goal_distances)
            print(f"Step {step}: å¹³å‡ç›®æ¨™è·é›¢ = {avg_distance:.3f}")
    
    print(f"âœ… ä»¿çœŸå®Œæˆ: ç”Ÿæˆ {len(trajectories)} å€‹è»Œè·¡é»")
    return trajectories, obstacles