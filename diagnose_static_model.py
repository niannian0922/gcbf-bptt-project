#!/usr/bin/env python3
"""
ğŸ” è¯Šæ–­é™æ­¢æ¨¡å‹é—®é¢˜
æ‰¾å‡ºä¸ºä»€ä¹ˆçœŸå®æ¨¡å‹è¾“å‡ºæ¥è¿‘é›¶åŠ¨ä½œ
"""

import torch
import numpy as np
import os

print("ğŸ” è¯Šæ–­é™æ­¢æ¨¡å‹é—®é¢˜")
print("=" * 50)

# åŠ è½½æ¨¡å‹
model_path = 'logs/full_collaboration_training/models/500/policy.pt'
device = torch.device('cpu')

try:
    policy_dict = torch.load(model_path, map_location=device, weights_only=True)
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {len(policy_dict)} å±‚")
    
    # åˆ†ææ¨¡å‹æƒé‡
    print("\nğŸ” åˆ†ææ¨¡å‹æƒé‡åˆ†å¸ƒ:")
    for key, param in policy_dict.items():
        if 'weight' in key and param.numel() > 100:
            mean_val = torch.mean(torch.abs(param)).item()
            max_val = torch.max(torch.abs(param)).item()
            std_val = torch.std(param).item()
            print(f"  {key}: å‡å€¼={mean_val:.6f}, æœ€å¤§={max_val:.6f}, æ ‡å‡†å·®={std_val:.6f}")
            
            if mean_val < 1e-6:
                print(f"    âš ï¸ æƒé‡è¿‡å°ï¼Œå¯èƒ½å¯¼è‡´è¾“å‡ºæ¥è¿‘é›¶")
    
    # æ£€æŸ¥è¾“å‡ºå±‚
    if 'policy_head.action_mlp.2.weight' in policy_dict:
        output_weights = policy_dict['policy_head.action_mlp.2.weight']
        print(f"\nğŸ¯ è¾“å‡ºå±‚æƒé‡: {output_weights.shape}")
        print(f"   è¾“å‡ºæƒé‡ç»Ÿè®¡: å‡å€¼={torch.mean(torch.abs(output_weights)):.6f}")
        print(f"   è¾“å‡ºæƒé‡èŒƒå›´: [{torch.min(output_weights):.6f}, {torch.max(output_weights):.6f}]")
        
        if torch.max(torch.abs(output_weights)) < 0.01:
            print("   âŒ è¾“å‡ºå±‚æƒé‡è¿‡å°ï¼Œä¼šå¯¼è‡´åŠ¨ä½œæ¥è¿‘é›¶!")
        
except Exception as e:
    print(f"âŒ æ¨¡å‹åˆ†æå¤±è´¥: {e}")
    exit()

# å¯¼å…¥ç¯å¢ƒå¹¶æµ‹è¯•
try:
    from gcbfplus.env import DoubleIntegratorEnv
    from gcbfplus.env.multi_agent_env import MultiAgentState
    from gcbfplus.policy.bptt_policy import BPTTPolicy
    print("\nâœ… ç¯å¢ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ ç¯å¢ƒå¯¼å…¥å¤±è´¥: {e}")
    exit()

# åˆ›å»ºç®€å•æµ‹è¯•ç¯å¢ƒ
env_config = {
    'num_agents': 6,
    'area_size': 4.0,
    'dt': 0.02,
    'mass': 0.5,
    'agent_radius': 0.15,
    'max_force': 1.0,
    'max_steps': 100,
    'obstacles': {
        'enabled': True,
        'count': 2,
        'positions': [[0, 0.7], [0, -0.7]],
        'radii': [0.3, 0.3]
    }
}

try:
    env = DoubleIntegratorEnv(env_config)
    env = env.to(device)
    print(f"âœ… æµ‹è¯•ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
except Exception as e:
    print(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
    exit()

# åˆ›å»ºç­–ç•¥ç½‘ç»œ
input_dim = 9  # æœ‰éšœç¢ç‰©ç¯å¢ƒ
try:
    policy_config = {
        'input_dim': input_dim,
        'output_dim': 2,
        'hidden_dim': 256,
        'node_dim': input_dim,
        'edge_dim': 4,
        'n_layers': 2,
        'msg_hidden_sizes': [256, 256],
        'aggr_hidden_sizes': [256],
        'update_hidden_sizes': [256, 256],
        'predict_alpha': True,
        'perception': {
            'input_dim': input_dim,
            'hidden_dim': 256,
            'num_layers': 2,
            'activation': 'relu',
            'use_vision': False
        },
        'memory': {
            'hidden_dim': 256,
            'memory_size': 32,
            'num_heads': 4
        },
        'policy_head': {
            'output_dim': 2,
            'predict_alpha': True,
            'hidden_dims': [256, 256],
            'action_scale': 1.0
        },
        'device': device
    }
    
    policy = BPTTPolicy(policy_config)
    policy = policy.to(device)
    policy.load_state_dict(policy_dict)
    policy.eval()
    print("âœ… ç­–ç•¥ç½‘ç»œåˆ›å»ºæˆåŠŸ")
    
except Exception as e:
    print(f"âŒ ç­–ç•¥ç½‘ç»œåˆ›å»ºå¤±è´¥: {e}")
    exit()

# è¯¦ç»†æµ‹è¯•ä¸åŒåœºæ™¯
print("\nğŸ§ª æµ‹è¯•ä¸åŒåœºæ™¯çš„ç­–ç•¥è¾“å‡º:")

test_scenarios = [
    {
        'name': 'è¿œè·ç¦»ç›®æ ‡',
        'start_positions': [[-2.0, 0], [-2.0, 0.3], [-2.0, -0.3], [-2.0, 0.6], [-2.0, -0.6], [-2.0, 0.9]],
        'goal_positions': [[2.0, 0], [2.0, 0.3], [2.0, -0.3], [2.0, 0.6], [2.0, -0.6], [2.0, 0.9]]
    },
    {
        'name': 'ç´§æ€¥é¿éšœ',
        'start_positions': [[-0.5, 0], [-0.3, 0], [-0.1, 0], [0.1, 0], [0.3, 0], [0.5, 0]],
        'goal_positions': [[2.0, 0], [2.0, 0.3], [2.0, -0.3], [2.0, 0.6], [2.0, -0.6], [2.0, 0.9]]
    },
    {
        'name': 'æç«¯è·ç¦»',
        'start_positions': [[-3.0, 0], [-3.0, 0.2], [-3.0, -0.2], [-3.0, 0.4], [-3.0, -0.4], [-3.0, 0.6]],
        'goal_positions': [[3.0, 0], [3.0, 0.2], [3.0, -0.2], [3.0, 0.4], [3.0, -0.4], [3.0, 0.6]]
    }
]

with torch.no_grad():
    for scenario in test_scenarios:
        print(f"\nğŸ“ åœºæ™¯: {scenario['name']}")
        
        # åˆ›å»ºæµ‹è¯•çŠ¶æ€
        num_agents = len(scenario['start_positions'])
        positions = torch.tensor(scenario['start_positions'], device=device).unsqueeze(0).float()
        goals = torch.tensor(scenario['goal_positions'], device=device).unsqueeze(0).float()
        velocities = torch.zeros_like(positions)
        
        state = MultiAgentState(
            positions=positions,
            velocities=velocities,
            goals=goals,
            batch_size=1
        )
        
        try:
            # è·å–è§‚æµ‹
            observations = env.get_observations(state)
            print(f"   è§‚æµ‹å½¢çŠ¶: {observations.shape}")
            print(f"   è§‚æµ‹èŒƒå›´: [{torch.min(observations):.4f}, {torch.max(observations):.4f}]")
            
            # ç­–ç•¥æ¨ç†
            policy_output = policy(observations, state)
            actions = policy_output.actions[0].cpu().numpy()
            
            # åˆ†æåŠ¨ä½œ
            action_magnitudes = [np.linalg.norm(a) for a in actions]
            avg_action_mag = np.mean(action_magnitudes)
            max_action_mag = np.max(action_magnitudes)
            
            print(f"   åŠ¨ä½œå½¢çŠ¶: {actions.shape}")
            print(f"   å¹³å‡åŠ¨ä½œå¼ºåº¦: {avg_action_mag:.6f}")
            print(f"   æœ€å¤§åŠ¨ä½œå¼ºåº¦: {max_action_mag:.6f}")
            print(f"   åŠ¨ä½œèŒƒå›´: [{np.min(actions):.6f}, {np.max(actions):.6f}]")
            
            # è®¡ç®—æœŸæœ›çš„åŠ¨ä½œå¼ºåº¦
            goal_distances = [np.linalg.norm(scenario['goal_positions'][i] - scenario['start_positions'][i]) for i in range(num_agents)]
            avg_goal_distance = np.mean(goal_distances)
            expected_action = min(0.5, avg_goal_distance * 0.1)  # ç®€å•å¯å‘å¼
            
            print(f"   å¹³å‡ç›®æ ‡è·ç¦»: {avg_goal_distance:.3f}")
            print(f"   æœŸæœ›åŠ¨ä½œå¼ºåº¦: ~{expected_action:.3f}")
            
            if avg_action_mag < 0.001:
                print("   âŒ åŠ¨ä½œå¼ºåº¦è¿‡å°!")
                
                # è¯¦ç»†åˆ†ææ¯ä¸ªæ™ºèƒ½ä½“
                for i, (pos, goal, action) in enumerate(zip(scenario['start_positions'], scenario['goal_positions'], actions)):
                    direction = np.array(goal) - np.array(pos)
                    distance = np.linalg.norm(direction)
                    unit_direction = direction / distance if distance > 0 else np.array([0, 0])
                    
                    action_magnitude = np.linalg.norm(action)
                    action_direction = action / action_magnitude if action_magnitude > 0 else np.array([0, 0])
                    
                    alignment = np.dot(unit_direction, action_direction) if action_magnitude > 0 else 0
                    
                    print(f"     æ™ºèƒ½ä½“{i}: è·ç¦»={distance:.3f}, åŠ¨ä½œå¼ºåº¦={action_magnitude:.6f}, æ–¹å‘å¯¹é½={alignment:.3f}")
            else:
                print("   âœ… æ£€æµ‹åˆ°æœ‰æ•ˆåŠ¨ä½œ")
                
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")

print("\nğŸ” è¯Šæ–­ç»“è®º:")
print("å¦‚æœæ‰€æœ‰åœºæ™¯çš„åŠ¨ä½œå¼ºåº¦éƒ½ < 0.001:")
print("  1. æ¨¡å‹å¯èƒ½æ”¶æ•›åˆ°é™æ­¢ç­–ç•¥")
print("  2. è¾“å‡ºå±‚æƒé‡å¯èƒ½è¿‡å°")
print("  3. è®­ç»ƒè¿‡ç¨‹å¯èƒ½æœ‰é—®é¢˜")
print("  4. è§‚æµ‹é¢„å¤„ç†å¯èƒ½ä¸åŒ¹é…")

print("\nğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
print("  1. æ£€æŸ¥è®­ç»ƒæ—¥å¿—ç¡®è®¤æŸå¤±æ˜¯å¦æ­£å¸¸ä¸‹é™")
print("  2. å°è¯•åŠ è½½è®­ç»ƒæ—©æœŸçš„æ£€æŸ¥ç‚¹")
print("  3. æ‰‹åŠ¨è®¾ç½®åˆç†çš„åŠ¨ä½œæ¥ç”Ÿæˆ'åº”è¯¥çš„'å¯è§†åŒ–")
print("  4. æ£€æŸ¥CBFç½‘ç»œæ˜¯å¦è¿‡åº¦æŠ‘åˆ¶äº†åŠ¨ä½œ")
 
"""
ğŸ” è¯Šæ–­é™æ­¢æ¨¡å‹é—®é¢˜
æ‰¾å‡ºä¸ºä»€ä¹ˆçœŸå®æ¨¡å‹è¾“å‡ºæ¥è¿‘é›¶åŠ¨ä½œ
"""

import torch
import numpy as np
import os

print("ğŸ” è¯Šæ–­é™æ­¢æ¨¡å‹é—®é¢˜")
print("=" * 50)

# åŠ è½½æ¨¡å‹
model_path = 'logs/full_collaboration_training/models/500/policy.pt'
device = torch.device('cpu')

try:
    policy_dict = torch.load(model_path, map_location=device, weights_only=True)
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {len(policy_dict)} å±‚")
    
    # åˆ†ææ¨¡å‹æƒé‡
    print("\nğŸ” åˆ†ææ¨¡å‹æƒé‡åˆ†å¸ƒ:")
    for key, param in policy_dict.items():
        if 'weight' in key and param.numel() > 100:
            mean_val = torch.mean(torch.abs(param)).item()
            max_val = torch.max(torch.abs(param)).item()
            std_val = torch.std(param).item()
            print(f"  {key}: å‡å€¼={mean_val:.6f}, æœ€å¤§={max_val:.6f}, æ ‡å‡†å·®={std_val:.6f}")
            
            if mean_val < 1e-6:
                print(f"    âš ï¸ æƒé‡è¿‡å°ï¼Œå¯èƒ½å¯¼è‡´è¾“å‡ºæ¥è¿‘é›¶")
    
    # æ£€æŸ¥è¾“å‡ºå±‚
    if 'policy_head.action_mlp.2.weight' in policy_dict:
        output_weights = policy_dict['policy_head.action_mlp.2.weight']
        print(f"\nğŸ¯ è¾“å‡ºå±‚æƒé‡: {output_weights.shape}")
        print(f"   è¾“å‡ºæƒé‡ç»Ÿè®¡: å‡å€¼={torch.mean(torch.abs(output_weights)):.6f}")
        print(f"   è¾“å‡ºæƒé‡èŒƒå›´: [{torch.min(output_weights):.6f}, {torch.max(output_weights):.6f}]")
        
        if torch.max(torch.abs(output_weights)) < 0.01:
            print("   âŒ è¾“å‡ºå±‚æƒé‡è¿‡å°ï¼Œä¼šå¯¼è‡´åŠ¨ä½œæ¥è¿‘é›¶!")
        
except Exception as e:
    print(f"âŒ æ¨¡å‹åˆ†æå¤±è´¥: {e}")
    exit()

# å¯¼å…¥ç¯å¢ƒå¹¶æµ‹è¯•
try:
    from gcbfplus.env import DoubleIntegratorEnv
    from gcbfplus.env.multi_agent_env import MultiAgentState
    from gcbfplus.policy.bptt_policy import BPTTPolicy
    print("\nâœ… ç¯å¢ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ ç¯å¢ƒå¯¼å…¥å¤±è´¥: {e}")
    exit()

# åˆ›å»ºç®€å•æµ‹è¯•ç¯å¢ƒ
env_config = {
    'num_agents': 6,
    'area_size': 4.0,
    'dt': 0.02,
    'mass': 0.5,
    'agent_radius': 0.15,
    'max_force': 1.0,
    'max_steps': 100,
    'obstacles': {
        'enabled': True,
        'count': 2,
        'positions': [[0, 0.7], [0, -0.7]],
        'radii': [0.3, 0.3]
    }
}

try:
    env = DoubleIntegratorEnv(env_config)
    env = env.to(device)
    print(f"âœ… æµ‹è¯•ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
except Exception as e:
    print(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
    exit()

# åˆ›å»ºç­–ç•¥ç½‘ç»œ
input_dim = 9  # æœ‰éšœç¢ç‰©ç¯å¢ƒ
try:
    policy_config = {
        'input_dim': input_dim,
        'output_dim': 2,
        'hidden_dim': 256,
        'node_dim': input_dim,
        'edge_dim': 4,
        'n_layers': 2,
        'msg_hidden_sizes': [256, 256],
        'aggr_hidden_sizes': [256],
        'update_hidden_sizes': [256, 256],
        'predict_alpha': True,
        'perception': {
            'input_dim': input_dim,
            'hidden_dim': 256,
            'num_layers': 2,
            'activation': 'relu',
            'use_vision': False
        },
        'memory': {
            'hidden_dim': 256,
            'memory_size': 32,
            'num_heads': 4
        },
        'policy_head': {
            'output_dim': 2,
            'predict_alpha': True,
            'hidden_dims': [256, 256],
            'action_scale': 1.0
        },
        'device': device
    }
    
    policy = BPTTPolicy(policy_config)
    policy = policy.to(device)
    policy.load_state_dict(policy_dict)
    policy.eval()
    print("âœ… ç­–ç•¥ç½‘ç»œåˆ›å»ºæˆåŠŸ")
    
except Exception as e:
    print(f"âŒ ç­–ç•¥ç½‘ç»œåˆ›å»ºå¤±è´¥: {e}")
    exit()

# è¯¦ç»†æµ‹è¯•ä¸åŒåœºæ™¯
print("\nğŸ§ª æµ‹è¯•ä¸åŒåœºæ™¯çš„ç­–ç•¥è¾“å‡º:")

test_scenarios = [
    {
        'name': 'è¿œè·ç¦»ç›®æ ‡',
        'start_positions': [[-2.0, 0], [-2.0, 0.3], [-2.0, -0.3], [-2.0, 0.6], [-2.0, -0.6], [-2.0, 0.9]],
        'goal_positions': [[2.0, 0], [2.0, 0.3], [2.0, -0.3], [2.0, 0.6], [2.0, -0.6], [2.0, 0.9]]
    },
    {
        'name': 'ç´§æ€¥é¿éšœ',
        'start_positions': [[-0.5, 0], [-0.3, 0], [-0.1, 0], [0.1, 0], [0.3, 0], [0.5, 0]],
        'goal_positions': [[2.0, 0], [2.0, 0.3], [2.0, -0.3], [2.0, 0.6], [2.0, -0.6], [2.0, 0.9]]
    },
    {
        'name': 'æç«¯è·ç¦»',
        'start_positions': [[-3.0, 0], [-3.0, 0.2], [-3.0, -0.2], [-3.0, 0.4], [-3.0, -0.4], [-3.0, 0.6]],
        'goal_positions': [[3.0, 0], [3.0, 0.2], [3.0, -0.2], [3.0, 0.4], [3.0, -0.4], [3.0, 0.6]]
    }
]

with torch.no_grad():
    for scenario in test_scenarios:
        print(f"\nğŸ“ åœºæ™¯: {scenario['name']}")
        
        # åˆ›å»ºæµ‹è¯•çŠ¶æ€
        num_agents = len(scenario['start_positions'])
        positions = torch.tensor(scenario['start_positions'], device=device).unsqueeze(0).float()
        goals = torch.tensor(scenario['goal_positions'], device=device).unsqueeze(0).float()
        velocities = torch.zeros_like(positions)
        
        state = MultiAgentState(
            positions=positions,
            velocities=velocities,
            goals=goals,
            batch_size=1
        )
        
        try:
            # è·å–è§‚æµ‹
            observations = env.get_observations(state)
            print(f"   è§‚æµ‹å½¢çŠ¶: {observations.shape}")
            print(f"   è§‚æµ‹èŒƒå›´: [{torch.min(observations):.4f}, {torch.max(observations):.4f}]")
            
            # ç­–ç•¥æ¨ç†
            policy_output = policy(observations, state)
            actions = policy_output.actions[0].cpu().numpy()
            
            # åˆ†æåŠ¨ä½œ
            action_magnitudes = [np.linalg.norm(a) for a in actions]
            avg_action_mag = np.mean(action_magnitudes)
            max_action_mag = np.max(action_magnitudes)
            
            print(f"   åŠ¨ä½œå½¢çŠ¶: {actions.shape}")
            print(f"   å¹³å‡åŠ¨ä½œå¼ºåº¦: {avg_action_mag:.6f}")
            print(f"   æœ€å¤§åŠ¨ä½œå¼ºåº¦: {max_action_mag:.6f}")
            print(f"   åŠ¨ä½œèŒƒå›´: [{np.min(actions):.6f}, {np.max(actions):.6f}]")
            
            # è®¡ç®—æœŸæœ›çš„åŠ¨ä½œå¼ºåº¦
            goal_distances = [np.linalg.norm(scenario['goal_positions'][i] - scenario['start_positions'][i]) for i in range(num_agents)]
            avg_goal_distance = np.mean(goal_distances)
            expected_action = min(0.5, avg_goal_distance * 0.1)  # ç®€å•å¯å‘å¼
            
            print(f"   å¹³å‡ç›®æ ‡è·ç¦»: {avg_goal_distance:.3f}")
            print(f"   æœŸæœ›åŠ¨ä½œå¼ºåº¦: ~{expected_action:.3f}")
            
            if avg_action_mag < 0.001:
                print("   âŒ åŠ¨ä½œå¼ºåº¦è¿‡å°!")
                
                # è¯¦ç»†åˆ†ææ¯ä¸ªæ™ºèƒ½ä½“
                for i, (pos, goal, action) in enumerate(zip(scenario['start_positions'], scenario['goal_positions'], actions)):
                    direction = np.array(goal) - np.array(pos)
                    distance = np.linalg.norm(direction)
                    unit_direction = direction / distance if distance > 0 else np.array([0, 0])
                    
                    action_magnitude = np.linalg.norm(action)
                    action_direction = action / action_magnitude if action_magnitude > 0 else np.array([0, 0])
                    
                    alignment = np.dot(unit_direction, action_direction) if action_magnitude > 0 else 0
                    
                    print(f"     æ™ºèƒ½ä½“{i}: è·ç¦»={distance:.3f}, åŠ¨ä½œå¼ºåº¦={action_magnitude:.6f}, æ–¹å‘å¯¹é½={alignment:.3f}")
            else:
                print("   âœ… æ£€æµ‹åˆ°æœ‰æ•ˆåŠ¨ä½œ")
                
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")

print("\nğŸ” è¯Šæ–­ç»“è®º:")
print("å¦‚æœæ‰€æœ‰åœºæ™¯çš„åŠ¨ä½œå¼ºåº¦éƒ½ < 0.001:")
print("  1. æ¨¡å‹å¯èƒ½æ”¶æ•›åˆ°é™æ­¢ç­–ç•¥")
print("  2. è¾“å‡ºå±‚æƒé‡å¯èƒ½è¿‡å°")
print("  3. è®­ç»ƒè¿‡ç¨‹å¯èƒ½æœ‰é—®é¢˜")
print("  4. è§‚æµ‹é¢„å¤„ç†å¯èƒ½ä¸åŒ¹é…")

print("\nğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
print("  1. æ£€æŸ¥è®­ç»ƒæ—¥å¿—ç¡®è®¤æŸå¤±æ˜¯å¦æ­£å¸¸ä¸‹é™")
print("  2. å°è¯•åŠ è½½è®­ç»ƒæ—©æœŸçš„æ£€æŸ¥ç‚¹")
print("  3. æ‰‹åŠ¨è®¾ç½®åˆç†çš„åŠ¨ä½œæ¥ç”Ÿæˆ'åº”è¯¥çš„'å¯è§†åŒ–")
print("  4. æ£€æŸ¥CBFç½‘ç»œæ˜¯å¦è¿‡åº¦æŠ‘åˆ¶äº†åŠ¨ä½œ")
 
 
 
 