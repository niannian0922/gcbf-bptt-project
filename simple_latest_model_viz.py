#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæœ€æ–°æ¨¡åž‹å¯è§†åŒ–
ä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½ï¼Œé¿å…å¤æ‚é…ç½®é—®é¢˜
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import os
from datetime import datetime

print("ðŸŽ¯ ç®€åŒ–ç‰ˆæœ€æ–°æ¨¡åž‹å¯è§†åŒ–")
print("=" * 50)

# æ£€æŸ¥æ¨¡åž‹æ–‡ä»¶
model_path = 'logs/full_collaboration_training/models/500/policy.pt'
if not os.path.exists(model_path):
    print("âŒ æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨")
    exit()

print(f"âœ… æ¨¡åž‹æ–‡ä»¶å­˜åœ¨: {os.path.getsize(model_path)/(1024*1024):.1f}MB")

# åŠ è½½æ¨¡åž‹æƒé‡
try:
    device = torch.device('cpu')
    policy_dict = torch.load(model_path, map_location=device, weights_only=True)
    print(f"âœ… æ¨¡åž‹æƒé‡åŠ è½½æˆåŠŸ: {len(policy_dict)} å±‚")
    
    # æŽ¨æ–­è¾“å…¥ç»´åº¦
    if 'perception.mlp.0.weight' in policy_dict:
        input_dim = policy_dict['perception.mlp.0.weight'].shape[1]
        print(f"ðŸŽ¯ æŽ¨æ–­è¾“å…¥ç»´åº¦: {input_dim}")
    else:
        input_dim = 9
        print(f"âš ï¸ ä½¿ç”¨é»˜è®¤è¾“å…¥ç»´åº¦: {input_dim}")
        
except Exception as e:
    print(f"âŒ æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
    exit()

# å¯¼å…¥çŽ¯å¢ƒ
try:
    from gcbfplus.env import DoubleIntegratorEnv
    from gcbfplus.env.multi_agent_env import MultiAgentState
    from gcbfplus.policy.bptt_policy import BPTTPolicy
    print("âœ… çŽ¯å¢ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ çŽ¯å¢ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    exit()

# åˆ›å»ºçŽ¯å¢ƒ
try:
    env_config = {
        'num_agents': 6,
        'area_size': 4.0,
        'dt': 0.02,
        'mass': 0.5,
        'agent_radius': 0.15,
        'max_force': 0.5,
        'max_steps': 120,
        'obstacles': {
            'enabled': True if input_dim == 9 else False,
            'count': 2,
            'positions': [[0, 0.7], [0, -0.7]],
            'radii': [0.3, 0.3]
        }
    }
    
    env = DoubleIntegratorEnv(env_config)
    env = env.to(device)
    print(f"âœ… çŽ¯å¢ƒåˆ›å»ºæˆåŠŸ: {env.num_agents} æ™ºèƒ½ä½“")
    
except Exception as e:
    print(f"âŒ çŽ¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
    exit()

# åˆ›å»ºç­–ç•¥ç½‘ç»œ
try:
    policy_config = {
        'input_dim': int(input_dim),
        'output_dim': 2,
        'hidden_dim': 256,
        'node_dim': int(input_dim),
        'edge_dim': 4,
        'n_layers': 2,
        'msg_hidden_sizes': [256, 256],
        'aggr_hidden_sizes': [256],
        'update_hidden_sizes': [256, 256],
        'predict_alpha': True,
        'perception': {
            'input_dim': int(input_dim),
            'hidden_dim': 256,
            'num_layers': 2,
            'activation': 'relu',
            'use_vision': False
        },
        'memory': {
            'hidden_dim': 256,
            'memory_size': 32,
            'num_heads': 4
        },
        'policy_head': {
            'output_dim': 2,
            'predict_alpha': True,
            'hidden_dims': [256, 256],
            'action_scale': 1.0
        },
        'device': device
    }
    
    policy = BPTTPolicy(policy_config)
    policy = policy.to(device)
    policy.load_state_dict(policy_dict)
    policy.eval()
    print("âœ… ç­–ç•¥ç½‘ç»œåˆ›å»ºæˆåŠŸ")
    
except Exception as e:
    print(f"âŒ ç­–ç•¥ç½‘ç»œåˆ›å»ºå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    exit()

# ç”Ÿæˆç®€å•è½¨è¿¹
print("ðŸŽ¬ ç”Ÿæˆè½¨è¿¹...")

# åˆ›å»ºåˆå§‹çŠ¶æ€
num_agents = env.num_agents
positions = torch.zeros(1, num_agents, 2, device=device)
velocities = torch.zeros(1, num_agents, 2, device=device)
goals = torch.zeros(1, num_agents, 2, device=device)

for i in range(num_agents):
    positions[0, i] = torch.tensor([-1.5, (i - num_agents/2) * 0.3], device=device)
    goals[0, i] = torch.tensor([1.5, (i - num_agents/2) * 0.3], device=device)

current_state = MultiAgentState(
    positions=positions,
    velocities=velocities,
    goals=goals,
    batch_size=1
)

# è¿è¡ŒæŽ¨ç†
trajectory_positions = []
trajectory_actions = []
num_steps = 60  # å‡å°‘æ­¥æ•°ä»¥åŠ å¿«ç”Ÿæˆ

print(f"ðŸ“ ç”Ÿæˆ {num_steps} æ­¥...")

with torch.no_grad():
    for step in range(num_steps):
        # è®°å½•ä½ç½®
        pos = current_state.positions[0].cpu().numpy()
        trajectory_positions.append(pos.copy())
        
        try:
            # ç­–ç•¥æŽ¨ç†
            observations = env.get_observations(current_state)
            policy_output = policy(observations, current_state)
            actions = policy_output.actions[0].cpu().numpy()
            alphas = policy_output.alphas[0].cpu().numpy() if hasattr(policy_output, 'alphas') else np.ones(num_agents) * 0.5
            
            trajectory_actions.append(actions.copy())
            
            if step % 20 == 0:
                action_mag = np.mean([np.linalg.norm(a) for a in actions])
                print(f"  æ­¥éª¤ {step}: åŠ¨ä½œå¼ºåº¦={action_mag:.4f}")
            
            # çŽ¯å¢ƒæ­¥è¿›
            actions_tensor = torch.tensor(actions, device=device).unsqueeze(0)
            alphas_tensor = torch.tensor(alphas, device=device).unsqueeze(0)
            
            step_result = env.step(current_state, actions_tensor, alphas_tensor)
            current_state = step_result.next_state
            
        except Exception as e:
            print(f"âš ï¸ æ­¥éª¤ {step} å¤±è´¥: {e}")
            # ä½¿ç”¨é›¶åŠ¨ä½œ
            actions = np.zeros((num_agents, 2))
            trajectory_actions.append(actions)

print(f"âœ… è½¨è¿¹ç”Ÿæˆå®Œæˆ: {len(trajectory_positions)} æ­¥")

# åˆ†æžè½¨è¿¹
if trajectory_actions:
    all_actions = np.concatenate(trajectory_actions)
    avg_action = np.mean([np.linalg.norm(a) for a in all_actions])
    print(f"ðŸ“Š å¹³å‡åŠ¨ä½œå¼ºåº¦: {avg_action:.4f}")

# åˆ›å»ºç®€å•å¯è§†åŒ–
print("ðŸŽ¨ åˆ›å»ºå¯è§†åŒ–...")

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
fig.suptitle('ðŸŽ¯ æœ€æ–°åä½œè®­ç»ƒæ¨¡åž‹ (CBFä¿®å¤+åä½œæŸå¤±) - çœŸå®žè½¨è¿¹', fontsize=16, fontweight='bold')

# ä¸»è½¨è¿¹å›¾
ax1.set_xlim(-2.0, 2.0)
ax1.set_ylim(-1.0, 1.0)
ax1.set_aspect('equal')
ax1.set_title('ðŸš æœ€æ–°çœŸå®žç¥žç»ç½‘ç»œç­–ç•¥è½¨è¿¹')
ax1.grid(True, alpha=0.3)

# ç»˜åˆ¶éšœç¢ç‰©ï¼ˆå¦‚æžœæœ‰ï¼‰
if env_config['obstacles']['enabled']:
    for pos, radius in zip(env_config['obstacles']['positions'], env_config['obstacles']['radii']):
        circle = plt.Circle(pos, radius, color='red', alpha=0.7)
        ax1.add_patch(circle)

# æ™ºèƒ½ä½“é¢œè‰²
colors = ['red', 'green', 'blue', 'orange', 'purple', 'brown'][:num_agents]

# è½¨è¿¹çº¿
trail_lines = []
drone_dots = []

for i in range(num_agents):
    line, = ax1.plot([], [], '-', color=colors[i], linewidth=2, label=f'æ™ºèƒ½ä½“{i+1}')
    trail_lines.append(line)
    dot, = ax1.plot([], [], 'o', color=colors[i], markersize=8, markeredgecolor='black')
    drone_dots.append(dot)

ax1.legend()

# åŠ¨ä½œå¼ºåº¦å›¾
ax2.set_title('ðŸ§  ç­–ç•¥ç½‘ç»œåŠ¨ä½œè¾“å‡º')
ax2.set_xlabel('æ—¶é—´æ­¥')
ax2.set_ylabel('åŠ¨ä½œå¼ºåº¦')
ax2.grid(True, alpha=0.3)

def animate(frame):
    if frame >= len(trajectory_positions):
        return trail_lines + drone_dots
    
    current_pos = trajectory_positions[frame]
    
    # æ›´æ–°è½¨è¿¹
    for i in range(num_agents):
        trail_x = [pos[i, 0] for pos in trajectory_positions[:frame+1]]
        trail_y = [pos[i, 1] for pos in trajectory_positions[:frame+1]]
        trail_lines[i].set_data(trail_x, trail_y)
        drone_dots[i].set_data([current_pos[i, 0]], [current_pos[i, 1]])
    
    # æ›´æ–°åŠ¨ä½œå›¾
    if frame > 5 and len(trajectory_actions) > frame:
        steps = list(range(frame+1))
        action_mags = []
        for step in range(frame+1):
            if step < len(trajectory_actions):
                step_actions = trajectory_actions[step]
                avg_mag = np.mean([np.linalg.norm(a) for a in step_actions])
                action_mags.append(avg_mag)
            else:
                action_mags.append(0)
        
        ax2.clear()
        ax2.plot(steps, action_mags, 'red', linewidth=2)
        ax2.set_title(f'ðŸ§  ç­–ç•¥ç½‘ç»œåŠ¨ä½œè¾“å‡º (æ­¥æ•°: {frame})')
        ax2.set_xlabel('æ—¶é—´æ­¥')
        ax2.set_ylabel('åŠ¨ä½œå¼ºåº¦')
        ax2.grid(True, alpha=0.3)
    
    return trail_lines + drone_dots

# åˆ›å»ºåŠ¨ç”»
anim = FuncAnimation(fig, animate, frames=len(trajectory_positions), 
                    interval=200, blit=False, repeat=True)

# ä¿å­˜
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_path = f'SIMPLE_LATEST_REAL_{timestamp}.gif'

try:
    print("ðŸ’¾ ä¿å­˜å¯è§†åŒ–...")
    anim.save(output_path, writer='pillow', fps=5, dpi=100)
    
    file_size = os.path.getsize(output_path) / (1024 * 1024)
    print(f"âœ… ä¿å­˜æˆåŠŸ: {output_path}")
    print(f"ðŸ“ æ–‡ä»¶å¤§å°: {file_size:.2f}MB")
    print(f"ðŸŽ¯ è¿™æ˜¯åŸºäºŽæ‚¨æœ€æ–°2.4MBåä½œè®­ç»ƒæ¨¡åž‹çš„çœŸå®žå¯è§†åŒ–!")
    
except Exception as e:
    print(f"âš ï¸ åŠ¨ç”»ä¿å­˜å¤±è´¥: {e}")
    # ä¿å­˜é™æ€å›¾
    static_path = f'SIMPLE_LATEST_STATIC_{timestamp}.png'
    plt.tight_layout()
    plt.savefig(static_path, dpi=120, bbox_inches='tight')
    print(f"âœ… é™æ€å›¾ä¿å­˜: {static_path}")

plt.close()
print("ðŸŽ‰ å¯è§†åŒ–ç”Ÿæˆå®Œæˆ!")
 
"""
ç®€åŒ–ç‰ˆæœ€æ–°æ¨¡åž‹å¯è§†åŒ–
ä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½ï¼Œé¿å…å¤æ‚é…ç½®é—®é¢˜
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import os
from datetime import datetime

print("ðŸŽ¯ ç®€åŒ–ç‰ˆæœ€æ–°æ¨¡åž‹å¯è§†åŒ–")
print("=" * 50)

# æ£€æŸ¥æ¨¡åž‹æ–‡ä»¶
model_path = 'logs/full_collaboration_training/models/500/policy.pt'
if not os.path.exists(model_path):
    print("âŒ æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨")
    exit()

print(f"âœ… æ¨¡åž‹æ–‡ä»¶å­˜åœ¨: {os.path.getsize(model_path)/(1024*1024):.1f}MB")

# åŠ è½½æ¨¡åž‹æƒé‡
try:
    device = torch.device('cpu')
    policy_dict = torch.load(model_path, map_location=device, weights_only=True)
    print(f"âœ… æ¨¡åž‹æƒé‡åŠ è½½æˆåŠŸ: {len(policy_dict)} å±‚")
    
    # æŽ¨æ–­è¾“å…¥ç»´åº¦
    if 'perception.mlp.0.weight' in policy_dict:
        input_dim = policy_dict['perception.mlp.0.weight'].shape[1]
        print(f"ðŸŽ¯ æŽ¨æ–­è¾“å…¥ç»´åº¦: {input_dim}")
    else:
        input_dim = 9
        print(f"âš ï¸ ä½¿ç”¨é»˜è®¤è¾“å…¥ç»´åº¦: {input_dim}")
        
except Exception as e:
    print(f"âŒ æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
    exit()

# å¯¼å…¥çŽ¯å¢ƒ
try:
    from gcbfplus.env import DoubleIntegratorEnv
    from gcbfplus.env.multi_agent_env import MultiAgentState
    from gcbfplus.policy.bptt_policy import BPTTPolicy
    print("âœ… çŽ¯å¢ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ çŽ¯å¢ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    exit()

# åˆ›å»ºçŽ¯å¢ƒ
try:
    env_config = {
        'num_agents': 6,
        'area_size': 4.0,
        'dt': 0.02,
        'mass': 0.5,
        'agent_radius': 0.15,
        'max_force': 0.5,
        'max_steps': 120,
        'obstacles': {
            'enabled': True if input_dim == 9 else False,
            'count': 2,
            'positions': [[0, 0.7], [0, -0.7]],
            'radii': [0.3, 0.3]
        }
    }
    
    env = DoubleIntegratorEnv(env_config)
    env = env.to(device)
    print(f"âœ… çŽ¯å¢ƒåˆ›å»ºæˆåŠŸ: {env.num_agents} æ™ºèƒ½ä½“")
    
except Exception as e:
    print(f"âŒ çŽ¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
    exit()

# åˆ›å»ºç­–ç•¥ç½‘ç»œ
try:
    policy_config = {
        'input_dim': int(input_dim),
        'output_dim': 2,
        'hidden_dim': 256,
        'node_dim': int(input_dim),
        'edge_dim': 4,
        'n_layers': 2,
        'msg_hidden_sizes': [256, 256],
        'aggr_hidden_sizes': [256],
        'update_hidden_sizes': [256, 256],
        'predict_alpha': True,
        'perception': {
            'input_dim': int(input_dim),
            'hidden_dim': 256,
            'num_layers': 2,
            'activation': 'relu',
            'use_vision': False
        },
        'memory': {
            'hidden_dim': 256,
            'memory_size': 32,
            'num_heads': 4
        },
        'policy_head': {
            'output_dim': 2,
            'predict_alpha': True,
            'hidden_dims': [256, 256],
            'action_scale': 1.0
        },
        'device': device
    }
    
    policy = BPTTPolicy(policy_config)
    policy = policy.to(device)
    policy.load_state_dict(policy_dict)
    policy.eval()
    print("âœ… ç­–ç•¥ç½‘ç»œåˆ›å»ºæˆåŠŸ")
    
except Exception as e:
    print(f"âŒ ç­–ç•¥ç½‘ç»œåˆ›å»ºå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    exit()

# ç”Ÿæˆç®€å•è½¨è¿¹
print("ðŸŽ¬ ç”Ÿæˆè½¨è¿¹...")

# åˆ›å»ºåˆå§‹çŠ¶æ€
num_agents = env.num_agents
positions = torch.zeros(1, num_agents, 2, device=device)
velocities = torch.zeros(1, num_agents, 2, device=device)
goals = torch.zeros(1, num_agents, 2, device=device)

for i in range(num_agents):
    positions[0, i] = torch.tensor([-1.5, (i - num_agents/2) * 0.3], device=device)
    goals[0, i] = torch.tensor([1.5, (i - num_agents/2) * 0.3], device=device)

current_state = MultiAgentState(
    positions=positions,
    velocities=velocities,
    goals=goals,
    batch_size=1
)

# è¿è¡ŒæŽ¨ç†
trajectory_positions = []
trajectory_actions = []
num_steps = 60  # å‡å°‘æ­¥æ•°ä»¥åŠ å¿«ç”Ÿæˆ

print(f"ðŸ“ ç”Ÿæˆ {num_steps} æ­¥...")

with torch.no_grad():
    for step in range(num_steps):
        # è®°å½•ä½ç½®
        pos = current_state.positions[0].cpu().numpy()
        trajectory_positions.append(pos.copy())
        
        try:
            # ç­–ç•¥æŽ¨ç†
            observations = env.get_observations(current_state)
            policy_output = policy(observations, current_state)
            actions = policy_output.actions[0].cpu().numpy()
            alphas = policy_output.alphas[0].cpu().numpy() if hasattr(policy_output, 'alphas') else np.ones(num_agents) * 0.5
            
            trajectory_actions.append(actions.copy())
            
            if step % 20 == 0:
                action_mag = np.mean([np.linalg.norm(a) for a in actions])
                print(f"  æ­¥éª¤ {step}: åŠ¨ä½œå¼ºåº¦={action_mag:.4f}")
            
            # çŽ¯å¢ƒæ­¥è¿›
            actions_tensor = torch.tensor(actions, device=device).unsqueeze(0)
            alphas_tensor = torch.tensor(alphas, device=device).unsqueeze(0)
            
            step_result = env.step(current_state, actions_tensor, alphas_tensor)
            current_state = step_result.next_state
            
        except Exception as e:
            print(f"âš ï¸ æ­¥éª¤ {step} å¤±è´¥: {e}")
            # ä½¿ç”¨é›¶åŠ¨ä½œ
            actions = np.zeros((num_agents, 2))
            trajectory_actions.append(actions)

print(f"âœ… è½¨è¿¹ç”Ÿæˆå®Œæˆ: {len(trajectory_positions)} æ­¥")

# åˆ†æžè½¨è¿¹
if trajectory_actions:
    all_actions = np.concatenate(trajectory_actions)
    avg_action = np.mean([np.linalg.norm(a) for a in all_actions])
    print(f"ðŸ“Š å¹³å‡åŠ¨ä½œå¼ºåº¦: {avg_action:.4f}")

# åˆ›å»ºç®€å•å¯è§†åŒ–
print("ðŸŽ¨ åˆ›å»ºå¯è§†åŒ–...")

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
fig.suptitle('ðŸŽ¯ æœ€æ–°åä½œè®­ç»ƒæ¨¡åž‹ (CBFä¿®å¤+åä½œæŸå¤±) - çœŸå®žè½¨è¿¹', fontsize=16, fontweight='bold')

# ä¸»è½¨è¿¹å›¾
ax1.set_xlim(-2.0, 2.0)
ax1.set_ylim(-1.0, 1.0)
ax1.set_aspect('equal')
ax1.set_title('ðŸš æœ€æ–°çœŸå®žç¥žç»ç½‘ç»œç­–ç•¥è½¨è¿¹')
ax1.grid(True, alpha=0.3)

# ç»˜åˆ¶éšœç¢ç‰©ï¼ˆå¦‚æžœæœ‰ï¼‰
if env_config['obstacles']['enabled']:
    for pos, radius in zip(env_config['obstacles']['positions'], env_config['obstacles']['radii']):
        circle = plt.Circle(pos, radius, color='red', alpha=0.7)
        ax1.add_patch(circle)

# æ™ºèƒ½ä½“é¢œè‰²
colors = ['red', 'green', 'blue', 'orange', 'purple', 'brown'][:num_agents]

# è½¨è¿¹çº¿
trail_lines = []
drone_dots = []

for i in range(num_agents):
    line, = ax1.plot([], [], '-', color=colors[i], linewidth=2, label=f'æ™ºèƒ½ä½“{i+1}')
    trail_lines.append(line)
    dot, = ax1.plot([], [], 'o', color=colors[i], markersize=8, markeredgecolor='black')
    drone_dots.append(dot)

ax1.legend()

# åŠ¨ä½œå¼ºåº¦å›¾
ax2.set_title('ðŸ§  ç­–ç•¥ç½‘ç»œåŠ¨ä½œè¾“å‡º')
ax2.set_xlabel('æ—¶é—´æ­¥')
ax2.set_ylabel('åŠ¨ä½œå¼ºåº¦')
ax2.grid(True, alpha=0.3)

def animate(frame):
    if frame >= len(trajectory_positions):
        return trail_lines + drone_dots
    
    current_pos = trajectory_positions[frame]
    
    # æ›´æ–°è½¨è¿¹
    for i in range(num_agents):
        trail_x = [pos[i, 0] for pos in trajectory_positions[:frame+1]]
        trail_y = [pos[i, 1] for pos in trajectory_positions[:frame+1]]
        trail_lines[i].set_data(trail_x, trail_y)
        drone_dots[i].set_data([current_pos[i, 0]], [current_pos[i, 1]])
    
    # æ›´æ–°åŠ¨ä½œå›¾
    if frame > 5 and len(trajectory_actions) > frame:
        steps = list(range(frame+1))
        action_mags = []
        for step in range(frame+1):
            if step < len(trajectory_actions):
                step_actions = trajectory_actions[step]
                avg_mag = np.mean([np.linalg.norm(a) for a in step_actions])
                action_mags.append(avg_mag)
            else:
                action_mags.append(0)
        
        ax2.clear()
        ax2.plot(steps, action_mags, 'red', linewidth=2)
        ax2.set_title(f'ðŸ§  ç­–ç•¥ç½‘ç»œåŠ¨ä½œè¾“å‡º (æ­¥æ•°: {frame})')
        ax2.set_xlabel('æ—¶é—´æ­¥')
        ax2.set_ylabel('åŠ¨ä½œå¼ºåº¦')
        ax2.grid(True, alpha=0.3)
    
    return trail_lines + drone_dots

# åˆ›å»ºåŠ¨ç”»
anim = FuncAnimation(fig, animate, frames=len(trajectory_positions), 
                    interval=200, blit=False, repeat=True)

# ä¿å­˜
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_path = f'SIMPLE_LATEST_REAL_{timestamp}.gif'

try:
    print("ðŸ’¾ ä¿å­˜å¯è§†åŒ–...")
    anim.save(output_path, writer='pillow', fps=5, dpi=100)
    
    file_size = os.path.getsize(output_path) / (1024 * 1024)
    print(f"âœ… ä¿å­˜æˆåŠŸ: {output_path}")
    print(f"ðŸ“ æ–‡ä»¶å¤§å°: {file_size:.2f}MB")
    print(f"ðŸŽ¯ è¿™æ˜¯åŸºäºŽæ‚¨æœ€æ–°2.4MBåä½œè®­ç»ƒæ¨¡åž‹çš„çœŸå®žå¯è§†åŒ–!")
    
except Exception as e:
    print(f"âš ï¸ åŠ¨ç”»ä¿å­˜å¤±è´¥: {e}")
    # ä¿å­˜é™æ€å›¾
    static_path = f'SIMPLE_LATEST_STATIC_{timestamp}.png'
    plt.tight_layout()
    plt.savefig(static_path, dpi=120, bbox_inches='tight')
    print(f"âœ… é™æ€å›¾ä¿å­˜: {static_path}")

plt.close()
print("ðŸŽ‰ å¯è§†åŒ–ç”Ÿæˆå®Œæˆ!")
 
 
 
 