@echo off
echo ğŸ¯ ç”Ÿæˆæœ€æ–°çœŸå®æ¨¡å‹å¯è§†åŒ–
echo ====================================
echo.

echo ğŸ“ æœ€æ–°æ¨¡å‹è·¯å¾„: logs\full_collaboration_training\models\500\
echo ğŸ“Š æ¨¡å‹å¤§å°: 2.5MB
echo ğŸ“… è®­ç»ƒæ—¶é—´: 2025/08/05 19:59
echo ğŸ¯ ç±»å‹: CBFä¿®å¤+åä½œæŸå¤±è®­ç»ƒåçš„æœ€æ–°æ¨¡å‹
echo.

echo ğŸš€ å¼€å§‹ç”ŸæˆåŸºäºæœ€æ–°è®­ç»ƒæ¨¡å‹çš„çœŸå®å¯è§†åŒ–...
echo.

python -c "
import sys
import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from datetime import datetime

print('ğŸ¯ æœ€æ–°çœŸå®æ¨¡å‹å¯è§†åŒ–ç”Ÿæˆå™¨')
print('=' * 60)

# æœ€æ–°æ¨¡å‹è·¯å¾„
model_path = 'logs/full_collaboration_training/models/500/'
policy_path = os.path.join(model_path, 'policy.pt')
cbf_path = os.path.join(model_path, 'cbf.pt')

print(f'ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}')
print(f'ğŸ“Š ç­–ç•¥æ–‡ä»¶: {os.path.exists(policy_path)} ({os.path.getsize(policy_path)/(1024*1024):.1f}MB)')
print(f'ğŸ“Š CBFæ–‡ä»¶: {os.path.exists(cbf_path)} ({os.path.getsize(cbf_path)/1024:.1f}KB)')

# å¯¼å…¥æ¨¡å—
try:
    from gcbfplus.env import DoubleIntegratorEnv
    from gcbfplus.env.multi_agent_env import MultiAgentState  
    from gcbfplus.policy.bptt_policy import BPTTPolicy
    print('âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ')
except Exception as e:
    print(f'âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}')
    sys.exit(1)

# åŠ è½½æ¨¡å‹
try:
    device = torch.device('cpu')
    policy_state_dict = torch.load(policy_path, map_location=device, weights_only=True)
    print(f'âœ… æœ€æ–°ç­–ç•¥æƒé‡åŠ è½½æˆåŠŸ ({len(policy_state_dict)} å±‚)')
    
    # æ¨æ–­è¾“å…¥ç»´åº¦
    if 'perception.mlp.0.weight' in policy_state_dict:
        input_dim = policy_state_dict['perception.mlp.0.weight'].shape[1]
        print(f'ğŸ” æ¨¡å‹è¾“å…¥ç»´åº¦: {input_dim}')
    else:
        input_dim = 9  # é»˜è®¤ä½¿ç”¨æœ‰éšœç¢ç‰©ç¯å¢ƒ
        print(f'âš ï¸ ä½¿ç”¨é»˜è®¤è¾“å…¥ç»´åº¦: {input_dim}')
        
except Exception as e:
    print(f'âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}')
    sys.exit(1)

# åˆ›å»ºç¯å¢ƒ
try:
    env_config = {
        'name': 'DoubleIntegrator',
        'num_agents': 6,
        'area_size': 4.0,
        'dt': 0.02,
        'mass': 0.5,
        'agent_radius': 0.15,
        'comm_radius': 1.0,
        'max_force': 0.5,
        'max_steps': 120,
        'social_radius': 0.4,
        'obstacles': {
            'enabled': True if input_dim == 9 else False,
            'count': 2,
            'positions': [[0, 0.7], [0, -0.7]],
            'radii': [0.3, 0.3]
        }
    }
    
    env = DoubleIntegratorEnv(env_config)
    env = env.to(device)
    print(f'âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ: {env.num_agents}æ™ºèƒ½ä½“, {env.observation_shape}ç»´è§‚æµ‹')
    
except Exception as e:
    print(f'âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}')
    sys.exit(1)

# åˆ›å»ºç­–ç•¥ç½‘ç»œ
try:
    policy_config = {
        'type': 'bptt',
        'input_dim': env.observation_shape,
        'output_dim': env.action_shape,
        'hidden_dim': 256,
        'node_dim': env.observation_shape,
        'edge_dim': 4,
        'n_layers': 2,
        'msg_hidden_sizes': [256, 256],
        'aggr_hidden_sizes': [256],
        'update_hidden_sizes': [256, 256],
        'predict_alpha': True,
        'perception': {
            'input_dim': env.observation_shape,
            'hidden_dim': 256,
            'num_layers': 2,
            'activation': 'relu',
            'use_vision': False
        },
        'memory': {
            'hidden_dim': 256,
            'memory_size': 32,
            'num_heads': 4
        },
        'policy_head': {
            'output_dim': env.action_shape,
            'predict_alpha': True,
            'hidden_dims': [256, 256],
            'action_scale': 1.0
        },
        'device': device
    }
    
    policy = BPTTPolicy(policy_config)
    policy = policy.to(device)
    policy.load_state_dict(policy_state_dict)
    policy.eval()
    print('âœ… æœ€æ–°ç­–ç•¥ç½‘ç»œåŠ è½½æˆåŠŸ')
    
except Exception as e:
    print(f'âŒ ç­–ç•¥ç½‘ç»œåˆ›å»ºå¤±è´¥: {e}')
    sys.exit(1)

# ç”Ÿæˆè½¨è¿¹
print('ğŸ¬ å¼€å§‹ç”Ÿæˆæœ€æ–°æ¨¡å‹è½¨è¿¹...')

# åˆ›å»ºåä½œæµ‹è¯•åœºæ™¯
num_agents = env.num_agents
positions = torch.zeros(1, num_agents, 2, device=device)
velocities = torch.zeros(1, num_agents, 2, device=device)
goals = torch.zeros(1, num_agents, 2, device=device)

# è®¾è®¡éœ€è¦åä½œçš„åœºæ™¯
for i in range(num_agents):
    # å·¦ä¾§èµ·å§‹ï¼Œéœ€è¦é€šè¿‡ä¸­é—´éšœç¢åŒºåŸŸåˆ°è¾¾å³ä¾§
    start_x = -1.8
    start_y = (i - num_agents/2) * 0.35
    target_x = 1.8  
    target_y = (i - num_agents/2) * 0.35
    
    positions[0, i] = torch.tensor([start_x, start_y], device=device)
    goals[0, i] = torch.tensor([target_x, target_y], device=device)

current_state = MultiAgentState(
    positions=positions,
    velocities=velocities,
    goals=goals,
    batch_size=1
)

# è¿è¡Œæœ€æ–°æ¨¡å‹æ¨ç†
trajectory_data = {
    'positions': [],
    'actions': [],
    'velocities': [],
    'goal_distances': []
}

num_steps = 120
print(f'ğŸ“ ç”Ÿæˆ {num_steps} æ­¥è½¨è¿¹...')

with torch.no_grad():
    for step in range(num_steps):
        # è®°å½•çŠ¶æ€
        positions_np = current_state.positions[0].cpu().numpy()
        velocities_np = current_state.velocities[0].cpu().numpy()
        goals_np = current_state.goals[0].cpu().numpy()
        
        trajectory_data['positions'].append(positions_np.copy())
        trajectory_data['velocities'].append(velocities_np.copy())
        
        # è®¡ç®—ç›®æ ‡è·ç¦»
        goal_distances = [np.linalg.norm(positions_np[i] - goals_np[i]) for i in range(len(positions_np))]
        trajectory_data['goal_distances'].append(goal_distances)
        
        # ç­–ç•¥æ¨ç†
        try:
            observations = env.get_observations(current_state)
            policy_output = policy(observations, current_state)
            actions = policy_output.actions[0].cpu().numpy()
            alphas = policy_output.alphas[0].cpu().numpy() if hasattr(policy_output, 'alphas') else np.ones(len(positions_np)) * 0.5
            
            trajectory_data['actions'].append(actions.copy())
            
            if step % 30 == 0:
                action_mag = np.mean([np.linalg.norm(a) for a in actions])
                avg_goal_dist = np.mean(goal_distances)
                print(f'  æ­¥éª¤ {step:3d}: åŠ¨ä½œå¼ºåº¦={action_mag:.4f}, ç›®æ ‡è·ç¦»={avg_goal_dist:.3f}')
                
        except Exception as e:
            print(f'âš ï¸ æ¨ç†å¤±è´¥ (æ­¥éª¤ {step}): {e}')
            actions = np.zeros((len(positions_np), 2))
            alphas = np.ones(len(positions_np)) * 0.5
            trajectory_data['actions'].append(actions)
        
        # ç¯å¢ƒæ­¥è¿›
        try:
            actions_tensor = torch.tensor(actions, device=device).unsqueeze(0)
            alphas_tensor = torch.tensor(alphas, device=device).unsqueeze(0)
            
            step_result = env.step(current_state, actions_tensor, alphas_tensor)
            current_state = step_result.next_state
            
            # æ£€æŸ¥å®Œæˆ
            if np.mean(goal_distances) < 0.3:
                print(f'ğŸ¯ ä»»åŠ¡å®Œæˆ! (æ­¥æ•°: {step+1})')
                break
                
        except Exception as e:
            print(f'âš ï¸ ç¯å¢ƒæ­¥è¿›å¤±è´¥ (æ­¥éª¤ {step}): {e}')
            break

# åˆ†æè½¨è¿¹è´¨é‡
if trajectory_data['actions']:
    all_actions = np.concatenate(trajectory_data['actions'])
    avg_action = np.mean([np.linalg.norm(a) for a in all_actions])
    print(f'ğŸ“Š è½¨è¿¹åˆ†æ:')
    print(f'   ç”Ÿæˆæ­¥æ•°: {len(trajectory_data[\"positions\"])}')
    print(f'   å¹³å‡åŠ¨ä½œå¼ºåº¦: {avg_action:.4f}')
    
    if avg_action > 0.001:
        print('âœ… æœ€æ–°æ¨¡å‹æœ‰æœ‰æ•ˆè¾“å‡º')
    else:
        print('âš ï¸ è­¦å‘Š: åŠ¨ä½œå¼ºåº¦è¾ƒå°')
else:
    print('âŒ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆè½¨è¿¹')
    sys.exit(1)

# åˆ›å»ºå¯è§†åŒ–
print('ğŸ¨ åˆ›å»ºæœ€æ–°æ¨¡å‹å¯è§†åŒ–...')

positions_history = trajectory_data['positions']
actions_history = trajectory_data['actions']
goal_distances_history = trajectory_data['goal_distances']

num_agents = len(positions_history[0])
num_steps = len(positions_history)

# åˆ›å»ºå›¾å½¢
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
fig.suptitle('ğŸ¯ æœ€æ–°åä½œè®­ç»ƒæ¨¡å‹ (CBFä¿®å¤+åä½œæŸå¤±) - çœŸå®å¯è§†åŒ–', fontsize=18, fontweight='bold')

# ä¸»è½¨è¿¹å›¾
ax1.set_xlim(-2.5, 2.5)
ax1.set_ylim(-1.5, 1.5)
ax1.set_aspect('equal')
ax1.set_title('ğŸš æœ€æ–°çœŸå®ç¥ç»ç½‘ç»œç­–ç•¥è½¨è¿¹', fontsize=14)
ax1.grid(True, alpha=0.3)

# ç»˜åˆ¶éšœç¢ç‰©
if env_config['obstacles']['enabled']:
    for i, (pos, radius) in enumerate(zip(env_config['obstacles']['positions'], env_config['obstacles']['radii'])):
        circle = plt.Circle(pos, radius, color='red', alpha=0.8, label='éšœç¢ç‰©' if i == 0 else '')
        ax1.add_patch(circle)

# èµ·å§‹å’Œç›®æ ‡åŒºåŸŸ
start_zone = plt.Rectangle((-2.2, -1.2), 0.8, 2.4, fill=False, edgecolor='green', linestyle='--', linewidth=2, alpha=0.8, label='èµ·å§‹åŒºåŸŸ')
ax1.add_patch(start_zone)
target_zone = plt.Rectangle((1.4, -1.2), 0.8, 2.4, fill=False, edgecolor='blue', linestyle='--', linewidth=2, alpha=0.8, label='ç›®æ ‡åŒºåŸŸ')
ax1.add_patch(target_zone)

# æ™ºèƒ½ä½“é¢œè‰²
colors = ['#FF4444', '#44FF44', '#4444FF', '#FFAA44', '#FF44AA', '#44AAFF'][:num_agents]

# åˆå§‹åŒ–åŠ¨ç”»å…ƒç´ 
trail_lines = []
drone_dots = []

for i in range(num_agents):
    line, = ax1.plot([], [], '-', color=colors[i], alpha=0.8, linewidth=3, label=f'æ™ºèƒ½ä½“{i+1}' if i < 3 else '')
    trail_lines.append(line)
    
    drone, = ax1.plot([], [], 'o', color=colors[i], markersize=14, markeredgecolor='black', markeredgewidth=2, zorder=5)
    drone_dots.append(drone)

ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

# åˆ†æå›¾è¡¨
ax2.set_title('ğŸ§  æœ€æ–°ç­–ç•¥ç½‘ç»œè¾“å‡º', fontsize=12)
ax2.set_xlabel('æ—¶é—´æ­¥')
ax2.set_ylabel('åŠ¨ä½œå¼ºåº¦')
ax2.grid(True, alpha=0.3)

ax3.set_title('ğŸ¤ åä½œæŸå¤±æ•ˆæœ', fontsize=12) 
ax3.set_xlabel('æ—¶é—´æ­¥')
ax3.set_ylabel('å¹³å‡æ™ºèƒ½ä½“é—´è·')
ax3.grid(True, alpha=0.3)

ax4.set_title('ğŸ¯ ä»»åŠ¡å®Œæˆè¿›åº¦', fontsize=12)
ax4.set_xlabel('æ—¶é—´æ­¥')
ax4.set_ylabel('å¹³å‡ç›®æ ‡è·ç¦»')
ax4.grid(True, alpha=0.3)

def animate(frame):
    if frame >= num_steps:
        return trail_lines + drone_dots
    
    current_positions = positions_history[frame]
    
    # æ›´æ–°è½¨è¿¹å’Œæ™ºèƒ½ä½“
    for i in range(num_agents):
        trail_x = [pos[i, 0] for pos in positions_history[:frame+1]]
        trail_y = [pos[i, 1] for pos in positions_history[:frame+1]]
        trail_lines[i].set_data(trail_x, trail_y)
        
        drone_dots[i].set_data([current_positions[i, 0]], [current_positions[i, 1]])
    
    # æ›´æ–°åˆ†æå›¾è¡¨
    if frame > 5:
        steps = list(range(frame+1))
        
        # ç­–ç•¥è¾“å‡º
        if len(actions_history) > frame:
            action_magnitudes = []
            for step in range(frame+1):
                if step < len(actions_history):
                    step_actions = actions_history[step]
                    avg_magnitude = np.mean([np.linalg.norm(a) for a in step_actions])
                    action_magnitudes.append(avg_magnitude)
                else:
                    action_magnitudes.append(0)
            
            ax2.clear()
            ax2.plot(steps, action_magnitudes, 'purple', linewidth=3, label='å¹³å‡åŠ¨ä½œå¼ºåº¦')
            ax2.fill_between(steps, action_magnitudes, alpha=0.3, color='purple')
            ax2.set_title(f'ğŸ§  æœ€æ–°ç­–ç•¥ç½‘ç»œè¾“å‡º (æ­¥æ•°: {frame})')
            ax2.set_xlabel('æ—¶é—´æ­¥')
            ax2.set_ylabel('åŠ¨ä½œå¼ºåº¦')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        
        # åä½œæŒ‡æ ‡
        avg_distances = []
        for step in range(frame+1):
            if step < len(positions_history):
                pos = positions_history[step]
                distances = []
                for i in range(len(pos)):
                    for j in range(i+1, len(pos)):
                        dist = np.linalg.norm(pos[i] - pos[j])
                        distances.append(dist)
                avg_distances.append(np.mean(distances) if distances else 0)
            else:
                avg_distances.append(0)
        
        ax3.clear()
        ax3.plot(steps, avg_distances, 'orange', linewidth=3, label='å¹³å‡æ™ºèƒ½ä½“é—´è·')
        ax3.fill_between(steps, avg_distances, alpha=0.3, color='orange')
        ax3.set_title(f'ğŸ¤ åä½œæŸå¤±æ•ˆæœ (æ­¥æ•°: {frame})')
        ax3.set_xlabel('æ—¶é—´æ­¥')
        ax3.set_ylabel('å¹³å‡æ™ºèƒ½ä½“é—´è·')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # ä»»åŠ¡è¿›åº¦
        if len(goal_distances_history) > frame:
            avg_goal_dists = []
            for step in range(frame+1):
                if step < len(goal_distances_history):
                    avg_dist = np.mean(goal_distances_history[step])
                    avg_goal_dists.append(avg_dist)
                else:
                    avg_goal_dists.append(0)
            
            ax4.clear()
            ax4.plot(steps, avg_goal_dists, 'green', linewidth=3, label='å¹³å‡ç›®æ ‡è·ç¦»')
            ax4.fill_between(steps, avg_goal_dists, alpha=0.3, color='green')
            ax4.set_title(f'ğŸ¯ ä»»åŠ¡å®Œæˆè¿›åº¦ (æ­¥æ•°: {frame})')
            ax4.set_xlabel('æ—¶é—´æ­¥')
            ax4.set_ylabel('å¹³å‡ç›®æ ‡è·ç¦»')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
    
    return trail_lines + drone_dots

# åˆ›å»ºåŠ¨ç”»
anim = FuncAnimation(fig, animate, frames=num_steps, interval=150, blit=False, repeat=True)

# ä¿å­˜
timestamp = datetime.now().strftime('%%Y%%m%%d_%%H%%M%%S')
output_path = f'LATEST_REAL_COLLABORATION_{timestamp}.gif'

try:
    print('ğŸ’¾ ä¿å­˜æœ€æ–°çœŸå®æ¨¡å‹å¯è§†åŒ–...')
    anim.save(output_path, writer='pillow', fps=6, dpi=120)
    
    file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
    print(f'âœ… ä¿å­˜æˆåŠŸ: {output_path}')
    print(f'ğŸ“ æ–‡ä»¶å¤§å°: {file_size:.2f}MB')
    print(f'ğŸ” çœŸå®æ€§ä¿è¯:')
    print(f'   æ¨¡å‹æ¥æº: {model_path}')
    print(f'   è®­ç»ƒæ—¶é—´: 2025/08/05 19:59')
    print(f'   æ¨¡å‹å¤§å°: 2.5MB')
    print(f'   åŒ…å«ä¿®å¤: CBFç»´åº¦ä¿®å¤ + åä½œæŸå¤±åŠŸèƒ½')
    print(f'   æ•°æ®æ¥æº: 100%% æœ€æ–°çœŸå®ç¥ç»ç½‘ç»œç­–ç•¥è¾“å‡º')
    print(f'   ç”Ÿæˆå¸§æ•°: {num_steps}')
    
except Exception as e:
    print(f'âš ï¸ ä¿å­˜å¤±è´¥: {e}')
    static_path = f'LATEST_REAL_STATIC_{timestamp}.png'
    plt.tight_layout()
    plt.savefig(static_path, dpi=150, bbox_inches='tight')
    print(f'âœ… é™æ€å›¾ä¿å­˜: {static_path}')

plt.close()
print('ğŸ‰ æœ€æ–°çœŸå®æ¨¡å‹å¯è§†åŒ–å®Œæˆ!')
"

echo.
echo ğŸ‰ æœ€æ–°çœŸå®æ¨¡å‹å¯è§†åŒ–ç”Ÿæˆå®Œæˆ!
pause
 
 
 
 