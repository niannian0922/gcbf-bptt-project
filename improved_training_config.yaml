env:
  agent_radius: 0.05
  area_size: 2.0
  cbf_alpha: 1.5
  comm_radius: 1.0
  dt: 0.03
  mass: 0.1
  max_steps: 300
  num_agents: 6
  obstacles:
    bottleneck: true
    gap_position: 0.5
    gap_width: 0.4
    obstacle_radius: 0.05
    obstacle_spacing: 0.1
    wall_height: 1.6
    wall_thickness: 0.1
networks:
  cbf:
    hidden_dim: 256
    input_dim: 256
    n_layers: 4
  policy:
    memory:
      hidden_dim: 128
      num_layers: 2
    perception:
      activation: relu
      hidden_dim: 128
    policy_head:
      activation: relu
      alpha_bounds:
      - 0.8
      - 3.0
      hidden_dims:
      - 256
      - 128
      output_dim: 2
      predict_alpha: true
training:
  alpha_reg_weight: 0.01
  collision_penalty: 10.0
  control_weight: 0.1
  eval_horizon: 100
  eval_interval: 500
  goal_weight: 1.0
  horizon_length: 50
  jerk_weight: 0.05
  learning_rate: 0.0003
  lr_gamma: 0.9
  lr_step_size: 5000
  max_grad_norm: 0.5
  safety_weight: 20.0
  save_interval: 2000
  training_steps: 50000
  use_lr_scheduler: true
