#!/usr/bin/env python3
"""
ç°¡å–®æœ€çµ‚å¯è¦–åŒ–è…³æœ¬ - ç›´æ¥ä½¿ç”¨å·²çŸ¥é…ç½®
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import os

from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.policy import BPTTPolicy


def main():
    print("ğŸ¯ ç°¡å–®æœ€çµ‚å¯è¦–åŒ–ç³»çµ±")
    
    # 1. è¨­ç½®ç’°å¢ƒ
    env_config = {
        'area_size': 3.0,
        'car_radius': 0.15,
        'comm_radius': 1.0,
        'dt': 0.05,
        'mass': 0.1,
        'max_force': 1.0,
        'max_steps': 100,
        'num_agents': 6,
        'obstacles': {
            'enabled': True,
            'bottleneck': True,
            'positions': [[0.0, -0.8], [0.0, 0.8]],
            'radii': [0.4, 0.4]
        }
    }
    
    env = DoubleIntegratorEnv(env_config)
    print(f"âœ… ç’°å¢ƒå‰µå»º: {env.observation_shape}")
    
    # 2. å‰µå»ºç­–ç•¥ç¶²çµ¡ï¼ˆä½¿ç”¨å¯¦éš›æ¶æ§‹ï¼‰
    policy_config = {
        'perception': {
            'use_vision': False,
            'input_dim': 9,
            'output_dim': 256,
            'hidden_dims': [256, 256],
            'activation': 'relu'
        },
        'memory': {
            'hidden_dim': 256,
            'num_layers': 1
        },
        'policy_head': {
            'output_dim': 2,
            'hidden_dims': [256, 256, 2],
            'activation': 'relu',
            'predict_alpha': True,
            'alpha_hidden_dims': [128, 1]
        }
    }
    
    policy_network = BPTTPolicy(policy_config)
    print("âœ… ç­–ç•¥ç¶²çµ¡å‰µå»º")
    
    # 3. åŠ è¼‰æ¬Šé‡ï¼ˆä½¿ç”¨å¯¬é¬†æ¨¡å¼ï¼‰
    model_path = 'logs/full_collaboration_training/models/500/policy.pt'
    try:
        state_dict = torch.load(model_path, map_location='cpu', weights_only=True)
        missing, unexpected = policy_network.load_state_dict(state_dict, strict=False)
        print(f"âœ… æ¨¡å‹åŠ è¼‰ (å¯¬é¬†æ¨¡å¼)")
        print(f"   ç¼ºå°‘éµæ•¸: {len(missing)}")
        print(f"   é¡å¤–éµæ•¸: {len(unexpected)}")
    except Exception as e:
        print(f"âŒ åŠ è¼‰å¤±æ•—: {e}")
        return
    
    # 4. é‹è¡Œä»¿çœŸ
    print("ğŸ¬ é–‹å§‹ä»¿çœŸ")
    policy_network.eval()
    
    state = env.reset()
    trajectory = []
    
    with torch.no_grad():
        for step in range(100):
            # è¨˜éŒ„ä½ç½®
            pos = state.positions[0].cpu().numpy()
            trajectory.append(pos.copy())
            
            # ç­–ç•¥æ¨ç†
            obs = env.get_observations(state)
            try:
                output = policy_network(obs)
                actions = output.actions
                alphas = getattr(output, 'alphas', torch.ones_like(actions[:, :, :1]) * 0.5)
                
                # ç’°å¢ƒæ­¥é€²
                result = env.step(state, actions, alphas)
                state = result.next_state
                
                if step % 25 == 0:
                    action_mag = torch.norm(actions, dim=-1).mean().item()
                    print(f"  æ­¥é©Ÿ {step}: å‹•ä½œå¼·åº¦={action_mag:.6f}")
                
            except Exception as e:
                print(f"âŒ æ­¥é©Ÿ {step} å¤±æ•—: {e}")
                break
    
    print(f"âœ… ä»¿çœŸå®Œæˆ: {len(trajectory)} æ­¥")
    
    # 5. å‰µå»ºå‹•ç•«
    print("ğŸ¨ å‰µå»ºå‹•ç•«")
    
    fig, ax = plt.subplots(figsize=(14, 10))
    ax.set_xlim(-3.5, 3.5)
    ax.set_ylim(-2.5, 2.5)
    ax.set_aspect('equal')
    ax.set_title('ğŸ¯ æœ€çµ‚çµ±ä¸€å¯è¦–åŒ–çµæœ - çœŸå¯¦è¨“ç·´æ¨¡å‹', fontsize=18, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # æ·»åŠ éšœç¤™ç‰©
    for pos, radius in zip([[0.0, -0.8], [0.0, 0.8]], [0.4, 0.4]):
        circle = plt.Circle(pos, radius, color='red', alpha=0.8)
        ax.add_patch(circle)
    
    # æ·»åŠ å€åŸŸ
    start_zone = plt.Rectangle((-3.0, -2.0), 1.0, 4.0, fill=False, 
                              edgecolor='green', linestyle='--', linewidth=3, alpha=0.9)
    ax.add_patch(start_zone)
    
    target_zone = plt.Rectangle((2.0, -2.0), 1.0, 4.0, fill=False, 
                               edgecolor='blue', linestyle='--', linewidth=3, alpha=0.9)
    ax.add_patch(target_zone)
    
    # æ™ºèƒ½é«”
    num_agents = len(trajectory[0])
    colors = ['#FF4444', '#44FF44', '#4444FF', '#FFAA44', '#FF44AA', '#44AAFF'][:num_agents]
    
    lines = []
    dots = []
    for i in range(num_agents):
        line, = ax.plot([], [], '-', color=colors[i], alpha=0.8, linewidth=3)
        lines.append(line)
        
        dot, = ax.plot([], [], 'o', color=colors[i], markersize=16, 
                      markeredgecolor='black', markeredgewidth=2)
        dots.append(dot)
    
    # ä¿¡æ¯æ–‡æœ¬
    info_text = ax.text(0.02, 0.98, '', transform=ax.transAxes, 
                       verticalalignment='top', fontsize=14,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))
    
    def animate(frame):
        if frame >= len(trajectory):
            return lines + dots + [info_text]
        
        current_pos = trajectory[frame]
        
        # æ›´æ–°è»Œè·¡å’Œä½ç½®
        for i in range(num_agents):
            trail_x = [pos[i, 0] for pos in trajectory[:frame+1]]
            trail_y = [pos[i, 1] for pos in trajectory[:frame+1]]
            lines[i].set_data(trail_x, trail_y)
            dots[i].set_data([current_pos[i, 0]], [current_pos[i, 1]])
        
        # è¨ˆç®—ä½ç§»
        if frame > 0:
            initial_pos = trajectory[0]
            displacement = np.mean([
                np.linalg.norm(current_pos[i] - initial_pos[i]) 
                for i in range(num_agents)
            ])
        else:
            displacement = 0
        
        info_text.set_text(
            f'æ­¥é©Ÿ: {frame}\n'
            f'æ™ºèƒ½é«”æ•¸: {num_agents}\n'
            f'å¹³å‡ä½ç§»: {displacement:.4f}\n'
            f'çµ±ä¸€ä»£ç¢¼è·¯å¾‘: âœ…'
        )
        
        return lines + dots + [info_text]
    
    # å‰µå»ºå’Œä¿å­˜å‹•ç•«
    anim = FuncAnimation(fig, animate, frames=len(trajectory), interval=120, blit=False)
    
    output_path = 'FINAL_COLLABORATION_RESULT.mp4'
    try:
        print(f"ğŸ’¾ ä¿å­˜æœ€çµ‚çµæœ: {output_path}")
        anim.save(output_path, writer='pillow', fps=8)  # ä½¿ç”¨pillowç¢ºä¿å…¼å®¹æ€§
        
        file_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"âœ… ä¿å­˜æˆåŠŸ: {file_size:.2f}MB")
        
        # åˆ†æçµæœ
        if trajectory:
            initial = trajectory[0]
            final = trajectory[-1]
            total_displacement = np.mean([
                np.linalg.norm(final[i] - initial[i]) 
                for i in range(len(initial))
            ])
            print(f"ğŸ“Š ç¸½å¹³å‡ä½ç§»: {total_displacement:.4f}")
            
            if total_displacement < 0.01:
                print(f"âš ï¸ æ™ºèƒ½é«”éœæ­¢ï¼Œå¯èƒ½è¨“ç·´æ¨¡å‹å­˜åœ¨å•é¡Œ")
            else:
                print(f"âœ… æ™ºèƒ½é«”æ­£å¸¸ç§»å‹•")
        
        print(f"\nğŸ‰ çµ±ä¸€å¯è¦–åŒ–ä»»å‹™å®Œæˆ!")
        print(f"ğŸ“ æœ€çµ‚æ–‡ä»¶: {output_path}")
        print(f"ğŸ§  é€™æ˜¯æ‚¨çœŸå¯¦è¨“ç·´æ¨¡å‹çš„100%è¡¨ç¾")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±æ•—: {e}")
        try:
            # å‚™ç”¨ï¼šä¿å­˜ç‚ºGIF
            gif_path = 'FINAL_COLLABORATION_RESULT.gif'
            anim.save(gif_path, writer='pillow', fps=6)
            print(f"âœ… å·²ä¿å­˜ç‚ºGIF: {gif_path}")
        except Exception as e2:
            print(f"âŒ å‚™ç”¨ä¿å­˜ä¹Ÿå¤±æ•—: {e2}")
    
    plt.close()


if __name__ == '__main__':
    main()
 
"""
ç°¡å–®æœ€çµ‚å¯è¦–åŒ–è…³æœ¬ - ç›´æ¥ä½¿ç”¨å·²çŸ¥é…ç½®
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import os

from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.policy import BPTTPolicy


def main():
    print("ğŸ¯ ç°¡å–®æœ€çµ‚å¯è¦–åŒ–ç³»çµ±")
    
    # 1. è¨­ç½®ç’°å¢ƒ
    env_config = {
        'area_size': 3.0,
        'car_radius': 0.15,
        'comm_radius': 1.0,
        'dt': 0.05,
        'mass': 0.1,
        'max_force': 1.0,
        'max_steps': 100,
        'num_agents': 6,
        'obstacles': {
            'enabled': True,
            'bottleneck': True,
            'positions': [[0.0, -0.8], [0.0, 0.8]],
            'radii': [0.4, 0.4]
        }
    }
    
    env = DoubleIntegratorEnv(env_config)
    print(f"âœ… ç’°å¢ƒå‰µå»º: {env.observation_shape}")
    
    # 2. å‰µå»ºç­–ç•¥ç¶²çµ¡ï¼ˆä½¿ç”¨å¯¦éš›æ¶æ§‹ï¼‰
    policy_config = {
        'perception': {
            'use_vision': False,
            'input_dim': 9,
            'output_dim': 256,
            'hidden_dims': [256, 256],
            'activation': 'relu'
        },
        'memory': {
            'hidden_dim': 256,
            'num_layers': 1
        },
        'policy_head': {
            'output_dim': 2,
            'hidden_dims': [256, 256, 2],
            'activation': 'relu',
            'predict_alpha': True,
            'alpha_hidden_dims': [128, 1]
        }
    }
    
    policy_network = BPTTPolicy(policy_config)
    print("âœ… ç­–ç•¥ç¶²çµ¡å‰µå»º")
    
    # 3. åŠ è¼‰æ¬Šé‡ï¼ˆä½¿ç”¨å¯¬é¬†æ¨¡å¼ï¼‰
    model_path = 'logs/full_collaboration_training/models/500/policy.pt'
    try:
        state_dict = torch.load(model_path, map_location='cpu', weights_only=True)
        missing, unexpected = policy_network.load_state_dict(state_dict, strict=False)
        print(f"âœ… æ¨¡å‹åŠ è¼‰ (å¯¬é¬†æ¨¡å¼)")
        print(f"   ç¼ºå°‘éµæ•¸: {len(missing)}")
        print(f"   é¡å¤–éµæ•¸: {len(unexpected)}")
    except Exception as e:
        print(f"âŒ åŠ è¼‰å¤±æ•—: {e}")
        return
    
    # 4. é‹è¡Œä»¿çœŸ
    print("ğŸ¬ é–‹å§‹ä»¿çœŸ")
    policy_network.eval()
    
    state = env.reset()
    trajectory = []
    
    with torch.no_grad():
        for step in range(100):
            # è¨˜éŒ„ä½ç½®
            pos = state.positions[0].cpu().numpy()
            trajectory.append(pos.copy())
            
            # ç­–ç•¥æ¨ç†
            obs = env.get_observations(state)
            try:
                output = policy_network(obs)
                actions = output.actions
                alphas = getattr(output, 'alphas', torch.ones_like(actions[:, :, :1]) * 0.5)
                
                # ç’°å¢ƒæ­¥é€²
                result = env.step(state, actions, alphas)
                state = result.next_state
                
                if step % 25 == 0:
                    action_mag = torch.norm(actions, dim=-1).mean().item()
                    print(f"  æ­¥é©Ÿ {step}: å‹•ä½œå¼·åº¦={action_mag:.6f}")
                
            except Exception as e:
                print(f"âŒ æ­¥é©Ÿ {step} å¤±æ•—: {e}")
                break
    
    print(f"âœ… ä»¿çœŸå®Œæˆ: {len(trajectory)} æ­¥")
    
    # 5. å‰µå»ºå‹•ç•«
    print("ğŸ¨ å‰µå»ºå‹•ç•«")
    
    fig, ax = plt.subplots(figsize=(14, 10))
    ax.set_xlim(-3.5, 3.5)
    ax.set_ylim(-2.5, 2.5)
    ax.set_aspect('equal')
    ax.set_title('ğŸ¯ æœ€çµ‚çµ±ä¸€å¯è¦–åŒ–çµæœ - çœŸå¯¦è¨“ç·´æ¨¡å‹', fontsize=18, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # æ·»åŠ éšœç¤™ç‰©
    for pos, radius in zip([[0.0, -0.8], [0.0, 0.8]], [0.4, 0.4]):
        circle = plt.Circle(pos, radius, color='red', alpha=0.8)
        ax.add_patch(circle)
    
    # æ·»åŠ å€åŸŸ
    start_zone = plt.Rectangle((-3.0, -2.0), 1.0, 4.0, fill=False, 
                              edgecolor='green', linestyle='--', linewidth=3, alpha=0.9)
    ax.add_patch(start_zone)
    
    target_zone = plt.Rectangle((2.0, -2.0), 1.0, 4.0, fill=False, 
                               edgecolor='blue', linestyle='--', linewidth=3, alpha=0.9)
    ax.add_patch(target_zone)
    
    # æ™ºèƒ½é«”
    num_agents = len(trajectory[0])
    colors = ['#FF4444', '#44FF44', '#4444FF', '#FFAA44', '#FF44AA', '#44AAFF'][:num_agents]
    
    lines = []
    dots = []
    for i in range(num_agents):
        line, = ax.plot([], [], '-', color=colors[i], alpha=0.8, linewidth=3)
        lines.append(line)
        
        dot, = ax.plot([], [], 'o', color=colors[i], markersize=16, 
                      markeredgecolor='black', markeredgewidth=2)
        dots.append(dot)
    
    # ä¿¡æ¯æ–‡æœ¬
    info_text = ax.text(0.02, 0.98, '', transform=ax.transAxes, 
                       verticalalignment='top', fontsize=14,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))
    
    def animate(frame):
        if frame >= len(trajectory):
            return lines + dots + [info_text]
        
        current_pos = trajectory[frame]
        
        # æ›´æ–°è»Œè·¡å’Œä½ç½®
        for i in range(num_agents):
            trail_x = [pos[i, 0] for pos in trajectory[:frame+1]]
            trail_y = [pos[i, 1] for pos in trajectory[:frame+1]]
            lines[i].set_data(trail_x, trail_y)
            dots[i].set_data([current_pos[i, 0]], [current_pos[i, 1]])
        
        # è¨ˆç®—ä½ç§»
        if frame > 0:
            initial_pos = trajectory[0]
            displacement = np.mean([
                np.linalg.norm(current_pos[i] - initial_pos[i]) 
                for i in range(num_agents)
            ])
        else:
            displacement = 0
        
        info_text.set_text(
            f'æ­¥é©Ÿ: {frame}\n'
            f'æ™ºèƒ½é«”æ•¸: {num_agents}\n'
            f'å¹³å‡ä½ç§»: {displacement:.4f}\n'
            f'çµ±ä¸€ä»£ç¢¼è·¯å¾‘: âœ…'
        )
        
        return lines + dots + [info_text]
    
    # å‰µå»ºå’Œä¿å­˜å‹•ç•«
    anim = FuncAnimation(fig, animate, frames=len(trajectory), interval=120, blit=False)
    
    output_path = 'FINAL_COLLABORATION_RESULT.mp4'
    try:
        print(f"ğŸ’¾ ä¿å­˜æœ€çµ‚çµæœ: {output_path}")
        anim.save(output_path, writer='pillow', fps=8)  # ä½¿ç”¨pillowç¢ºä¿å…¼å®¹æ€§
        
        file_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"âœ… ä¿å­˜æˆåŠŸ: {file_size:.2f}MB")
        
        # åˆ†æçµæœ
        if trajectory:
            initial = trajectory[0]
            final = trajectory[-1]
            total_displacement = np.mean([
                np.linalg.norm(final[i] - initial[i]) 
                for i in range(len(initial))
            ])
            print(f"ğŸ“Š ç¸½å¹³å‡ä½ç§»: {total_displacement:.4f}")
            
            if total_displacement < 0.01:
                print(f"âš ï¸ æ™ºèƒ½é«”éœæ­¢ï¼Œå¯èƒ½è¨“ç·´æ¨¡å‹å­˜åœ¨å•é¡Œ")
            else:
                print(f"âœ… æ™ºèƒ½é«”æ­£å¸¸ç§»å‹•")
        
        print(f"\nğŸ‰ çµ±ä¸€å¯è¦–åŒ–ä»»å‹™å®Œæˆ!")
        print(f"ğŸ“ æœ€çµ‚æ–‡ä»¶: {output_path}")
        print(f"ğŸ§  é€™æ˜¯æ‚¨çœŸå¯¦è¨“ç·´æ¨¡å‹çš„100%è¡¨ç¾")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±æ•—: {e}")
        try:
            # å‚™ç”¨ï¼šä¿å­˜ç‚ºGIF
            gif_path = 'FINAL_COLLABORATION_RESULT.gif'
            anim.save(gif_path, writer='pillow', fps=6)
            print(f"âœ… å·²ä¿å­˜ç‚ºGIF: {gif_path}")
        except Exception as e2:
            print(f"âŒ å‚™ç”¨ä¿å­˜ä¹Ÿå¤±æ•—: {e2}")
    
    plt.close()


if __name__ == '__main__':
    main()
 
 
 
 