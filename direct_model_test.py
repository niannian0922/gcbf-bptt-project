#!/usr/bin/env python3
"""
ç›´æ¥æ¨¡å‹æµ‹è¯• - æœ€ç®€åŒ–ç‰ˆæœ¬
"""
print("å¼€å§‹ç›´æ¥æ¨¡å‹æµ‹è¯•")

try:
    import torch
    print("âœ… torchå¯¼å…¥æˆåŠŸ")
    
    import os
    model_path = 'logs/full_collaboration_training/models/500/policy.pt'
    print(f"ğŸ“ æ£€æŸ¥æ–‡ä»¶: {os.path.exists(model_path)}")
    
    if os.path.exists(model_path):
        size = os.path.getsize(model_path) / (1024*1024)
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {size:.1f}MB")
        
        print("ğŸ”„ å°è¯•åŠ è½½æƒé‡...")
        policy_dict = torch.load(model_path, map_location='cpu', weights_only=True)
        print(f"âœ… æƒé‡åŠ è½½æˆåŠŸ: {len(policy_dict)} å±‚")
        
        # æŸ¥çœ‹ç¬¬ä¸€ä¸ªæƒé‡çš„å½¢çŠ¶
        first_key = list(policy_dict.keys())[0]
        first_shape = policy_dict[first_key].shape
        print(f"ğŸ“ ç¬¬ä¸€å±‚å½¢çŠ¶: {first_key} -> {first_shape}")
        
        # æŸ¥æ‰¾æ„ŸçŸ¥å±‚
        if 'perception.mlp.0.weight' in policy_dict:
            perception_shape = policy_dict['perception.mlp.0.weight'].shape
            print(f"ğŸ§  æ„ŸçŸ¥å±‚å½¢çŠ¶: {perception_shape}")
            print(f"ğŸ¯ è¾“å…¥ç»´åº¦: {perception_shape[1]}")
        
        print("ğŸ‰ æ¨¡å‹æµ‹è¯•æˆåŠŸ!")
        
    else:
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        
except Exception as e:
    print(f"âŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()

print("æµ‹è¯•å®Œæˆ")
 
 
 
 