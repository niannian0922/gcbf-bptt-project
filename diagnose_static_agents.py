#!/usr/bin/env python3
"""
ğŸ” è¯Šæ–­æ™ºèƒ½ä½“ä¸åŠ¨çš„é—®é¢˜
æ£€æŸ¥ç­–ç•¥ç½‘ç»œã€ç¯å¢ƒé…ç½®ã€åŠ¨ä½œç”Ÿæˆç­‰å…³é”®ç¯èŠ‚
"""

import torch
import torch.nn as nn
import numpy as np
import yaml
import os

def diagnose_static_agents():
    """è¯Šæ–­æ™ºèƒ½ä½“é™æ­¢çš„åŸå› """
    print("ğŸ” è¯Šæ–­æ™ºèƒ½ä½“é™æ­¢é—®é¢˜")
    print("=" * 60)
    
    try:
        # 1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        model_dir = "logs/full_collaboration_training/models/500"
        policy_path = os.path.join(model_dir, "policy.pt")
        
        print("ğŸ“ æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
        if not os.path.exists(policy_path):
            print(f"âŒ ç­–ç•¥æ¨¡å‹æœªæ‰¾åˆ°: {policy_path}")
            return False
        
        file_size = os.path.getsize(policy_path) / (1024 * 1024)  # MB
        print(f"âœ… ç­–ç•¥æ¨¡å‹å­˜åœ¨: {file_size:.1f}MB")
        
        # 2. åŠ è½½é…ç½®
        print("\nğŸ“‹ åŠ è½½é…ç½®...")
        with open('config/simple_collaboration.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # è¡¥å……ç½‘ç»œé…ç½®
        config['networks'] = {
            'policy': {
                'type': 'bptt',
                'layers': [256, 256],
                'activation': 'relu',
                'hidden_dim': 256,
                'input_dim': 6,
                'node_dim': 6,
                'edge_dim': 4,
                'n_layers': 2,
                'msg_hidden_sizes': [256, 256],
                'aggr_hidden_sizes': [256],
                'update_hidden_sizes': [256, 256],
                'predict_alpha': True,
                'perception': {
                    'input_dim': 6,
                    'hidden_dim': 256,
                    'num_layers': 2,
                    'activation': 'relu',
                    'use_vision': False
                },
                'memory': {
                    'hidden_dim': 256,
                    'memory_size': 32,
                    'num_heads': 4
                },
                'policy_head': {
                    'output_dim': 2,
                    'predict_alpha': True,
                    'hidden_dims': [256, 256],
                    'action_scale': 1.0
                }
            }
        }
        
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   ğŸ¤– æ™ºèƒ½ä½“æ•°é‡: {config['env']['num_agents']}")
        print(f"   ğŸ“ ç¤¾äº¤åŠå¾„: {config['env']['social_radius']}")
        print(f"   âš¡ æœ€å¤§åŠ›: {config['env']['max_force']}")
        print(f"   â° æ—¶é—´æ­¥é•¿: {config['env']['dt']}")
        
        # 3. åˆ›å»ºç¯å¢ƒ
        print("\nğŸŒ åˆ›å»ºç¯å¢ƒ...")
        from gcbfplus.env import DoubleIntegratorEnv
        from gcbfplus.env.multi_agent_env import MultiAgentState
        
        device = torch.device('cpu')
        env = DoubleIntegratorEnv(config['env'])
        env = env.to(device)
        
        print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        print(f"   ğŸ“Š è§‚æµ‹ç»´åº¦: {env.observation_shape}")
        print(f"   ğŸ¯ åŠ¨ä½œç»´åº¦: {env.action_shape}")
        print(f"   ğŸ¤– æ™ºèƒ½ä½“åŠå¾„: {env.agent_radius}")
        print(f"   âš¡ æœ€å¤§åŠ›: {env.max_force}")
        
        # 4. åˆ›å»ºç­–ç•¥ç½‘ç»œ
        print("\nğŸ§  åˆ›å»ºç­–ç•¥ç½‘ç»œ...")
        from gcbfplus.policy.bptt_policy import create_policy_from_config
        
        policy_network = create_policy_from_config(config['networks']['policy'])
        policy_network = policy_network.to(device)
        
        print(f"âœ… ç­–ç•¥ç½‘ç»œåˆ›å»ºæˆåŠŸ")
        
        # 5. å°è¯•åŠ è½½æƒé‡
        print("\nğŸ’¾ åŠ è½½ç­–ç•¥æƒé‡...")
        try:
            policy_state_dict = torch.load(policy_path, map_location='cpu', weights_only=True)
            policy_network.load_state_dict(policy_state_dict)
            print(f"âœ… ç­–ç•¥æƒé‡åŠ è½½æˆåŠŸ")
            weights_loaded = True
        except Exception as e:
            print(f"âŒ ç­–ç•¥æƒé‡åŠ è½½å¤±è´¥: {e}")
            print(f"ğŸ”§ ä½¿ç”¨éšæœºæƒé‡æµ‹è¯•...")
            weights_loaded = False
        
        # 6. åˆ›å»ºæµ‹è¯•çŠ¶æ€
        print("\nğŸ¬ åˆ›å»ºæµ‹è¯•çŠ¶æ€...")
        batch_size = 1
        num_agents = config['env']['num_agents']
        
        # åˆ›å»ºåˆ†æ•£çš„åˆå§‹ä½ç½®
        positions = torch.zeros(batch_size, num_agents, 2, device=device)
        velocities = torch.zeros(batch_size, num_agents, 2, device=device)
        goals = torch.zeros(batch_size, num_agents, 2, device=device)
        
        for i in range(num_agents):
            # å·¦ä¾§èµ·å§‹ä½ç½®
            positions[0, i] = torch.tensor([-1.5 + i * 0.3, (i - num_agents/2) * 0.5], device=device)
            # å³ä¾§ç›®æ ‡ä½ç½®
            goals[0, i] = torch.tensor([1.5 + i * 0.3, (i - num_agents/2) * 0.5], device=device)
        
        test_state = MultiAgentState(
            positions=positions,
            velocities=velocities,
            goals=goals,
            batch_size=batch_size
        )
        
        print(f"âœ… æµ‹è¯•çŠ¶æ€åˆ›å»ºæˆåŠŸ")
        print(f"   ğŸ“ èµ·å§‹ä½ç½®: {positions[0, 0]}")
        print(f"   ğŸ¯ ç›®æ ‡ä½ç½®: {goals[0, 0]}")
        
        # 7. æµ‹è¯•è§‚æµ‹ç”Ÿæˆ
        print("\nğŸ‘ï¸ æµ‹è¯•è§‚æµ‹ç”Ÿæˆ...")
        try:
            observations = env.get_observation(test_state)
            print(f"âœ… è§‚æµ‹ç”ŸæˆæˆåŠŸ")
            print(f"   ğŸ“Š è§‚æµ‹å½¢çŠ¶: {observations.shape}")
            print(f"   ğŸ“ˆ è§‚æµ‹èŒƒå›´: [{observations.min():.3f}, {observations.max():.3f}]")
            print(f"   ğŸ“‹ ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“è§‚æµ‹: {observations[0, 0, :3]}")  # æ˜¾ç¤ºå‰3ç»´
        except Exception as e:
            print(f"âŒ è§‚æµ‹ç”Ÿæˆå¤±è´¥: {e}")
            return False
        
        # 8. æµ‹è¯•ç­–ç•¥ç½‘ç»œå‰å‘ä¼ æ’­
        print("\nğŸ¤– æµ‹è¯•ç­–ç•¥ç½‘ç»œ...")
        try:
            with torch.no_grad():
                actions, alphas = policy_network(observations)
            
            print(f"âœ… ç­–ç•¥ç½‘ç»œå·¥ä½œæ­£å¸¸")
            print(f"   ğŸ¯ åŠ¨ä½œå½¢çŠ¶: {actions.shape}")
            print(f"   ğŸ“ˆ åŠ¨ä½œèŒƒå›´: [{actions.min():.3f}, {actions.max():.3f}]")
            print(f"   ğŸ”„ Alphaå½¢çŠ¶: {alphas.shape if alphas is not None else 'None'}")
            
            # æ£€æŸ¥åŠ¨ä½œæ˜¯å¦ä¸ºé›¶
            action_magnitude = torch.norm(actions, dim=-1)
            print(f"   âš¡ åŠ¨ä½œå¹…åº¦: å¹³å‡={action_magnitude.mean():.6f}, æœ€å¤§={action_magnitude.max():.6f}")
            
            if action_magnitude.max() < 1e-6:
                print(f"âš ï¸ è­¦å‘Š: åŠ¨ä½œå¹…åº¦æå°ï¼Œæ™ºèƒ½ä½“å¯èƒ½ä¸ä¼šç§»åŠ¨!")
                print(f"   å¯èƒ½åŸå› : 1) æƒé‡æœªæ­£ç¡®åŠ è½½ 2) ç½‘ç»œè¾“å‡ºè¢«çº¦æŸ 3) å­¦ä¹ ç‡è¿‡å°")
            
            # æ˜¾ç¤ºå…·ä½“åŠ¨ä½œ
            print(f"   ğŸ“‹ ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“åŠ¨ä½œ: {actions[0, 0]}")
            print(f"   ğŸ“‹ ç¬¬äºŒä¸ªæ™ºèƒ½ä½“åŠ¨ä½œ: {actions[0, 1]}")
            
        except Exception as e:
            print(f"âŒ ç­–ç•¥ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
            return False
        
        # 9. æµ‹è¯•ç¯å¢ƒæ­¥è¿›
        print("\nğŸ”„ æµ‹è¯•ç¯å¢ƒæ­¥è¿›...")
        try:
            step_result = env.step(test_state, actions, alphas)
            new_state = step_result.next_state
            
            print(f"âœ… ç¯å¢ƒæ­¥è¿›æˆåŠŸ")
            
            # æ£€æŸ¥ä½ç½®å˜åŒ–
            position_change = torch.norm(new_state.positions - test_state.positions, dim=-1)
            print(f"   ğŸ“ ä½ç½®å˜åŒ–: å¹³å‡={position_change.mean():.6f}, æœ€å¤§={position_change.max():.6f}")
            
            if position_change.max() < 1e-6:
                print(f"âš ï¸ è­¦å‘Š: ä½ç½®å˜åŒ–æå°ï¼Œæ™ºèƒ½ä½“å®é™…ä¸Šæ²¡æœ‰ç§»åŠ¨!")
                print(f"   ğŸ“ åŸå§‹ä½ç½®: {test_state.positions[0, 0]}")
                print(f"   ğŸ“ æ–°ä½ç½®: {new_state.positions[0, 0]}")
                print(f"   âš¡ åº”ç”¨åŠ¨ä½œ: {actions[0, 0]}")
                print(f"   â° æ—¶é—´æ­¥é•¿: {env.dt}")
                print(f"   ğŸ’ª æœ€å¤§åŠ›: {env.max_force}")
            else:
                print(f"âœ… æ™ºèƒ½ä½“æ­£å¸¸ç§»åŠ¨")
                print(f"   ğŸ“ ä½ç½®å˜åŒ–: {new_state.positions[0, 0] - test_state.positions[0, 0]}")
            
        except Exception as e:
            print(f"âŒ ç¯å¢ƒæ­¥è¿›å¤±è´¥: {e}")
            return False
        
        # 10. åˆ†æé—®é¢˜
        print(f"\nğŸ“Š é—®é¢˜åˆ†ææ€»ç»“:")
        print(f"=" * 50)
        
        if not weights_loaded:
            print(f"ğŸ”´ ä¸»è¦é—®é¢˜: ç­–ç•¥æƒé‡æœªæ­£ç¡®åŠ è½½")
            print(f"   ğŸ’¡ è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥æƒé‡æ–‡ä»¶å…¼å®¹æ€§")
        elif action_magnitude.max() < 1e-6:
            print(f"ğŸ”´ ä¸»è¦é—®é¢˜: ç­–ç•¥ç½‘ç»œè¾“å‡ºæå°åŠ¨ä½œ")
            print(f"   ğŸ’¡ å¯èƒ½åŸå› : ç½‘ç»œæƒé‡åˆå§‹åŒ–ã€å­¦ä¹ ä¸å……åˆ†ã€æˆ–è¾“å‡ºè¢«é™åˆ¶")
        elif position_change.max() < 1e-6:
            print(f"ğŸ”´ ä¸»è¦é—®é¢˜: ç¯å¢ƒæœªå“åº”åŠ¨ä½œè¾“å…¥")
            print(f"   ğŸ’¡ å¯èƒ½åŸå› : æ—¶é—´æ­¥é•¿è¿‡å°ã€åŠ¨ä½œç¼©æ”¾é—®é¢˜")
        else:
            print(f"ğŸŸ¢ æ‰€æœ‰ç»„ä»¶æ­£å¸¸å·¥ä½œ")
            print(f"   ğŸ’¡ é—®é¢˜å¯èƒ½åœ¨å¯è§†åŒ–è„šæœ¬çš„åŠ¨ç”»é€»è¾‘")
        
        # 11. å»ºè®®çš„ä¿®å¤æ–¹æ¡ˆ
        print(f"\nğŸ› ï¸ å»ºè®®ä¿®å¤æ–¹æ¡ˆ:")
        print(f"1. ä½¿ç”¨éšæœºåŠ¨ä½œæµ‹è¯•ç¯å¢ƒå“åº”æ€§")
        print(f"2. æ£€æŸ¥ç­–ç•¥ç½‘ç»œæƒé‡æ–‡ä»¶")
        print(f"3. è°ƒæ•´åŠ¨ä½œç¼©æ”¾å’Œæ—¶é—´æ­¥é•¿")
        print(f"4. éªŒè¯å¯è§†åŒ–åŠ¨ç”»é€»è¾‘")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¯Šæ–­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸ” æ™ºèƒ½ä½“é™æ­¢é—®é¢˜è¯Šæ–­ç³»ç»Ÿ")
    print("æ£€æŸ¥ç­–ç•¥ç½‘ç»œã€ç¯å¢ƒé…ç½®ã€åŠ¨ä½œç”Ÿæˆç­‰å…³é”®ç¯èŠ‚")
    print("=" * 70)
    
    success = diagnose_static_agents()
    
    if success:
        print(f"\nâœ… è¯Šæ–­å®Œæˆ!")
    else:
        print(f"\nâŒ è¯Šæ–­å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")