#!/usr/bin/env python3
"""
ğŸ”§ ä¿®å¤è¾¹ç•Œå¯è§†åŒ–
è§£å†³æ— äººæœºè·‘å‡ºç”»é¢çš„é—®é¢˜
ç¡®ä¿æ™ºèƒ½ä½“åœ¨å¯è§†èŒƒå›´å†…è¿åŠ¨
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import yaml
import os
from datetime import datetime

def create_bounded_visualization():
    """åˆ›å»ºè¾¹ç•Œé™åˆ¶çš„å¯è§†åŒ–"""
    print("ğŸ”§ ä¿®å¤è¾¹ç•Œå¯è§†åŒ–é—®é¢˜")
    print("=" * 50)
    print("ğŸ¯ ç›®æ ‡: ç¡®ä¿æ— äººæœºåœ¨ç”»é¢å†…ç§»åŠ¨")
    print("ğŸš å†…å®¹: æ— äººæœºç¼–é˜Ÿåä½œç»•è¿‡éšœç¢ç‰©")
    print("=" * 50)
    
    # ä¿®å¤åçš„é…ç½®
    config = {
        'env': {
            'name': 'DoubleIntegrator',
            'num_agents': 6,
            'area_size': 4.0,  # å¢å¤§åŒºåŸŸ
            'dt': 0.02,  # å‡å°æ—¶é—´æ­¥é•¿ï¼Œå¢åŠ ç¨³å®šæ€§
            'mass': 0.5,  # å¢å¤§è´¨é‡ï¼Œå‡å°‘åŠ é€Ÿåº¦
            'agent_radius': 0.15,
            'comm_radius': 1.0,
            'max_force': 0.5,  # å‡å°æœ€å¤§åŠ›
            'max_steps': 150,
            'social_radius': 0.4,
            'obstacles': {
                'enabled': True,
                'count': 2,
                'positions': [[0, 0.6], [0, -0.6]],
                'radii': [0.25, 0.25]
            }
        }
    }
    
    try:
        # å¯¼å…¥
        from gcbfplus.env import DoubleIntegratorEnv
        from gcbfplus.env.multi_agent_env import MultiAgentState
        from gcbfplus.policy.bptt_policy import create_policy_from_config
        
        print("âœ… å¯¼å…¥æˆåŠŸ")
        
        # åˆ›å»ºç¯å¢ƒ
        device = torch.device('cpu')
        env = DoubleIntegratorEnv(config['env'])
        env = env.to(device)
        
        print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        print(f"   ğŸ“Š è§‚æµ‹ç»´åº¦: {env.observation_shape}")
        print(f"   ğŸ¯ åŠ¨ä½œç»´åº¦: {env.action_shape}")
        print(f"   âš¡ æœ€å¤§åŠ›: {env.max_force}")
        print(f"   â° æ—¶é—´æ­¥é•¿: {env.dt}")
        
        # åˆ›å»ºç­–ç•¥ç½‘ç»œé…ç½®ï¼ˆé™ä½åŠ¨ä½œç¼©æ”¾ï¼‰
        policy_config = {
            'type': 'bptt',
            'hidden_dim': 128,  # å‡å°ç½‘ç»œ
            'input_dim': 6,
            'node_dim': 6,
            'edge_dim': 4,
            'n_layers': 1,  # å‡å°‘å±‚æ•°
            'msg_hidden_sizes': [128],
            'aggr_hidden_sizes': [128],
            'update_hidden_sizes': [128],
            'predict_alpha': True,
            'perception': {
                'input_dim': 6,
                'hidden_dim': 128,
                'num_layers': 1,  # å‡å°‘å±‚æ•°
                'activation': 'relu',
                'use_vision': False
            },
            'memory': {
                'hidden_dim': 128,
                'memory_size': 16,
                'num_heads': 2
            },
            'policy_head': {
                'output_dim': 2,
                'predict_alpha': True,
                'hidden_dims': [64],  # å‡å°è¾“å‡ºå±‚
                'action_scale': 0.2  # å¤§å¹…å‡å°åŠ¨ä½œç¼©æ”¾
            }
        }
        
        # åˆ›å»ºç­–ç•¥ç½‘ç»œ
        policy = create_policy_from_config(policy_config)
        policy.eval()
        
        print("âœ… ç­–ç•¥ç½‘ç»œåˆ›å»ºæˆåŠŸï¼ˆé™ä½åŠ¨ä½œç¼©æ”¾ï¼‰")
        
        # åˆ›å»ºç¨³å®šçš„åˆå§‹åœºæ™¯
        initial_state = create_stable_scenario(device, config['env']['num_agents'])
        
        print("âœ… ç¨³å®šåœºæ™¯åˆ›å»ºæˆåŠŸ")
        
        # è¿è¡Œç¨³å®šæ¨¡æ‹Ÿ
        trajectory_data = run_stable_simulation(env, policy, initial_state, config)
        
        print(f"âœ… ç¨³å®šæ¨¡æ‹Ÿå®Œæˆ: {len(trajectory_data['positions'])} æ­¥")
        
        # ç”Ÿæˆæœ‰ç•Œå¯è§†åŒ–
        output_file = create_bounded_animation(trajectory_data, config)
        
        print(f"ğŸ‰ æœ‰ç•Œå¯è§†åŒ–å®Œæˆ: {output_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_stable_scenario(device, num_agents):
    """åˆ›å»ºç¨³å®šçš„åˆå§‹åœºæ™¯"""
    from gcbfplus.env.multi_agent_env import MultiAgentState
    
    # æ›´ç´§å‡‘çš„åˆå§‹ä½ç½®
    positions = torch.zeros(1, num_agents, 2, device=device)
    velocities = torch.zeros(1, num_agents, 2, device=device)
    goals = torch.zeros(1, num_agents, 2, device=device)
    
    # å·¦ä¾§ç´§å‡‘ç¼–é˜Ÿ
    formation_x = -1.5
    target_x = 1.5
    
    for i in range(num_agents):
        # ç®€å•çš„2x3ç½‘æ ¼æ’åˆ—
        row = i // 3
        col = i % 3
        
        positions[0, i] = torch.tensor([
            formation_x + col * 0.2,
            (row - 0.5) * 0.4
        ], device=device)
        
        # å¯¹åº”çš„ç›®æ ‡ä½ç½®
        goals[0, i] = torch.tensor([
            target_x + col * 0.2,
            (row - 0.5) * 0.4
        ], device=device)
    
    print(f"   ğŸ“ åˆå§‹ä½ç½®èŒƒå›´: x=[{positions[0, :, 0].min():.2f}, {positions[0, :, 0].max():.2f}]")
    print(f"   ğŸ¯ ç›®æ ‡ä½ç½®èŒƒå›´: x=[{goals[0, :, 0].min():.2f}, {goals[0, :, 0].max():.2f}]")
    
    return MultiAgentState(
        positions=positions,
        velocities=velocities,
        goals=goals,
        batch_size=1
    )

def run_stable_simulation(env, policy, initial_state, config):
    """è¿è¡Œç¨³å®šæ¨¡æ‹Ÿ"""
    num_steps = 150
    max_position = 3.0  # ä½ç½®è¾¹ç•Œ
    max_velocity = 2.0  # é€Ÿåº¦è¾¹ç•Œ
    max_action = 0.3    # åŠ¨ä½œè¾¹ç•Œ
    
    trajectory_data = {
        'positions': [],
        'velocities': [],
        'actions': [],
        'goal_distances': [],
        'bounded_info': [],
        'config': config
    }
    
    current_state = initial_state
    
    with torch.no_grad():
        for step in range(num_steps):
            positions = current_state.positions[0].cpu().numpy()
            velocities = current_state.velocities[0].cpu().numpy()
            goal_positions = current_state.goals[0].cpu().numpy()
            
            trajectory_data['positions'].append(positions.copy())
            trajectory_data['velocities'].append(velocities.copy())
            
            # è·å–ç­–ç•¥åŠ¨ä½œ
            try:
                observations = env.get_observation(current_state)
                actions, alphas = policy(observations)
                
                # ä¸¥æ ¼é™åˆ¶åŠ¨ä½œ
                actions = torch.clamp(actions, -max_action, max_action)
                
            except Exception as e:
                print(f"   âš ï¸ ç­–ç•¥å¤±è´¥ï¼Œä½¿ç”¨ç›®æ ‡å¯¼å‘åŠ¨ä½œ")
                # å®‰å…¨çš„ç›®æ ‡å¯¼å‘åŠ¨ä½œ
                actions = torch.zeros(1, len(positions), 2, device=current_state.positions.device)
                for i in range(len(positions)):
                    direction = goal_positions[i] - positions[i]
                    distance = np.linalg.norm(direction)
                    if distance > 0.1:
                        # é™åˆ¶åŠ¨ä½œå¤§å°
                        normalized_direction = direction / distance
                        action_magnitude = min(distance * 0.5, max_action)
                        actions[0, i] = torch.tensor(normalized_direction * action_magnitude)
            
            # è®°å½•åŠ¨ä½œ
            trajectory_data['actions'].append(actions[0].cpu().numpy())
            
            # è®¡ç®—ç›®æ ‡è·ç¦»
            goal_distances = [np.linalg.norm(positions[i] - goal_positions[i]) 
                            for i in range(len(positions))]
            trajectory_data['goal_distances'].append(goal_distances)
            
            # è¾¹ç•Œæ£€æŸ¥ä¿¡æ¯
            pos_bounds = {
                'min_x': positions[:, 0].min(),
                'max_x': positions[:, 0].max(),
                'min_y': positions[:, 1].min(),
                'max_y': positions[:, 1].max()
            }
            
            vel_magnitudes = [np.linalg.norm(vel) for vel in velocities]
            action_magnitudes = [np.linalg.norm(action) for action in actions[0].cpu().numpy()]
            
            bounded_info = {
                'step': step,
                'position_bounds': pos_bounds,
                'max_velocity': max(vel_magnitudes),
                'max_action': max(action_magnitudes),
                'avg_goal_distance': np.mean(goal_distances),
                'out_of_bounds': any(abs(p) > max_position for pos in positions for p in pos)
            }
            trajectory_data['bounded_info'].append(bounded_info)
            
            # ç¯å¢ƒæ­¥è¿›
            try:
                step_result = env.step(current_state, actions, alphas)
                next_state = step_result.next_state
                
                # å¼ºåˆ¶è¾¹ç•Œé™åˆ¶
                next_positions = next_state.positions.clone()
                next_velocities = next_state.velocities.clone()
                
                # ä½ç½®è¾¹ç•Œ
                next_positions = torch.clamp(next_positions, -max_position, max_position)
                
                # é€Ÿåº¦è¾¹ç•Œ
                for i in range(next_velocities.shape[1]):
                    vel_magnitude = torch.norm(next_velocities[0, i])
                    if vel_magnitude > max_velocity:
                        next_velocities[0, i] = next_velocities[0, i] / vel_magnitude * max_velocity
                
                # æ›´æ–°çŠ¶æ€
                next_state = MultiAgentState(
                    positions=next_positions,
                    velocities=next_velocities,
                    goals=next_state.goals,
                    batch_size=next_state.batch_size
                )
                
                current_state = next_state
                
                # æ˜¾ç¤ºè¿›åº¦
                if step % 30 == 0:
                    print(f"   æ­¥éª¤ {step:3d}: ä½ç½®èŒƒå›´=[{pos_bounds['min_x']:.2f}, {pos_bounds['max_x']:.2f}], "
                          f"ç›®æ ‡è·ç¦»={bounded_info['avg_goal_distance']:.3f}")
                
                # æ£€æŸ¥å®Œæˆ
                if bounded_info['avg_goal_distance'] < 0.4:
                    print(f"   ğŸ¯ åˆ°è¾¾ç›®æ ‡! (æ­¥æ•°: {step+1})")
                    break
                    
            except Exception as e:
                print(f"   âš ï¸ ç¯å¢ƒæ­¥è¿›å¤±è´¥: {e}")
                break
    
    return trajectory_data

def create_bounded_animation(trajectory_data, config):
    """åˆ›å»ºæœ‰ç•ŒåŠ¨ç”»"""
    positions_history = trajectory_data['positions']
    if not positions_history:
        return None
    
    num_agents = len(positions_history[0])
    num_steps = len(positions_history)
    obstacles = config['env']['obstacles']
    
    # è®¡ç®—å®é™…ä½ç½®èŒƒå›´
    all_positions = np.concatenate(positions_history, axis=0)
    min_x, max_x = all_positions[:, 0].min() - 0.5, all_positions[:, 0].max() + 0.5
    min_y, max_y = all_positions[:, 1].min() - 0.5, all_positions[:, 1].max() + 0.5
    
    print(f"   ğŸ“ å®é™…ä½ç½®èŒƒå›´: x=[{min_x:.2f}, {max_x:.2f}], y=[{min_y:.2f}, {max_y:.2f}]")
    
    # åˆ›å»ºå›¾å½¢
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('ğŸš ä¿®å¤ç‰ˆæ— äººæœºç¼–é˜Ÿåä½œ (ç¡®ä¿åœ¨ç”»é¢å†…)', fontsize=16, fontweight='bold')
    
    # ä¸»è½¨è¿¹å›¾ - ä½¿ç”¨è®¡ç®—å‡ºçš„èŒƒå›´
    ax1.set_xlim(min_x, max_x)
    ax1.set_ylim(min_y, max_y)
    ax1.set_aspect('equal')
    ax1.set_title('ğŸš æ— äººæœºç¼–é˜Ÿåä½œ (æœ‰ç•Œç‰ˆæœ¬)')
    ax1.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶éšœç¢ç‰©
    for i, (pos, radius) in enumerate(zip(obstacles['positions'], obstacles['radii'])):
        circle = plt.Circle(pos, radius, color='red', alpha=0.8, 
                          label='éšœç¢ç‰©' if i == 0 else "")
        ax1.add_patch(circle)
    
    # ç»˜åˆ¶è¾¹ç•Œ
    boundary = plt.Rectangle((min_x, min_y), max_x - min_x, max_y - min_y, 
                           fill=False, edgecolor='gray', linestyle='--', linewidth=2, 
                           label='å¯è§†è¾¹ç•Œ')
    ax1.add_patch(boundary)
    
    # æ— äººæœºé¢œè‰²
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
    
    # åˆå§‹åŒ–
    trail_lines = []
    drone_dots = []
    goal_markers = []
    
    for i in range(num_agents):
        # è½¨è¿¹
        line, = ax1.plot([], [], '-', color=colors[i % len(colors)], alpha=0.7, linewidth=2)
        trail_lines.append(line)
        
        # æ— äººæœº
        drone, = ax1.plot([], [], 'o', color=colors[i % len(colors)], markersize=12, 
                         markeredgecolor='black', markeredgewidth=2)
        drone_dots.append(drone)
        
        # ç›®æ ‡
        goal, = ax1.plot([], [], 's', color=colors[i % len(colors)], markersize=8, alpha=0.7)
        goal_markers.append(goal)
    
    ax1.legend()
    
    # ä½ç½®è¾¹ç•Œç›‘æ§
    ax2.set_title('ğŸ“ ä½ç½®è¾¹ç•Œç›‘æ§')
    ax2.set_xlabel('æ—¶é—´æ­¥')
    ax2.set_ylabel('ä½ç½®')
    ax2.grid(True, alpha=0.3)
    
    # é€Ÿåº¦å’ŒåŠ¨ä½œç›‘æ§
    ax3.set_title('âš¡ é€Ÿåº¦ä¸åŠ¨ä½œç›‘æ§')
    ax3.set_xlabel('æ—¶é—´æ­¥')
    ax3.set_ylabel('å¹…åº¦')
    ax3.grid(True, alpha=0.3)
    
    # ç›®æ ‡è·ç¦»
    ax4.set_title('ğŸ¯ ç›®æ ‡è·ç¦»å˜åŒ–')
    ax4.set_xlabel('æ—¶é—´æ­¥')
    ax4.set_ylabel('è·ç¦»')
    ax4.grid(True, alpha=0.3)
    
    def animate(frame):
        if frame >= num_steps:
            return trail_lines + drone_dots
        
        current_positions = positions_history[frame]
        
        # æ›´æ–°æ— äººæœºå’Œè½¨è¿¹
        for i, (line, drone, goal) in enumerate(zip(trail_lines, drone_dots, goal_markers)):
            if i < len(current_positions):
                # è½¨è¿¹
                trail_x = [pos[i, 0] for pos in positions_history[:frame+1]]
                trail_y = [pos[i, 1] for pos in positions_history[:frame+1]]
                line.set_data(trail_x, trail_y)
                
                # æ— äººæœº
                drone.set_data([current_positions[i, 0]], [current_positions[i, 1]])
                
                # ç›®æ ‡ (åŸºäºå®é™…ç›®æ ‡ä½ç½®)
                if frame < len(trajectory_data['goal_distances']):
                    # ä½¿ç”¨åˆå§‹ç›®æ ‡ä½ç½®çš„ä¼°ç®—
                    goal_x = 1.5 + (i % 3) * 0.2
                    goal_y = ((i // 3) - 0.5) * 0.4
                    goal.set_data([goal_x], [goal_y])
        
        # æ›´æ–°ç›‘æ§å›¾è¡¨
        if frame > 0 and trajectory_data['bounded_info']:
            steps = list(range(min(frame+1, len(trajectory_data['bounded_info']))))
            
            # ä½ç½®è¾¹ç•Œ
            bounds_info = trajectory_data['bounded_info'][:frame+1]
            min_xs = [info['position_bounds']['min_x'] for info in bounds_info]
            max_xs = [info['position_bounds']['max_x'] for info in bounds_info]
            min_ys = [info['position_bounds']['min_y'] for info in bounds_info]
            max_ys = [info['position_bounds']['max_y'] for info in bounds_info]
            
            ax2.clear()
            ax2.plot(steps, min_xs, 'b-', label='æœ€å°X', alpha=0.7)
            ax2.plot(steps, max_xs, 'b--', label='æœ€å¤§X', alpha=0.7)
            ax2.plot(steps, min_ys, 'r-', label='æœ€å°Y', alpha=0.7)
            ax2.plot(steps, max_ys, 'r--', label='æœ€å¤§Y', alpha=0.7)
            ax2.axhline(y=3.0, color='gray', linestyle=':', alpha=0.5, label='è¾¹ç•Œ')
            ax2.axhline(y=-3.0, color='gray', linestyle=':', alpha=0.5)
            ax2.set_title(f'ğŸ“ ä½ç½®è¾¹ç•Œç›‘æ§ (æ­¥æ•°: {frame})')
            ax2.set_xlabel('æ—¶é—´æ­¥')
            ax2.set_ylabel('ä½ç½®')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # é€Ÿåº¦å’ŒåŠ¨ä½œ
            max_vels = [info['max_velocity'] for info in bounds_info]
            max_actions = [info['max_action'] for info in bounds_info]
            
            ax3.clear()
            ax3.plot(steps, max_vels, 'g-', linewidth=2, label='æœ€å¤§é€Ÿåº¦')
            ax3.plot(steps, max_actions, 'orange', linewidth=2, label='æœ€å¤§åŠ¨ä½œ')
            ax3.set_title(f'âš¡ é€Ÿåº¦ä¸åŠ¨ä½œç›‘æ§ (æ­¥æ•°: {frame})')
            ax3.set_xlabel('æ—¶é—´æ­¥')
            ax3.set_ylabel('å¹…åº¦')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            
            # ç›®æ ‡è·ç¦»
            if len(trajectory_data['goal_distances']) > frame:
                avg_goal_dists = [np.mean(dists) for dists in trajectory_data['goal_distances'][:frame+1]]
                ax4.clear()
                ax4.plot(steps, avg_goal_dists, 'purple', linewidth=2, label='å¹³å‡ç›®æ ‡è·ç¦»')
                ax4.set_title(f'ğŸ¯ ç›®æ ‡è·ç¦»å˜åŒ– (æ­¥æ•°: {frame})')
                ax4.set_xlabel('æ—¶é—´æ­¥')
                ax4.set_ylabel('è·ç¦»')
                ax4.legend()
                ax4.grid(True, alpha=0.3)
        
        return trail_lines + drone_dots
    
    # åˆ›å»ºåŠ¨ç”»
    anim = FuncAnimation(fig, animate, frames=num_steps, 
                        interval=150, blit=False, repeat=True)
    
    # ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"BOUNDED_DRONE_FORMATION_{timestamp}.gif"
    
    try:
        print(f"ğŸ’¾ ä¿å­˜æœ‰ç•Œå¯è§†åŒ–...")
        anim.save(output_path, writer='pillow', fps=6, dpi=120)
        print(f"âœ… ä¿å­˜æˆåŠŸ: {output_path}")
        
        # æœ€ç»ˆåˆ†æ
        final_info = trajectory_data['bounded_info'][-1] if trajectory_data['bounded_info'] else {}
        print(f"ğŸ“Š æœ€ç»ˆçŠ¶æ€:")
        print(f"   ä½ç½®èŒƒå›´: x=[{final_info.get('position_bounds', {}).get('min_x', 0):.2f}, "
              f"{final_info.get('position_bounds', {}).get('max_x', 0):.2f}]")
        print(f"   æœ€å¤§é€Ÿåº¦: {final_info.get('max_velocity', 0):.3f}")
        print(f"   æœ€å¤§åŠ¨ä½œ: {final_info.get('max_action', 0):.3f}")
        print(f"   ç›®æ ‡è·ç¦»: {final_info.get('avg_goal_distance', 0):.3f}")
        print(f"   è¶…å‡ºè¾¹ç•Œ: {'âŒ å¦' if not final_info.get('out_of_bounds', True) else 'âš ï¸ æ˜¯'}")
        
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å¤±è´¥: {e}")
        # ä¿å­˜é™æ€å›¾
        static_path = f"BOUNDED_STATIC_{timestamp}.png"
        plt.tight_layout()
        plt.savefig(static_path, dpi=150, bbox_inches='tight')
        print(f"âœ… é™æ€å›¾ä¿å­˜: {static_path}")
        output_path = static_path
    
    plt.close()
    return output_path

if __name__ == "__main__":
    print("ğŸ”§ ä¿®å¤è¾¹ç•Œå¯è§†åŒ–ç³»ç»Ÿ")
    print("è§£å†³æ— äººæœºè·‘å‡ºç”»é¢çš„é—®é¢˜")
    print("ç¡®ä¿æ™ºèƒ½ä½“åœ¨å¯è§†èŒƒå›´å†…åä½œ")
    print("=" * 60)
    
    success = create_bounded_visualization()
    
    if success:
        print(f"\nğŸ‰ è¾¹ç•Œé—®é¢˜ä¿®å¤æˆåŠŸ!")
        print(f"ğŸš ç”Ÿæˆäº†ç¡®ä¿åœ¨ç”»é¢å†…çš„æ— äººæœºç¼–é˜Ÿåä½œå¯è§†åŒ–")
        print(f"ğŸ“ æ™ºèƒ½ä½“ç°åœ¨ä¼šä¿æŒåœ¨å¯è§†èŒƒå›´å†…")
        print(f"ğŸ“ æ£€æŸ¥æ–°çš„GIFæ–‡ä»¶")
    else:
        print(f"\nâŒ ä¿®å¤å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
 
 
 
 