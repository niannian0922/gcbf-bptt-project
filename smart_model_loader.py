#!/usr/bin/env python3
"""
æ™ºèƒ½æ¨¡å‹åŠ è¼‰å™¨ - æ ¹æ“šå¯¦éš›æ¬Šé‡å®Œå…¨é‡å»ºç¶²çµ¡æ¶æ§‹
"""

import torch
import torch.nn as nn
import yaml
import os
from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.policy import BPTTPolicy, create_policy_from_config
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import numpy as np


def analyze_model_architecture(policy_path):
    """
    åˆ†ææ¨¡å‹æ¬Šé‡ä¾†æ¨æ–·å®Œæ•´çš„ç¶²çµ¡æ¶æ§‹
    """
    print(f"ğŸ” åˆ†ææ¨¡å‹æ¶æ§‹: {policy_path}")
    
    state_dict = torch.load(policy_path, map_location='cpu', weights_only=True)
    
    print(f"ğŸ“‹ æ¨¡å‹æ¬Šé‡éµ:")
    for key, tensor in state_dict.items():
        print(f"  {key}: {tensor.shape}")
    
    # åˆ†æperceptionå±¤
    perception_input_dim = None
    perception_output_dim = None
    perception_layers = []
    
    if 'perception.mlp.0.weight' in state_dict:
        perception_input_dim = state_dict['perception.mlp.0.weight'].shape[1]
        perception_output_dim = state_dict['perception.mlp.0.weight'].shape[0]
        
    # æ‰¾åˆ°æ‰€æœ‰perceptionå±¤
    for key in state_dict.keys():
        if key.startswith('perception.mlp.') and key.endswith('.weight'):
            layer_num = int(key.split('.')[2])
            out_features = state_dict[key].shape[0]
            perception_layers.append((layer_num, out_features))
    
    perception_layers.sort()
    perception_hidden_dims = [dim for _, dim in perception_layers[:-1]]  # é™¤äº†æœ€å¾Œä¸€å±¤
    
    # åˆ†æmemoryå±¤
    memory_hidden_dim = None
    if 'memory.gru.weight_hh_l0' in state_dict:
        memory_hidden_dim = state_dict['memory.gru.weight_hh_l0'].shape[1]
    
    # åˆ†æpolicy_headå±¤
    policy_head_layers = []
    action_layers = []
    alpha_layers = []
    
    # æ‰¾action_layers
    for key in state_dict.keys():
        if key.startswith('policy_head.action_layers.') and key.endswith('.weight'):
            # å¾ 'policy_head.action_layers.N.weight' ä¸­æå– N
            parts = key.split('.')
            if len(parts) >= 4 and parts[3].isdigit():
                layer_num = int(parts[3])
                out_features = state_dict[key].shape[0]
                action_layers.append((layer_num, out_features))
    
    action_layers.sort()
    
    # æ‰¾alpha_network
    for key in state_dict.keys():
        if key.startswith('policy_head.alpha_network.') and key.endswith('.weight'):
            # å¾ 'policy_head.alpha_network.N.weight' ä¸­æå– N
            parts = key.split('.')
            if len(parts) >= 4 and parts[3].isdigit():
                layer_num = int(parts[3])
                out_features = state_dict[key].shape[0]
                alpha_layers.append((layer_num, out_features))
    
    alpha_layers.sort()
    
    # æ§‹å»ºå®Œæ•´é…ç½®
    config = {
        'perception': {
            'use_vision': False,
            'input_dim': perception_input_dim,
            'output_dim': perception_output_dim,
            'hidden_dims': perception_hidden_dims,
            'activation': 'relu'
        },
        'memory': {
            'hidden_dim': memory_hidden_dim,
            'num_layers': 1
        },
        'policy_head': {
            'output_dim': action_layers[-1][1] if action_layers else 2,  # æœ€å¾Œä¸€å±¤æ˜¯è¼¸å‡ºç¶­åº¦
            'hidden_dims': [dim for _, dim in action_layers[:-1]] if len(action_layers) > 1 else [perception_output_dim],
            'activation': 'relu',
            'predict_alpha': True,
            'alpha_hidden_dims': [dim for _, dim in alpha_layers[:-1]] if len(alpha_layers) > 1 else [perception_output_dim]
        }
    }
    
    print(f"âœ… æ¨æ–·çš„ç¶²çµ¡æ¶æ§‹:")
    print(f"  Perception: è¼¸å…¥={perception_input_dim}, è¼¸å‡º={perception_output_dim}, éš±è—å±¤={perception_hidden_dims}")
    print(f"  Memory: éš±è—ç¶­åº¦={memory_hidden_dim}")
    print(f"  Actionå±¤: {[dim for _, dim in action_layers]}")
    print(f"  Alphaå±¤: {[dim for _, dim in alpha_layers]}")
    
    return config


def create_exact_model(model_path, device='cpu'):
    """
    æ ¹æ“šå¯¦éš›æ¨¡å‹æ¬Šé‡å‰µå»ºå®Œå…¨åŒ¹é…çš„ç¶²çµ¡
    """
    print(f"ğŸ¯ å‰µå»ºç²¾ç¢ºåŒ¹é…çš„æ¨¡å‹")
    
    # åˆ†ææ¶æ§‹
    policy_config = analyze_model_architecture(model_path)
    
    # å‰µå»ºç¶²çµ¡
    policy_network = create_policy_from_config(policy_config)
    policy_network = policy_network.to(device)
    
    # åŠ è¼‰æ¬Šé‡
    state_dict = torch.load(model_path, map_location=device, weights_only=True)
    
    try:
        policy_network.load_state_dict(state_dict, strict=False)
        print(f"âœ… æ¨¡å‹åŠ è¼‰æˆåŠŸ (strict=False)")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è¼‰å¤±è´¥: {e}")
        return None, None
    
    return policy_network, policy_config


def create_simple_visualization(env, policy_network, device, num_steps=100):
    """
    å‰µå»ºç°¡å–®çš„å¯è¦–åŒ–
    """
    print(f"ğŸ¬ é–‹å§‹ç°¡å–®å¯è¦–åŒ– ({num_steps} æ­¥)")
    
    policy_network.eval()
    
    # åˆå§‹åŒ–ç’°å¢ƒ
    state = env.reset()
    
    trajectory_positions = []
    
    with torch.no_grad():
        for step in range(num_steps):
            # è¨˜éŒ„ä½ç½®
            current_positions = state.positions[0].cpu().numpy()
            trajectory_positions.append(current_positions.copy())
            
            # ç²å–è§€æ¸¬
            observations = env.get_observations(state).to(device)
            
            # ç­–ç•¥æ¨ç†
            try:
                policy_output = policy_network(observations, state)
                actions = policy_output.actions
                alphas = getattr(policy_output, 'alphas', torch.ones_like(actions[:, :, :1]) * 0.5)
                
                # æª¢æŸ¥å‹•ä½œ
                action_magnitude = torch.norm(actions, dim=-1).mean().item()
                if step % 20 == 0:
                    print(f"æ­¥é©Ÿ {step}: å‹•ä½œå¼·åº¦={action_magnitude:.6f}")
                
                # ç’°å¢ƒæ­¥é€²
                step_result = env.step(state, actions, alphas)
                state = step_result.next_state
                
            except Exception as e:
                print(f"âŒ æ­¥é©Ÿ {step} å¤±æ•—: {e}")
                break
    
    # å‰µå»ºå‹•ç•«
    print(f"ğŸ¨ å‰µå»ºå‹•ç•«...")
    
    fig, ax = plt.subplots(figsize=(12, 8))
    ax.set_xlim(-3, 3)
    ax.set_ylim(-2, 2)
    ax.set_aspect('equal')
    ax.set_title('ğŸ¯ çµ±ä¸€ä»£ç¢¼è·¯å¾‘ - çœŸå¯¦æ¨¡å‹å¯è¦–åŒ–', fontsize=16, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # æ·»åŠ éšœç¢ç‰©
    obstacles = [
        {'pos': [0.0, -0.8], 'radius': 0.4},
        {'pos': [0.0, 0.8], 'radius': 0.4}
    ]
    
    for obs in obstacles:
        circle = plt.Circle(obs['pos'], obs['radius'], color='red', alpha=0.8)
        ax.add_patch(circle)
    
    # èµ·å§‹å’Œç›®æ¨™å€åŸŸ
    start_zone = plt.Rectangle((-2.5, -1.5), 1.0, 3.0, fill=False, 
                              edgecolor='green', linestyle='--', linewidth=3, 
                              alpha=0.9, label='èµ·å§‹å€åŸŸ')
    ax.add_patch(start_zone)
    
    target_zone = plt.Rectangle((1.5, -1.5), 1.0, 3.0, fill=False, 
                               edgecolor='blue', linestyle='--', linewidth=3, 
                               alpha=0.9, label='ç›®æ¨™å€åŸŸ')
    ax.add_patch(target_zone)
    
    # æ™ºèƒ½é«”é¡è‰²
    num_agents = len(trajectory_positions[0])
    colors = ['#FF4444', '#44FF44', '#4444FF', '#FFAA44', '#FF44AA', '#44AAFF'][:num_agents]
    
    # è»Œè·¡ç·šå’Œé»
    trail_lines = []
    agent_dots = []
    
    for i in range(num_agents):
        line, = ax.plot([], [], '-', color=colors[i], alpha=0.8, linewidth=2,
                       label=f'æ™ºèƒ½é«”{i+1}' if i < 3 else "")
        trail_lines.append(line)
        
        dot, = ax.plot([], [], 'o', color=colors[i], markersize=12, 
                      markeredgecolor='black', markeredgewidth=2, zorder=5)
        agent_dots.append(dot)
    
    ax.legend()
    
    # æ·»åŠ ä¿¡æ¯æ–‡æœ¬
    info_text = ax.text(0.02, 0.98, '', transform=ax.transAxes, 
                       verticalalignment='top', fontsize=12,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    def animate(frame):
        if frame >= len(trajectory_positions):
            return trail_lines + agent_dots + [info_text]
        
        current_pos = trajectory_positions[frame]
        
        # æ›´æ–°è»Œè·¡å’Œæ™ºèƒ½é«”ä½ç½®
        for i in range(num_agents):
            trail_x = [pos[i, 0] for pos in trajectory_positions[:frame+1]]
            trail_y = [pos[i, 1] for pos in trajectory_positions[:frame+1]]
            trail_lines[i].set_data(trail_x, trail_y)
            
            agent_dots[i].set_data([current_pos[i, 0]], [current_pos[i, 1]])
        
        # è¨ˆç®—ç¸½ä½ç§»
        if frame > 0:
            initial_pos = trajectory_positions[0]
            total_displacement = np.mean([
                np.linalg.norm(current_pos[i] - initial_pos[i]) 
                for i in range(num_agents)
            ])
        else:
            total_displacement = 0
        
        info_text.set_text(f'æ­¥é©Ÿ: {frame}\næ™ºèƒ½é«”æ•¸: {num_agents}\nå¹³å‡ä½ç§»: {total_displacement:.3f}')
        
        return trail_lines + agent_dots + [info_text]
    
    anim = FuncAnimation(fig, animate, frames=len(trajectory_positions),
                        interval=100, blit=False, repeat=True)
    
    return anim, trajectory_positions


def main():
    """
    ä¸»å‡½æ•¸
    """
    print(f"ğŸ¯ æ™ºèƒ½æ¨¡å‹åŠ è¼‰å™¨")
    print(f"=" * 60)
    
    # è¨­ç½®
    model_dir = 'logs/full_collaboration_training'
    device = torch.device('cpu')
    
    # æ‰¾åˆ°æœ€æ–°æ¨¡å‹
    models_dir = os.path.join(model_dir, 'models')
    steps = [int(d) for d in os.listdir(models_dir) if d.isdigit()]
    latest_step = max(steps)
    
    policy_path = os.path.join(model_dir, 'models', str(latest_step), 'policy.pt')
    print(f"ğŸ“ ä½¿ç”¨æ¨¡å‹: {policy_path}")
    
    # å‰µå»ºç²¾ç¢ºæ¨¡å‹
    policy_network, policy_config = create_exact_model(policy_path, device)
    
    if policy_network is None:
        print(f"âŒ æ¨¡å‹å‰µå»ºå¤±æ•—")
        return
    
    # å‰µå»ºç’°å¢ƒ
    env_config = {
        'area_size': 3.0,
        'car_radius': 0.15,
        'comm_radius': 1.0,
        'dt': 0.05,
        'mass': 0.1,
        'max_force': 1.0,
        'max_steps': 80,
        'name': 'DoubleIntegrator',
        'num_agents': 6,
        'obstacles': {
            'enabled': True,
            'bottleneck': True,
            'positions': [[0.0, -0.8], [0.0, 0.8]],
            'radii': [0.4, 0.4]
        }
    }
    
    env = DoubleIntegratorEnv(env_config)
    env = env.to(device)
    
    print(f"ğŸŒ ç’°å¢ƒå‰µå»ºæˆåŠŸ")
    print(f"ğŸ“ è§€æ¸¬å½¢ç‹€: {env.observation_shape}")
    print(f"ğŸ“ å‹•ä½œå½¢ç‹€: {env.action_shape}")
    
    # å‰µå»ºå¯è¦–åŒ–
    anim, trajectory = create_simple_visualization(env, policy_network, device, 120)
    
    # ä¿å­˜å‹•ç•«
    output_path = 'SMART_FINAL_COLLABORATION_RESULT.mp4'
    try:
        print(f"ğŸ’¾ ä¿å­˜å‹•ç•«: {output_path}")
        anim.save(output_path, writer='ffmpeg', fps=10, dpi=150)
        
        file_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"âœ… ä¿å­˜æˆåŠŸ!")
        print(f"ğŸ“ æ–‡ä»¶: {output_path}")
        print(f"ğŸ“Š å¤§å°: {file_size:.2f}MB")
        
        # çµ±è¨ˆåˆ†æ
        if trajectory:
            initial_pos = trajectory[0]
            final_pos = trajectory[-1]
            total_displacement = np.mean([
                np.linalg.norm(final_pos[i] - initial_pos[i]) 
                for i in range(len(initial_pos))
            ])
            print(f"ğŸ“ˆ å¹³å‡ç¸½ä½ç§»: {total_displacement:.4f}")
            
            if total_displacement < 0.01:
                print(f"âš ï¸ è­¦å‘Š: æ™ºèƒ½é«”å¹¾ä¹éœæ­¢ï¼Œå¯èƒ½å­˜åœ¨æ¨¡å‹å•é¡Œ")
            else:
                print(f"âœ… æ™ºèƒ½é«”æ­£å¸¸ç§»å‹•")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±æ•—: {e}")
        # å˜—è©¦ä¿å­˜ç‚ºGIF
        try:
            gif_path = 'SMART_FINAL_COLLABORATION_RESULT.gif'
            anim.save(gif_path, writer='pillow', fps=8)
            print(f"âœ… å·²ä¿å­˜ç‚ºGIF: {gif_path}")
        except Exception as e2:
            print(f"âŒ GIFä¿å­˜ä¹Ÿå¤±æ•—: {e2}")
    
    plt.close()


if __name__ == '__main__':
    main()
 
"""
æ™ºèƒ½æ¨¡å‹åŠ è¼‰å™¨ - æ ¹æ“šå¯¦éš›æ¬Šé‡å®Œå…¨é‡å»ºç¶²çµ¡æ¶æ§‹
"""

import torch
import torch.nn as nn
import yaml
import os
from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.policy import BPTTPolicy, create_policy_from_config
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import numpy as np


def analyze_model_architecture(policy_path):
    """
    åˆ†ææ¨¡å‹æ¬Šé‡ä¾†æ¨æ–·å®Œæ•´çš„ç¶²çµ¡æ¶æ§‹
    """
    print(f"ğŸ” åˆ†ææ¨¡å‹æ¶æ§‹: {policy_path}")
    
    state_dict = torch.load(policy_path, map_location='cpu', weights_only=True)
    
    print(f"ğŸ“‹ æ¨¡å‹æ¬Šé‡éµ:")
    for key, tensor in state_dict.items():
        print(f"  {key}: {tensor.shape}")
    
    # åˆ†æperceptionå±¤
    perception_input_dim = None
    perception_output_dim = None
    perception_layers = []
    
    if 'perception.mlp.0.weight' in state_dict:
        perception_input_dim = state_dict['perception.mlp.0.weight'].shape[1]
        perception_output_dim = state_dict['perception.mlp.0.weight'].shape[0]
        
    # æ‰¾åˆ°æ‰€æœ‰perceptionå±¤
    for key in state_dict.keys():
        if key.startswith('perception.mlp.') and key.endswith('.weight'):
            layer_num = int(key.split('.')[2])
            out_features = state_dict[key].shape[0]
            perception_layers.append((layer_num, out_features))
    
    perception_layers.sort()
    perception_hidden_dims = [dim for _, dim in perception_layers[:-1]]  # é™¤äº†æœ€å¾Œä¸€å±¤
    
    # åˆ†æmemoryå±¤
    memory_hidden_dim = None
    if 'memory.gru.weight_hh_l0' in state_dict:
        memory_hidden_dim = state_dict['memory.gru.weight_hh_l0'].shape[1]
    
    # åˆ†æpolicy_headå±¤
    policy_head_layers = []
    action_layers = []
    alpha_layers = []
    
    # æ‰¾action_layers
    for key in state_dict.keys():
        if key.startswith('policy_head.action_layers.') and key.endswith('.weight'):
            # å¾ 'policy_head.action_layers.N.weight' ä¸­æå– N
            parts = key.split('.')
            if len(parts) >= 4 and parts[3].isdigit():
                layer_num = int(parts[3])
                out_features = state_dict[key].shape[0]
                action_layers.append((layer_num, out_features))
    
    action_layers.sort()
    
    # æ‰¾alpha_network
    for key in state_dict.keys():
        if key.startswith('policy_head.alpha_network.') and key.endswith('.weight'):
            # å¾ 'policy_head.alpha_network.N.weight' ä¸­æå– N
            parts = key.split('.')
            if len(parts) >= 4 and parts[3].isdigit():
                layer_num = int(parts[3])
                out_features = state_dict[key].shape[0]
                alpha_layers.append((layer_num, out_features))
    
    alpha_layers.sort()
    
    # æ§‹å»ºå®Œæ•´é…ç½®
    config = {
        'perception': {
            'use_vision': False,
            'input_dim': perception_input_dim,
            'output_dim': perception_output_dim,
            'hidden_dims': perception_hidden_dims,
            'activation': 'relu'
        },
        'memory': {
            'hidden_dim': memory_hidden_dim,
            'num_layers': 1
        },
        'policy_head': {
            'output_dim': action_layers[-1][1] if action_layers else 2,  # æœ€å¾Œä¸€å±¤æ˜¯è¼¸å‡ºç¶­åº¦
            'hidden_dims': [dim for _, dim in action_layers[:-1]] if len(action_layers) > 1 else [perception_output_dim],
            'activation': 'relu',
            'predict_alpha': True,
            'alpha_hidden_dims': [dim for _, dim in alpha_layers[:-1]] if len(alpha_layers) > 1 else [perception_output_dim]
        }
    }
    
    print(f"âœ… æ¨æ–·çš„ç¶²çµ¡æ¶æ§‹:")
    print(f"  Perception: è¼¸å…¥={perception_input_dim}, è¼¸å‡º={perception_output_dim}, éš±è—å±¤={perception_hidden_dims}")
    print(f"  Memory: éš±è—ç¶­åº¦={memory_hidden_dim}")
    print(f"  Actionå±¤: {[dim for _, dim in action_layers]}")
    print(f"  Alphaå±¤: {[dim for _, dim in alpha_layers]}")
    
    return config


def create_exact_model(model_path, device='cpu'):
    """
    æ ¹æ“šå¯¦éš›æ¨¡å‹æ¬Šé‡å‰µå»ºå®Œå…¨åŒ¹é…çš„ç¶²çµ¡
    """
    print(f"ğŸ¯ å‰µå»ºç²¾ç¢ºåŒ¹é…çš„æ¨¡å‹")
    
    # åˆ†ææ¶æ§‹
    policy_config = analyze_model_architecture(model_path)
    
    # å‰µå»ºç¶²çµ¡
    policy_network = create_policy_from_config(policy_config)
    policy_network = policy_network.to(device)
    
    # åŠ è¼‰æ¬Šé‡
    state_dict = torch.load(model_path, map_location=device, weights_only=True)
    
    try:
        policy_network.load_state_dict(state_dict, strict=False)
        print(f"âœ… æ¨¡å‹åŠ è¼‰æˆåŠŸ (strict=False)")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è¼‰å¤±è´¥: {e}")
        return None, None
    
    return policy_network, policy_config


def create_simple_visualization(env, policy_network, device, num_steps=100):
    """
    å‰µå»ºç°¡å–®çš„å¯è¦–åŒ–
    """
    print(f"ğŸ¬ é–‹å§‹ç°¡å–®å¯è¦–åŒ– ({num_steps} æ­¥)")
    
    policy_network.eval()
    
    # åˆå§‹åŒ–ç’°å¢ƒ
    state = env.reset()
    
    trajectory_positions = []
    
    with torch.no_grad():
        for step in range(num_steps):
            # è¨˜éŒ„ä½ç½®
            current_positions = state.positions[0].cpu().numpy()
            trajectory_positions.append(current_positions.copy())
            
            # ç²å–è§€æ¸¬
            observations = env.get_observations(state).to(device)
            
            # ç­–ç•¥æ¨ç†
            try:
                policy_output = policy_network(observations, state)
                actions = policy_output.actions
                alphas = getattr(policy_output, 'alphas', torch.ones_like(actions[:, :, :1]) * 0.5)
                
                # æª¢æŸ¥å‹•ä½œ
                action_magnitude = torch.norm(actions, dim=-1).mean().item()
                if step % 20 == 0:
                    print(f"æ­¥é©Ÿ {step}: å‹•ä½œå¼·åº¦={action_magnitude:.6f}")
                
                # ç’°å¢ƒæ­¥é€²
                step_result = env.step(state, actions, alphas)
                state = step_result.next_state
                
            except Exception as e:
                print(f"âŒ æ­¥é©Ÿ {step} å¤±æ•—: {e}")
                break
    
    # å‰µå»ºå‹•ç•«
    print(f"ğŸ¨ å‰µå»ºå‹•ç•«...")
    
    fig, ax = plt.subplots(figsize=(12, 8))
    ax.set_xlim(-3, 3)
    ax.set_ylim(-2, 2)
    ax.set_aspect('equal')
    ax.set_title('ğŸ¯ çµ±ä¸€ä»£ç¢¼è·¯å¾‘ - çœŸå¯¦æ¨¡å‹å¯è¦–åŒ–', fontsize=16, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # æ·»åŠ éšœç¢ç‰©
    obstacles = [
        {'pos': [0.0, -0.8], 'radius': 0.4},
        {'pos': [0.0, 0.8], 'radius': 0.4}
    ]
    
    for obs in obstacles:
        circle = plt.Circle(obs['pos'], obs['radius'], color='red', alpha=0.8)
        ax.add_patch(circle)
    
    # èµ·å§‹å’Œç›®æ¨™å€åŸŸ
    start_zone = plt.Rectangle((-2.5, -1.5), 1.0, 3.0, fill=False, 
                              edgecolor='green', linestyle='--', linewidth=3, 
                              alpha=0.9, label='èµ·å§‹å€åŸŸ')
    ax.add_patch(start_zone)
    
    target_zone = plt.Rectangle((1.5, -1.5), 1.0, 3.0, fill=False, 
                               edgecolor='blue', linestyle='--', linewidth=3, 
                               alpha=0.9, label='ç›®æ¨™å€åŸŸ')
    ax.add_patch(target_zone)
    
    # æ™ºèƒ½é«”é¡è‰²
    num_agents = len(trajectory_positions[0])
    colors = ['#FF4444', '#44FF44', '#4444FF', '#FFAA44', '#FF44AA', '#44AAFF'][:num_agents]
    
    # è»Œè·¡ç·šå’Œé»
    trail_lines = []
    agent_dots = []
    
    for i in range(num_agents):
        line, = ax.plot([], [], '-', color=colors[i], alpha=0.8, linewidth=2,
                       label=f'æ™ºèƒ½é«”{i+1}' if i < 3 else "")
        trail_lines.append(line)
        
        dot, = ax.plot([], [], 'o', color=colors[i], markersize=12, 
                      markeredgecolor='black', markeredgewidth=2, zorder=5)
        agent_dots.append(dot)
    
    ax.legend()
    
    # æ·»åŠ ä¿¡æ¯æ–‡æœ¬
    info_text = ax.text(0.02, 0.98, '', transform=ax.transAxes, 
                       verticalalignment='top', fontsize=12,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    def animate(frame):
        if frame >= len(trajectory_positions):
            return trail_lines + agent_dots + [info_text]
        
        current_pos = trajectory_positions[frame]
        
        # æ›´æ–°è»Œè·¡å’Œæ™ºèƒ½é«”ä½ç½®
        for i in range(num_agents):
            trail_x = [pos[i, 0] for pos in trajectory_positions[:frame+1]]
            trail_y = [pos[i, 1] for pos in trajectory_positions[:frame+1]]
            trail_lines[i].set_data(trail_x, trail_y)
            
            agent_dots[i].set_data([current_pos[i, 0]], [current_pos[i, 1]])
        
        # è¨ˆç®—ç¸½ä½ç§»
        if frame > 0:
            initial_pos = trajectory_positions[0]
            total_displacement = np.mean([
                np.linalg.norm(current_pos[i] - initial_pos[i]) 
                for i in range(num_agents)
            ])
        else:
            total_displacement = 0
        
        info_text.set_text(f'æ­¥é©Ÿ: {frame}\næ™ºèƒ½é«”æ•¸: {num_agents}\nå¹³å‡ä½ç§»: {total_displacement:.3f}')
        
        return trail_lines + agent_dots + [info_text]
    
    anim = FuncAnimation(fig, animate, frames=len(trajectory_positions),
                        interval=100, blit=False, repeat=True)
    
    return anim, trajectory_positions


def main():
    """
    ä¸»å‡½æ•¸
    """
    print(f"ğŸ¯ æ™ºèƒ½æ¨¡å‹åŠ è¼‰å™¨")
    print(f"=" * 60)
    
    # è¨­ç½®
    model_dir = 'logs/full_collaboration_training'
    device = torch.device('cpu')
    
    # æ‰¾åˆ°æœ€æ–°æ¨¡å‹
    models_dir = os.path.join(model_dir, 'models')
    steps = [int(d) for d in os.listdir(models_dir) if d.isdigit()]
    latest_step = max(steps)
    
    policy_path = os.path.join(model_dir, 'models', str(latest_step), 'policy.pt')
    print(f"ğŸ“ ä½¿ç”¨æ¨¡å‹: {policy_path}")
    
    # å‰µå»ºç²¾ç¢ºæ¨¡å‹
    policy_network, policy_config = create_exact_model(policy_path, device)
    
    if policy_network is None:
        print(f"âŒ æ¨¡å‹å‰µå»ºå¤±æ•—")
        return
    
    # å‰µå»ºç’°å¢ƒ
    env_config = {
        'area_size': 3.0,
        'car_radius': 0.15,
        'comm_radius': 1.0,
        'dt': 0.05,
        'mass': 0.1,
        'max_force': 1.0,
        'max_steps': 80,
        'name': 'DoubleIntegrator',
        'num_agents': 6,
        'obstacles': {
            'enabled': True,
            'bottleneck': True,
            'positions': [[0.0, -0.8], [0.0, 0.8]],
            'radii': [0.4, 0.4]
        }
    }
    
    env = DoubleIntegratorEnv(env_config)
    env = env.to(device)
    
    print(f"ğŸŒ ç’°å¢ƒå‰µå»ºæˆåŠŸ")
    print(f"ğŸ“ è§€æ¸¬å½¢ç‹€: {env.observation_shape}")
    print(f"ğŸ“ å‹•ä½œå½¢ç‹€: {env.action_shape}")
    
    # å‰µå»ºå¯è¦–åŒ–
    anim, trajectory = create_simple_visualization(env, policy_network, device, 120)
    
    # ä¿å­˜å‹•ç•«
    output_path = 'SMART_FINAL_COLLABORATION_RESULT.mp4'
    try:
        print(f"ğŸ’¾ ä¿å­˜å‹•ç•«: {output_path}")
        anim.save(output_path, writer='ffmpeg', fps=10, dpi=150)
        
        file_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"âœ… ä¿å­˜æˆåŠŸ!")
        print(f"ğŸ“ æ–‡ä»¶: {output_path}")
        print(f"ğŸ“Š å¤§å°: {file_size:.2f}MB")
        
        # çµ±è¨ˆåˆ†æ
        if trajectory:
            initial_pos = trajectory[0]
            final_pos = trajectory[-1]
            total_displacement = np.mean([
                np.linalg.norm(final_pos[i] - initial_pos[i]) 
                for i in range(len(initial_pos))
            ])
            print(f"ğŸ“ˆ å¹³å‡ç¸½ä½ç§»: {total_displacement:.4f}")
            
            if total_displacement < 0.01:
                print(f"âš ï¸ è­¦å‘Š: æ™ºèƒ½é«”å¹¾ä¹éœæ­¢ï¼Œå¯èƒ½å­˜åœ¨æ¨¡å‹å•é¡Œ")
            else:
                print(f"âœ… æ™ºèƒ½é«”æ­£å¸¸ç§»å‹•")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±æ•—: {e}")
        # å˜—è©¦ä¿å­˜ç‚ºGIF
        try:
            gif_path = 'SMART_FINAL_COLLABORATION_RESULT.gif'
            anim.save(gif_path, writer='pillow', fps=8)
            print(f"âœ… å·²ä¿å­˜ç‚ºGIF: {gif_path}")
        except Exception as e2:
            print(f"âŒ GIFä¿å­˜ä¹Ÿå¤±æ•—: {e2}")
    
    plt.close()


if __name__ == '__main__':
    main()
 
 
 
 