# Enhanced Diversity Training Configuration
# ğŸš€ ANTI-OVERFITTING: Maximum environment randomization to promote generalization
# Core Innovation: Drastically increased environmental diversity to combat overfitting

# å®éªŒåç§°
run_name: "enhanced_diversity_anti_overfitting"

# ğŸ¯ ENHANCED ENVIRONMENT: Maximum randomization and diversity
env:
  num_agents: 2                   # ä¿æŒæ™ºèƒ½ä½“æ•°é‡ä»¥åŒ¹é…æ¨¡å‹
  area_size: 2.0                  # ç¯å¢ƒå¤§å°
  agent_radius: 0.2               # æ™ºèƒ½ä½“åŠå¾„
  comm_radius: 1.0                # é€šä¿¡åŠå¾„
  mass: 0.1                       # æ™ºèƒ½ä½“è´¨é‡
  dt: 0.05                        # ä»¿çœŸæ—¶é—´æ­¥é•¿
  max_force: 1.0                  # æœ€å¤§åŠ›
  cbf_alpha: 1.0                  # CBFå®‰å…¨çº¦æŸçš„Alphaå‚æ•°
  max_steps: 300                  # æœ€å¤§æ­¥æ•°

  # CORE ENHANCEMENT: Dynamic obstacle configuration
  obstacles:
    enabled: true                 # å¯ç”¨éšœç¢ç‰©
    dynamic_count: true           # æ¯æ¬¡é‡ç½®æ—¶åŠ¨æ€æ”¹å˜éšœç¢ç‰©æ•°é‡
    count_range: [2, 8]           # éšœç¢ç‰©æ•°é‡èŒƒå›´: 2-8ä¸ª (é«˜å˜åŒ–æ€§)
    
    # Enhanced obstacle properties
    random: true                  # å¯ç”¨éšæœºéšœç¢ç‰©
    random_min_radius: 0.08       # å¢åŠ æœ€å°åŠå¾„èŒƒå›´ (æ›´å¤šæ ·æ€§)
    random_max_radius: 0.5        # å¢å¤§æœ€å¤§åŠå¾„ (æ›´é«˜æŒ‘æˆ˜)
    
    # ä¼ ç»Ÿé…ç½®ä¿æŒä¸ºç©º (å®Œå…¨ä¾èµ–åŠ¨æ€ç”Ÿæˆ)
    num_obstacles: 0
    positions: []
    radii: []

# ç½‘ç»œå‚æ•° (ç»§æ‰¿å† å†›æ¨¡å‹æ¶æ„)
networks:
  policy:
    perception:
      use_vision: false
      input_dim: 9                # ä¿®å¤ï¼šæ›´æ–°è‡³æœ‰éšœç¢ç‰©è§‚æµ‹ç»´åº¦ (6->9)
      hidden_dim: 64
      activation: relu
    memory:
      hidden_dim: 128
      num_layers: 1
    policy_head:
      input_dim: 128
      output_dim: 2
      hidden_dims: [256]
      activation: relu
      predict_alpha: true
      predict_margin: true        # ğŸš€ ä¿æŒè‡ªé€‚åº”å®‰å…¨è£•åº¦
      margin_hidden_dim: 64

  cbf:
    alpha: 1.0
    eps: 0.02
    safety_margin: 0.2            # åŸºç¡€å®‰å…¨è£•åº¦ï¼ˆå°†è¢«åŠ¨æ€è¦†ç›–ï¼‰
    use_qp: true

# è®­ç»ƒå‚æ•° - é’ˆå¯¹é«˜éš¾åº¦ç¯å¢ƒä¼˜åŒ–
training:
  training_steps: 10000           # ä¸­ç­‰è®­ç»ƒé•¿åº¦ - é¿å…è¿‡æ‹Ÿåˆ
  horizon_length: 50              # BPTT rollouté•¿åº¦
  eval_horizon: 300               # è¯„ä¼°rollouté•¿åº¦
  eval_interval: 200              # æ›´é¢‘ç¹è¯„ä¼°ä»¥ç›‘æ§æ³›åŒ–æ€§èƒ½
  save_interval: 1000             # æ¨¡å‹ä¿å­˜é—´éš”
  learning_rate: 0.0003           # æ›´ä½å­¦ä¹ ç‡é€‚åº”é«˜éš¾åº¦ç¯å¢ƒ
  max_grad_norm: 1.0              # æ¢¯åº¦è£å‰ª
  
  # å­¦ä¹ ç‡è°ƒåº¦å™¨ - æ›´ä¿å®ˆçš„è¡°å‡
  use_lr_scheduler: true
  lr_step_size: 2000              # æ›´é¢‘ç¹çš„å­¦ä¹ ç‡è°ƒæ•´
  lr_gamma: 0.9                   # æ›´æ¸©å’Œçš„è¡°å‡
  
  # æ—¶åºç¨³å®šæ€§çš„æ¢¯åº¦è¡°å‡
  gradient_decay_rate: 0.95

# æŸå¤±æƒé‡ - é’ˆå¯¹é«˜éš¾åº¦ç¯å¢ƒçš„å¹³è¡¡ç­–ç•¥
loss_weights:
  # åŸºç¡€æƒé‡ - æ›´æ³¨é‡å®‰å…¨æ€§
  goal_weight: 0.6                # é€‚åº¦é™ä½ç›®æ ‡æƒé‡ (åº”å¯¹é«˜éš¾åº¦)
  safety_weight: 15.0             # å¢åŠ å®‰å…¨æƒé‡ (åº”å¯¹æ›´å¤šéšœç¢ç‰©)
  
  # æ§åˆ¶æ­£åˆ™åŒ–æƒé‡ - æ›´å¼ºè°ƒå¹³æ»‘æ€§
  acceleration_loss_weight: 0.008 # å¢åŠ åŠ é€Ÿåº¦çº¦æŸ (åº”å¯¹å¤æ‚ç¯å¢ƒ)
  jerk_loss_weight: 0.015         # å¢åŠ æŠ–åŠ¨çº¦æŸ (ä¿ƒè¿›å¹³æ»‘é£è¡Œ)
  
  # å…¶ä»–æƒé‡
  control_weight: 0.1             # æ§åˆ¶åŠªåŠ›æŸå¤±
  alpha_reg_weight: 0.01          # alphaæ­£åˆ™åŒ–æƒé‡
  progress_weight: 0.03           # å‡å°‘è¿›åº¦æƒé‡ (é¿å…æ€¥èºè¡Œä¸º)
  
  # è‡ªé€‚åº”å®‰å…¨è£•åº¦æƒé‡
  margin_reg_weight: 0.08         # å¢åŠ è£•åº¦æ­£åˆ™åŒ– (ä¿ƒè¿›é€‚åº”æ€§)

# CORE INNOVATION 1: è‡ªé€‚åº”å®‰å…¨è£•åº¦å‚æ•°
use_adaptive_margin: true         # æ¿€æ´»è‡ªé€‚åº”è£•åº¦
min_safety_margin: 0.12           # æé«˜æœ€å°è£•åº¦ (åº”å¯¹é«˜å¯†åº¦éšœç¢ç‰©)
max_safety_margin: 0.6            # æ‰©å¤§æœ€å¤§è£•åº¦èŒƒå›´ (æ›´å¤§é€‚åº”æ€§)

# CORE INNOVATION 2: å®‰å…¨é—¨æ§Alphaæ­£åˆ™åŒ–
use_safety_gated_alpha_reg: true  # ä¿æŒå®‰å…¨é—¨æ§åˆ›æ–°
safety_loss_threshold: 0.015      # æé«˜å®‰å…¨é˜ˆå€¼ (æ›´ä¿å®ˆçš„è§¦å‘)

# CBFå‚æ•°
cbf_alpha: 1.0

# æ—¥å¿—é…ç½®
enable_episode_logging: true

# Wandbé…ç½®
wandb_config:
  project: "gcbf-enhanced-diversity-training"
  offline: true                   # ç¦»çº¿æ¨¡å¼

# å®éªŒå…ƒæ•°æ®
experiment_type: "enhanced_diversity_anti_overfitting"
description: "ENHANCED DIVERSITY TRAINING: Maximum environmental randomization to combat overfitting. Features: dynamic obstacle count (2-8), enhanced obstacle diversity, multi-strategy initial states, and adaptive difficulty. Goal: Learn robust, generalizable policies that perform well across diverse scenarios."

# é¢„æœŸæˆæœ
expected_improvements:
  - "Better generalization across different environments"
  - "Reduced overfitting compared to long training"
  - "More robust obstacle avoidance strategies"
  - "Improved cooperation in diverse scenarios"
  - "Stable performance across varying difficulty levels"
