# Enhanced Diversity Training Configuration
# 🚀 ANTI-OVERFITTING: Maximum environment randomization to promote generalization
# Core Innovation: Drastically increased environmental diversity to combat overfitting

# 实验名称
run_name: "enhanced_diversity_anti_overfitting"

# 🎯 ENHANCED ENVIRONMENT: Maximum randomization and diversity
env:
  num_agents: 2                   # 保持智能体数量以匹配模型
  area_size: 2.0                  # 环境大小
  agent_radius: 0.2               # 智能体半径
  comm_radius: 1.0                # 通信半径
  mass: 0.1                       # 智能体质量
  dt: 0.05                        # 仿真时间步长
  max_force: 1.0                  # 最大力
  cbf_alpha: 1.0                  # CBF安全约束的Alpha参数
  max_steps: 300                  # 最大步数

  # CORE ENHANCEMENT: Dynamic obstacle configuration
  obstacles:
    enabled: true                 # 启用障碍物
    dynamic_count: true           # 每次重置时动态改变障碍物数量
    count_range: [2, 8]           # 障碍物数量范围: 2-8个 (高变化性)
    
    # Enhanced obstacle properties
    random: true                  # 启用随机障碍物
    random_min_radius: 0.08       # 增加最小半径范围 (更多样性)
    random_max_radius: 0.5        # 增大最大半径 (更高挑战)
    
    # 传统配置保持为空 (完全依赖动态生成)
    num_obstacles: 0
    positions: []
    radii: []

# 网络参数 (继承冠军模型架构)
networks:
  policy:
    perception:
      use_vision: false
      input_dim: 9                # 修复：更新至有障碍物观测维度 (6->9)
      hidden_dim: 64
      activation: relu
    memory:
      hidden_dim: 128
      num_layers: 1
    policy_head:
      input_dim: 128
      output_dim: 2
      hidden_dims: [256]
      activation: relu
      predict_alpha: true
      predict_margin: true        # 🚀 保持自适应安全裕度
      margin_hidden_dim: 64

  cbf:
    alpha: 1.0
    eps: 0.02
    safety_margin: 0.2            # 基础安全裕度（将被动态覆盖）
    use_qp: true

# 训练参数 - 针对高难度环境优化
training:
  training_steps: 10000           # 中等训练长度 - 避免过拟合
  horizon_length: 50              # BPTT rollout长度
  eval_horizon: 300               # 评估rollout长度
  eval_interval: 200              # 更频繁评估以监控泛化性能
  save_interval: 1000             # 模型保存间隔
  learning_rate: 0.0003           # 更低学习率适应高难度环境
  max_grad_norm: 1.0              # 梯度裁剪
  
  # 学习率调度器 - 更保守的衰减
  use_lr_scheduler: true
  lr_step_size: 2000              # 更频繁的学习率调整
  lr_gamma: 0.9                   # 更温和的衰减
  
  # 时序稳定性的梯度衰减
  gradient_decay_rate: 0.95

# 损失权重 - 针对高难度环境的平衡策略
loss_weights:
  # 基础权重 - 更注重安全性
  goal_weight: 0.6                # 适度降低目标权重 (应对高难度)
  safety_weight: 15.0             # 增加安全权重 (应对更多障碍物)
  
  # 控制正则化权重 - 更强调平滑性
  acceleration_loss_weight: 0.008 # 增加加速度约束 (应对复杂环境)
  jerk_loss_weight: 0.015         # 增加抖动约束 (促进平滑飞行)
  
  # 其他权重
  control_weight: 0.1             # 控制努力损失
  alpha_reg_weight: 0.01          # alpha正则化权重
  progress_weight: 0.03           # 减少进度权重 (避免急躁行为)
  
  # 自适应安全裕度权重
  margin_reg_weight: 0.08         # 增加裕度正则化 (促进适应性)

# CORE INNOVATION 1: 自适应安全裕度参数
use_adaptive_margin: true         # 激活自适应裕度
min_safety_margin: 0.12           # 提高最小裕度 (应对高密度障碍物)
max_safety_margin: 0.6            # 扩大最大裕度范围 (更大适应性)

# CORE INNOVATION 2: 安全门控Alpha正则化
use_safety_gated_alpha_reg: true  # 保持安全门控创新
safety_loss_threshold: 0.015      # 提高安全阈值 (更保守的触发)

# CBF参数
cbf_alpha: 1.0

# 日志配置
enable_episode_logging: true

# Wandb配置
wandb_config:
  project: "gcbf-enhanced-diversity-training"
  offline: true                   # 离线模式

# 实验元数据
experiment_type: "enhanced_diversity_anti_overfitting"
description: "ENHANCED DIVERSITY TRAINING: Maximum environmental randomization to combat overfitting. Features: dynamic obstacle count (2-8), enhanced obstacle diversity, multi-strategy initial states, and adaptive difficulty. Goal: Learn robust, generalizable policies that perform well across diverse scenarios."

# 预期成果
expected_improvements:
  - "Better generalization across different environments"
  - "Reduced overfitting compared to long training"
  - "More robust obstacle avoidance strategies"
  - "Improved cooperation in diverse scenarios"
  - "Stable performance across varying difficulty levels"
