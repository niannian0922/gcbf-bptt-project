# Configuration file for BPTT (Backpropagation Through Time) training
# This configuration integrates GCBF+ framework with the "Back to Newton's Laws" training philosophy

# Experiment name
run_name: "GCBF+_BTN_BPTT"

# Environment parameters
env:
  num_agents: 8                   # Number of agents in the environment
  area_size: 1.0                  # Size of the square environment
  car_radius: 0.05                # Radius of each agent (for collision detection)
  comm_radius: 0.5                # Communication radius for graph construction
  mass: 0.1                       # Mass of each agent
  dt: 0.03                        # Simulation timestep

# Networks parameters
networks:
  # Policy GNN parameters
  policy:
    node_dim: 7                   # Position (2) + Velocity (2) + Goal direction (2) + Agent indicator (1)
    edge_dim: 4                   # Relative position (2) + Relative velocity (2)
    hidden_dim: 64                # Dimension of hidden layers
    n_layers: 2                   # Number of GNN layers
    msg_hidden_sizes: [64, 64]    # Hidden sizes for message network
    aggr_hidden_sizes: [64]       # Hidden sizes for attention network
    update_hidden_sizes: [64, 64] # Hidden sizes for update network
  
  # CBF GNN parameters
  cbf:
    node_dim: 7                   # Position (2) + Velocity (2) + Goal direction (2) + Agent indicator (1)
    edge_dim: 4                   # Relative position (2) + Relative velocity (2)
    hidden_dim: 64                # Dimension of hidden layers
    n_layers: 2                   # Number of GNN layers
    msg_hidden_sizes: [64, 64]    # Hidden sizes for message network
    aggr_hidden_sizes: [64]       # Hidden sizes for attention network
    update_hidden_sizes: [64, 64] # Hidden sizes for update network

# Training parameters
training:
  training_steps: 10000           # Total number of training steps
  horizon_length: 50              # Horizon length for BPTT rollouts
  eval_horizon: 100               # Horizon length for evaluation rollouts
  eval_interval: 100              # Interval between evaluations
  save_interval: 1000             # Interval between model saves
  learning_rate: 0.001            # Learning rate for Adam optimizer
  max_grad_norm: 1.0              # Maximum gradient norm (for clipping)
  
  # Learning rate scheduler
  use_lr_scheduler: true          # Whether to use learning rate scheduler
  lr_step_size: 2000              # Step size for learning rate scheduler
  lr_gamma: 0.5                   # Multiplier for learning rate scheduler

  # Loss weights
  goal_weight: 1.0                # Weight for goal reaching loss
  safety_weight: 10.0             # Weight for safety loss (CBF violation)
  control_weight: 0.1             # Weight for control effort loss
  cbf_alpha: 1.0                  # Alpha parameter in CBF condition (h_dot + alpha*h >= 0) 