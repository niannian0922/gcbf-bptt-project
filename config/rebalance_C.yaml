# Rebalance C Configuration
# Strategy: Balanced enhancement with moderate goal-seeking and relaxed regularization
# Changes: goal_weight: 0.1 → 0.7, jerk_loss_weight: 0.05 → 0.01, acceleration_loss_weight: 0.01 → 0.005

# 实验名称
run_name: "rebalance_C_training"

# 环境参数
env:
  num_agents: 2                   # 减少智能体数量以简化学习
  area_size: 2.0                  # 增大环境以提供更多空间
  agent_radius: 0.2               # 智能体半径
  comm_radius: 1.0                # 通信半径
  mass: 0.1                       # 智能体质量
  dt: 0.05                        # 仿真时间步长
  max_force: 1.0                  # 最大力
  cbf_alpha: 1.0                  # CBF安全约束的Alpha参数
  max_steps: 300                  # 最大步数

  # 障礙物配置（禁用以匹配6D模型）
  obstacles:
    enabled: false
    num_obstacles: 0
    positions: []
    radii: []

# 网络参数（匹配已训练模型架构）
networks:
  policy:
    perception:
      use_vision: false
      input_dim: 6                # 无障碍物环境：state(4) + goal(2)
      hidden_dim: 64
      activation: relu
    memory:
      hidden_dim: 128
      num_layers: 1
    policy_head:
      input_dim: 128
      output_dim: 2
      hidden_dims: [256]
      activation: relu
      predict_alpha: true

  cbf:
    alpha: 1.0
    eps: 0.02
    safety_margin: 0.2
    use_qp: true

# 训练参数
training:
  training_steps: 1000            # 快速测试权重效果
  horizon_length: 50              # BPTT rollout长度
  eval_horizon: 300               # 评估rollout长度
  eval_interval: 100              # 评估间隔
  save_interval: 500              # 模型保存间隔
  learning_rate: 0.0005           # 降低学习率以获得更稳定的训练
  max_grad_norm: 1.0              # 梯度裁剪
  
  # 学习率调度器
  use_lr_scheduler: true
  lr_step_size: 1000
  lr_gamma: 0.8
  
  # 时序稳定性的梯度衰减
  gradient_decay_rate: 0.95

# 损失权重 - 平衡增强策略
loss_weights:
  # 调整权重 - 策略C
  goal_weight: 0.7                # 从0.1增加到0.7 - 平衡的目标导向
  safety_weight: 10.0             # 保持高安全权重
  
  # 控制正则化权重 - 双重放松
  acceleration_loss_weight: 0.005 # 从0.01降低到0.005 - 更少加速度约束
  jerk_loss_weight: 0.01          # 从0.05降低到0.01 - 显著减少抖动约束
  
  # 其他现有权重
  control_weight: 0.1             # 控制努力损失
  alpha_reg_weight: 0.01          # alpha正则化权重
  progress_weight: 0.05           # 进度奖励权重

# CBF参数
cbf_alpha: 1.0

# 日志配置
enable_episode_logging: true

# Wandb配置
wandb_config:
  project: "gcbf-rebalance-experiments"
  offline: true                   # 离线模式

# 实验元数据
experiment_type: "weight_rebalancing"
description: "Strategy C: Balanced enhancement (goal 0.7) with relaxed regularization (accel 0.005, jerk 0.01)"
