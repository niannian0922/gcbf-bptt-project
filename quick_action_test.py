#!/usr/bin/env python3
"""
ğŸ” å¿«é€ŸåŠ¨ä½œæµ‹è¯•
æ£€æŸ¥ç­–ç•¥ç½‘ç»œæ˜¯å¦è¾“å‡ºæœ‰æ•ˆåŠ¨ä½œ
"""

import torch
import numpy as np
import yaml

def quick_action_test():
    """å¿«é€Ÿæµ‹è¯•åŠ¨ä½œè¾“å‡º"""
    print("ğŸ” å¿«é€ŸåŠ¨ä½œæµ‹è¯•")
    print("=" * 40)
    
    try:
        # åŠ è½½é…ç½®
        with open('config/simple_collaboration.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
        # åˆ›å»ºç¯å¢ƒ
        from gcbfplus.env import DoubleIntegratorEnv
        device = torch.device('cpu')
        env = DoubleIntegratorEnv(config['env'])
        
        print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ: è§‚æµ‹{env.observation_shape}, åŠ¨ä½œ{env.action_shape}")
        
        # åˆ›å»ºç®€å•æµ‹è¯•çŠ¶æ€
        from gcbfplus.env.multi_agent_env import MultiAgentState
        
        num_agents = config['env']['num_agents']
        positions = torch.randn(1, num_agents, 2) * 0.5
        velocities = torch.zeros(1, num_agents, 2)
        goals = torch.randn(1, num_agents, 2) * 0.5
        
        state = MultiAgentState(
            positions=positions,
            velocities=velocities,
            goals=goals,
            batch_size=1
        )
        
        print(f"âœ… æµ‹è¯•çŠ¶æ€åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•è§‚æµ‹
        obs = env.get_observation(state)
        print(f"âœ… è§‚æµ‹ç”Ÿæˆ: {obs.shape}, èŒƒå›´[{obs.min():.3f}, {obs.max():.3f}]")
        
        # åˆ›å»ºç­–ç•¥ç½‘ç»œé…ç½®
        policy_config = {
            'type': 'bptt',
            'hidden_dim': 256,
            'input_dim': 6,
            'node_dim': 6,
            'edge_dim': 4,
            'n_layers': 2,
            'msg_hidden_sizes': [256, 256],
            'aggr_hidden_sizes': [256],
            'update_hidden_sizes': [256, 256],
            'predict_alpha': True,
            'perception': {
                'input_dim': 6,
                'hidden_dim': 256,
                'num_layers': 2,
                'activation': 'relu',
                'use_vision': False
            },
            'memory': {
                'hidden_dim': 256,
                'memory_size': 32,
                'num_heads': 4
            },
            'policy_head': {
                'output_dim': 2,
                'predict_alpha': True,
                'hidden_dims': [256, 256],
                'action_scale': 1.0
            }
        }
        
        # åˆ›å»ºç­–ç•¥ç½‘ç»œ
        from gcbfplus.policy.bptt_policy import create_policy_from_config
        policy = create_policy_from_config(policy_config)
        
        print(f"âœ… ç­–ç•¥ç½‘ç»œåˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•éšæœºæƒé‡åŠ¨ä½œ
        with torch.no_grad():
            actions_random, alphas_random = policy(obs)
        
        action_mag = torch.norm(actions_random, dim=-1)
        print(f"ğŸ² éšæœºæƒé‡åŠ¨ä½œ:")
        print(f"   å½¢çŠ¶: {actions_random.shape}")
        print(f"   å¹…åº¦: å¹³å‡={action_mag.mean():.4f}, æœ€å¤§={action_mag.max():.4f}")
        print(f"   ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“: {actions_random[0, 0]}")
        
        # å°è¯•åŠ è½½è®­ç»ƒæƒé‡
        model_path = "logs/full_collaboration_training/models/500/policy.pt"
        try:
            state_dict = torch.load(model_path, map_location='cpu', weights_only=True)
            policy.load_state_dict(state_dict)
            print(f"âœ… è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ")
            
            # æµ‹è¯•è®­ç»ƒæƒé‡åŠ¨ä½œ
            with torch.no_grad():
                actions_trained, alphas_trained = policy(obs)
            
            action_mag_trained = torch.norm(actions_trained, dim=-1)
            print(f"ğŸ¯ è®­ç»ƒæƒé‡åŠ¨ä½œ:")
            print(f"   å½¢çŠ¶: {actions_trained.shape}")
            print(f"   å¹…åº¦: å¹³å‡={action_mag_trained.mean():.4f}, æœ€å¤§={action_mag_trained.max():.4f}")
            print(f"   ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“: {actions_trained[0, 0]}")
            
            # æ¯”è¾ƒ
            if action_mag_trained.max() < 1e-6:
                print(f"âŒ é—®é¢˜å‘ç°: è®­ç»ƒæƒé‡è¾“å‡ºæå°åŠ¨ä½œ!")
                print(f"   è¿™å°±æ˜¯æ™ºèƒ½ä½“ä¸åŠ¨çš„åŸå› !")
            else:
                print(f"âœ… è®­ç»ƒæƒé‡è¾“å‡ºæ­£å¸¸åŠ¨ä½œ")
                
        except Exception as e:
            print(f"âŒ è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥: {e}")
        
        # æµ‹è¯•ç¯å¢ƒæ­¥è¿›
        test_action = torch.ones(1, num_agents, 2) * 0.1  # å°çš„æµ‹è¯•åŠ¨ä½œ
        try:
            result = env.step(state, test_action, None)
            pos_change = torch.norm(result.next_state.positions - state.positions, dim=-1)
            print(f"ğŸ”„ ç¯å¢ƒæ­¥è¿›æµ‹è¯•:")
            print(f"   è¾“å…¥åŠ¨ä½œ: {test_action[0, 0]}")
            print(f"   ä½ç½®å˜åŒ–: å¹³å‡={pos_change.mean():.6f}, æœ€å¤§={pos_change.max():.6f}")
        except Exception as e:
            print(f"âŒ ç¯å¢ƒæ­¥è¿›å¤±è´¥: {e}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    quick_action_test()