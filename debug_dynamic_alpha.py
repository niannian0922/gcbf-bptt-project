#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ¨æ€Alphaè°ƒè¯•è„šæœ¬ - è‡ªé€‚åº”å®‰å…¨è¾¹è·çš„ä¸“é—¨è°ƒè¯•å·¥å…·

æ­¤è„šæœ¬ä¸“é—¨ç”¨äºéš”ç¦»å’Œåˆ†æåŠ¨æ€alphaæœºåˆ¶çš„è¡Œä¸ºï¼Œé€šè¿‡ï¼š
1. åŠ è½½é¢„è®­ç»ƒçš„"é»„é‡‘åŸºå‡†"å›ºå®šalphaæ¨¡å‹
2. åˆ›å»ºç®€åŒ–çš„ä¸¤æ™ºèƒ½ä½“å¯¹æ’åœºæ™¯
3. ä»…è®­ç»ƒalphaé¢„æµ‹å¤´ï¼Œè§‚å¯Ÿå­¦ä¹ åŠ¨æ€
4. æä¾›è¯¦ç»†çš„è°ƒè¯•è¾“å‡ºå’Œåˆ†æ

ä½œè€…: GCBF-BPTTé¡¹ç›®ç»„
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import yaml
from pathlib import Path
import matplotlib.pyplot as plt

from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.env.gcbf_safety_layer import GCBFSafetyLayer
from gcbfplus.policy import BPTTPolicy


class AlphaDebugger:
    """åŠ¨æ€Alphaè°ƒè¯•å™¨ - ä¸“é—¨ç”¨äºåˆ†æè‡ªé€‚åº”å®‰å…¨è¾¹è·å­¦ä¹ """
    
    def __init__(self, model_path: str = "logs/bptt/models/1000"):
        """
        åˆå§‹åŒ–è°ƒè¯•å™¨
        
        å‚æ•°:
            model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆé»„é‡‘åŸºå‡†ï¼‰
        """
        self.model_path = Path(model_path)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸ”§ åŠ¨æ€Alphaè°ƒè¯•å™¨åˆå§‹åŒ–")
        print(f"   ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"   æ¨¡å‹è·¯å¾„: {self.model_path}")
        
        # åŠ è½½é…ç½®å’Œæ¨¡å‹
        self._load_golden_model()
        
        # åˆ›å»ºç®€åŒ–ç¯å¢ƒ
        self._create_debug_environment()
        
        # è®¾ç½®è°ƒè¯•è®­ç»ƒ
        self._setup_debug_training()
    
    def _load_golden_model(self):
        """åŠ è½½é¢„è®­ç»ƒçš„é»„é‡‘åŸºå‡†æ¨¡å‹"""
        print("\nğŸ“‚ åŠ è½½é»„é‡‘åŸºå‡†æ¨¡å‹...")
        
        # åŠ è½½é…ç½®
        config_path = self.model_path / "config.yaml"
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        print(f"   é…ç½®æ–‡ä»¶: {config_path}")
        print(f"   åŸå§‹è®­ç»ƒæ­¥æ•°: {self.config['training']['training_steps']}")
        print(f"   åŸå§‹å®‰å…¨æƒé‡: {self.config['training']['safety_weight']}")
        
        # ä¿®æ”¹é…ç½®ä»¥æ”¯æŒåŠ¨æ€alpha
        self._modify_config_for_alpha_debug()
        
        # åˆ›å»ºç­–ç•¥ç½‘ç»œ
        self.policy = BPTTPolicy(self.config['networks']['policy']).to(self.device)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        policy_state_path = self.model_path / "policy.pt"
        if policy_state_path.exists():
            print(f"   åŠ è½½ç­–ç•¥æƒé‡: {policy_state_path}")
            state_dict = torch.load(policy_state_path, map_location=self.device)
            
            # å¤„ç†å¯èƒ½çš„é”®ä¸åŒ¹é…ï¼ˆæ—§æ¨¡å‹å…¼å®¹æ€§ï¼‰
            self._load_compatible_state_dict(state_dict)
        else:
            print("   âš ï¸  æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    def _modify_config_for_alpha_debug(self):
        """ä¿®æ”¹é…ç½®ä»¥æ”¯æŒåŠ¨æ€alphaè°ƒè¯•"""
        print("   ğŸ”§ ä¿®æ”¹é…ç½®ä»¥æ”¯æŒåŠ¨æ€alpha...")
        
        # ç¡®ä¿ç­–ç•¥å¤´å¯ç”¨alphaé¢„æµ‹
        if 'policy_head' not in self.config['networks']['policy']:
            self.config['networks']['policy']['policy_head'] = {}
        
        self.config['networks']['policy']['policy_head'].update({
            'predict_alpha': True,
            'alpha_hidden_dim': 64,
            'output_dim': 2,  # 2DåŠ¨ä½œç©ºé—´
            'alpha_bounds': [0.1, 5.0]  # alphaèŒƒå›´
        })
        
        # è®¾ç½®è°ƒè¯•ç¯å¢ƒå‚æ•°
        self.config['env'].update({
            'num_agents': 2,  # ç®€åŒ–ä¸ºä¸¤æ™ºèƒ½ä½“
            'area_size': 2.0,  # å¢å¤§ç©ºé—´
            'cbf_alpha': 1.0,  # åŸºç¡€alphaå€¼
            'comm_radius': 1.5,  # å¢å¤§é€šä¿¡èŒƒå›´
            'max_steps': 100   # å‡å°‘æ­¥æ•°ä»¥å¿«é€Ÿè°ƒè¯•
        })
        
        print(f"      âœ… æ™ºèƒ½ä½“æ•°é‡: {self.config['env']['num_agents']}")
        print(f"      âœ… å¯ç”¨åŠ¨æ€alpha: {self.config['networks']['policy']['policy_head']['predict_alpha']}")
    
    def _load_compatible_state_dict(self, state_dict):
        """åŠ è½½å…¼å®¹çš„çŠ¶æ€å­—å…¸ï¼Œå¤„ç†é”®åä¸åŒ¹é…"""
        try:
            # å°è¯•ç›´æ¥åŠ è½½
            self.policy.load_state_dict(state_dict, strict=False)
            print("   âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡")
        except Exception as e:
            print(f"   âš ï¸  æƒé‡åŠ è½½å¼‚å¸¸: {e}")
            print("   ğŸ”„ å°è¯•å…¼å®¹æ€§æ˜ å°„...")
            
            # åˆ›å»ºå…¼å®¹æ€§æ˜ å°„
            compatible_dict = {}
            policy_state = self.policy.state_dict()
            
            # æ˜ å°„å·²çŸ¥çš„é”®åå˜åŒ–
            key_mappings = {
                'head.': 'policy_head.',
                'memory.gru_cell': 'memory.gru',
                # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šæ˜ å°„
            }
            
            for old_key, tensor in state_dict.items():
                new_key = old_key
                for old_pattern, new_pattern in key_mappings.items():
                    new_key = new_key.replace(old_pattern, new_pattern)
                
                if new_key in policy_state:
                    compatible_dict[new_key] = tensor
                    print(f"      æ˜ å°„: {old_key} -> {new_key}")
            
            # åŠ è½½æ˜ å°„åçš„æƒé‡
            self.policy.load_state_dict(compatible_dict, strict=False)
            print("   âœ… ä½¿ç”¨å…¼å®¹æ€§æ˜ å°„æˆåŠŸåŠ è½½éƒ¨åˆ†æƒé‡")
    
    def _create_debug_environment(self):
        """åˆ›å»ºç®€åŒ–çš„è°ƒè¯•ç¯å¢ƒ"""
        print("\nğŸŒ åˆ›å»ºè°ƒè¯•ç¯å¢ƒ...")
        
        # åˆ›å»ºåŒç§¯åˆ†å™¨ç¯å¢ƒ
        self.env = DoubleIntegratorEnv(self.config['env'])
        
        # åˆ›å»ºå®‰å…¨å±‚
        self.safety_layer = GCBFSafetyLayer(
            alpha=self.config['env']['cbf_alpha'],
            device=self.device
        )
        
        print(f"   ç¯å¢ƒå°ºå¯¸: {self.config['env']['area_size']}m x {self.config['env']['area_size']}m")
        print(f"   æ™ºèƒ½ä½“æ•°é‡: {self.config['env']['num_agents']}")
        print(f"   åŸºç¡€CBF alpha: {self.config['env']['cbf_alpha']}")
        
        # åˆ›å»ºå¯¹æ’åˆå§‹çŠ¶æ€
        self._create_collision_scenario()
    
    def _create_collision_scenario(self):
        """åˆ›å»ºä¸¤æ™ºèƒ½ä½“æ­£é¢å¯¹æ’åœºæ™¯"""
        print("   ğŸš—ğŸ’¥ è®¾ç½®æ­£é¢å¯¹æ’åœºæ™¯...")
        
        # æ™ºèƒ½ä½“1: ä»å·¦ä¾§å‘å³ç§»åŠ¨
        agent1_pos = np.array([-0.8, 0.0])  # å·¦ä¾§èµ·å§‹
        agent1_vel = np.array([0.5, 0.0])   # å‘å³ç§»åŠ¨
        
        # æ™ºèƒ½ä½“2: ä»å³ä¾§å‘å·¦ç§»åŠ¨  
        agent2_pos = np.array([0.8, 0.0])   # å³ä¾§èµ·å§‹
        agent2_vel = np.array([-0.5, 0.0])  # å‘å·¦ç§»åŠ¨
        
        # æ„å»ºåˆå§‹çŠ¶æ€
        self.initial_positions = np.stack([agent1_pos, agent2_pos])
        self.initial_velocities = np.stack([agent1_vel, agent2_vel])
        
        print(f"      æ™ºèƒ½ä½“1: pos={agent1_pos}, vel={agent1_vel}")
        print(f"      æ™ºèƒ½ä½“2: pos={agent2_pos}, vel={agent2_vel}")
        print(f"      é¢„è®¡ç¢°æ’æ—¶é—´: {1.6/1.0:.1f}ç§’ (å¦‚æ— å¹²é¢„)")
    
    def _setup_debug_training(self):
        """è®¾ç½®è°ƒè¯•è®­ç»ƒ"""
        print("\nğŸ¯ è®¾ç½®è°ƒè¯•è®­ç»ƒ...")
        
        # å†»ç»“é™¤alpha_headå¤–çš„æ‰€æœ‰å‚æ•°
        self._freeze_non_alpha_parameters()
        
        # åˆ›å»ºåªé’ˆå¯¹alphaå‚æ•°çš„ä¼˜åŒ–å™¨
        alpha_params = []
        for name, param in self.policy.named_parameters():
            if 'alpha_network' in name and param.requires_grad:
                alpha_params.append(param)
        
        if len(alpha_params) == 0:
            print("   âš ï¸  æœªæ‰¾åˆ°å¯è®­ç»ƒçš„alphaå‚æ•°ï¼")
            # å¦‚æœæ²¡æœ‰alphaç½‘ç»œï¼Œéœ€è¦æ·»åŠ 
            self._add_alpha_network()
            alpha_params = [p for n, p in self.policy.named_parameters() 
                          if 'alpha_network' in n and p.requires_grad]
        
        self.alpha_optimizer = optim.Adam(alpha_params, lr=0.001)
        
        print(f"   å¯è®­ç»ƒalphaå‚æ•°æ•°é‡: {sum(p.numel() for p in alpha_params)}")
        print(f"   ä¼˜åŒ–å™¨: Adam (lr=0.001)")
        
        # è°ƒè¯•æŒ‡æ ‡å­˜å‚¨
        self.debug_metrics = {
            'steps': [],
            'predicted_alpha': [],
            'safety_loss': [],
            'alpha_reg_loss': [],
            'total_loss': [],
            'min_distance': [],
            'collision_occurred': []
        }
    
    def _freeze_non_alpha_parameters(self):
        """å†»ç»“é™¤alphaç½‘ç»œå¤–çš„æ‰€æœ‰å‚æ•°"""
        frozen_params = 0
        trainable_params = 0
        
        for name, param in self.policy.named_parameters():
            if 'alpha_network' in name:
                param.requires_grad = True
                trainable_params += param.numel()
                print(f"   ğŸ”“ å¯è®­ç»ƒ: {name} ({param.numel()} å‚æ•°)")
            else:
                param.requires_grad = False
                frozen_params += param.numel()
        
        print(f"   â„ï¸  å†»ç»“å‚æ•°: {frozen_params:,}")
        print(f"   ğŸ”¥ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    def _add_alpha_network(self):
        """å¦‚æœç¼ºå¤±ï¼Œæ·»åŠ alphaç½‘ç»œ"""
        print("   ğŸ”§ æ·»åŠ alphaé¢„æµ‹ç½‘ç»œ...")
        
        if not hasattr(self.policy.policy_head, 'alpha_network') or self.policy.policy_head.alpha_network is None:
            # è·å–è¾“å…¥ç»´åº¦
            input_dim = self.policy.policy_head.input_dim
            alpha_hidden_dim = 64
            
            # åˆ›å»ºalphaç½‘ç»œ
            self.policy.policy_head.alpha_network = nn.Sequential(
                nn.Linear(input_dim, alpha_hidden_dim),
                nn.ReLU(),
                nn.Linear(alpha_hidden_dim, 1),
                nn.Softplus()  # ç¡®ä¿alpha > 0
            ).to(self.device)
            
            # å¯ç”¨alphaé¢„æµ‹
            self.policy.policy_head.predict_alpha = True
            
            print(f"      âœ… åˆ›å»ºalphaç½‘ç»œ: {input_dim} -> {alpha_hidden_dim} -> 1")
    
    def run_debug_loop(self, max_steps: int = 100):
        """è¿è¡Œè°ƒè¯•è®­ç»ƒå¾ªç¯"""
        print(f"\nğŸš€ å¼€å§‹è°ƒè¯•è®­ç»ƒå¾ªç¯ (æœ€å¤§æ­¥æ•°: {max_steps})")
        print("=" * 60)
        
        for step in range(max_steps):
            # é‡ç½®ç¯å¢ƒåˆ°åˆå§‹å¯¹æ’çŠ¶æ€
            state = self._reset_to_collision_scenario()
            
            # å‰å‘ä¼ æ’­
            obs_tensor = torch.FloatTensor(state.observations).unsqueeze(0).to(self.device)
            actions, predicted_alpha = self.policy(obs_tensor)
            
            # æå–é¢„æµ‹çš„alphaå€¼
            if predicted_alpha is not None:
                alpha_value = predicted_alpha.mean().item()
            else:
                alpha_value = self.config['env']['cbf_alpha']  # ä½¿ç”¨é»˜è®¤å€¼
                print(f"   âš ï¸  æ­¥éª¤ {step}: æœªé¢„æµ‹alphaï¼Œä½¿ç”¨é»˜è®¤å€¼ {alpha_value}")
            
            # åº”ç”¨å®‰å…¨å±‚ï¼ˆä½¿ç”¨é¢„æµ‹çš„alphaï¼‰
            safe_actions = self._apply_safety_layer(state, actions.squeeze(0), alpha_value)
            
            # è®¡ç®—æŸå¤±
            safety_loss, alpha_reg_loss, total_loss = self._calculate_debug_losses(
                state, safe_actions, predicted_alpha, alpha_value
            )
            
            # åå‘ä¼ æ’­ï¼ˆä»…æ›´æ–°alphaç½‘ç»œï¼‰
            self.alpha_optimizer.zero_grad()
            total_loss.backward()
            self.alpha_optimizer.step()
            
            # è®¡ç®—é¢å¤–æŒ‡æ ‡
            min_distance = self._calculate_min_distance(state.positions)
            collision_occurred = min_distance < (2 * self.config['env']['agent_radius'])
            
            # å­˜å‚¨è°ƒè¯•æŒ‡æ ‡
            self._store_debug_metrics(step, alpha_value, safety_loss.item(), 
                                    alpha_reg_loss.item(), total_loss.item(), 
                                    min_distance, collision_occurred)
            
            # æ‰“å°è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            self._print_debug_info(step, alpha_value, safety_loss.item(), 
                                 alpha_reg_loss.item(), total_loss.item(), 
                                 min_distance, collision_occurred)
            
            # æ¯10æ­¥æ‰“å°åˆ†éš”çº¿
            if (step + 1) % 10 == 0:
                print("-" * 60)
        
        print("ğŸ è°ƒè¯•è®­ç»ƒå®Œæˆ!")
        self._analyze_results()
    
    def _reset_to_collision_scenario(self):
        """é‡ç½®ç¯å¢ƒåˆ°å¯¹æ’åœºæ™¯"""
        # é‡ç½®ç¯å¢ƒ
        state = self.env.reset()
        
        # è®¾ç½®å›ºå®šçš„å¯¹æ’åˆå§‹çŠ¶æ€
        state.positions = self.initial_positions.copy()
        state.velocities = self.initial_velocities.copy()
        
        # é‡ç½®ç­–ç•¥è®°å¿†
        if hasattr(self.policy, 'memory') and hasattr(self.policy.memory, 'reset'):
            self.policy.memory.reset()
        
        return state
    
    def _apply_safety_layer(self, state, actions, alpha_value):
        """åº”ç”¨å®‰å…¨å±‚çº¦æŸ"""
        # è½¬æ¢çŠ¶æ€ä¸ºå®‰å…¨å±‚æ ¼å¼
        positions = torch.FloatTensor(state.positions).to(self.device)
        velocities = torch.FloatTensor(state.velocities).to(self.device)
        
        # åº”ç”¨å®‰å…¨çº¦æŸï¼ˆä½¿ç”¨é¢„æµ‹çš„alphaï¼‰
        safe_actions = self.safety_layer.apply_safety_constraint(
            positions, velocities, actions, alpha=alpha_value
        )
        
        return safe_actions
    
    def _calculate_debug_losses(self, state, safe_actions, predicted_alpha, alpha_value):
        """è®¡ç®—è°ƒè¯•æŸå¤±"""
        # å®‰å…¨æŸå¤±ï¼šåŸºäºCBFçº¦æŸ
        positions = torch.FloatTensor(state.positions).to(self.device)
        velocities = torch.FloatTensor(state.velocities).to(self.device)
        
        # è®¡ç®—æœ€å°è·ç¦»
        distances = torch.cdist(positions, positions)
        distances = distances + torch.eye(len(positions), device=self.device) * 1e6  # å¿½ç•¥è‡ªèº«
        min_distance = distances.min()
        
        # å®‰å…¨æŸå¤±ï¼šè·ç¦»è¶Šå°ï¼ŒæŸå¤±è¶Šå¤§
        safety_radius = 2 * self.config['env']['agent_radius']
        safety_loss = torch.clamp(safety_radius - min_distance, min=0).pow(2)
        
        # Alphaæ­£åˆ™åŒ–æŸå¤±ï¼šé¼“åŠ±é€‚ä¸­çš„alphaå€¼
        if predicted_alpha is not None:
            target_alpha = 1.5  # ç›®æ ‡alphaå€¼
            alpha_reg_loss = (predicted_alpha.mean() - target_alpha).pow(2) * 0.01
        else:
            alpha_reg_loss = torch.tensor(0.0, device=self.device)
        
        # æ€»æŸå¤±
        total_loss = safety_loss + alpha_reg_loss
        
        return safety_loss, alpha_reg_loss, total_loss
    
    def _calculate_min_distance(self, positions):
        """è®¡ç®—æ™ºèƒ½ä½“é—´æœ€å°è·ç¦»"""
        if len(positions) < 2:
            return float('inf')
        
        distances = []
        for i in range(len(positions)):
            for j in range(i + 1, len(positions)):
                dist = np.linalg.norm(positions[i] - positions[j])
                distances.append(dist)
        
        return min(distances)
    
    def _store_debug_metrics(self, step, alpha_value, safety_loss, alpha_reg_loss, 
                           total_loss, min_distance, collision_occurred):
        """å­˜å‚¨è°ƒè¯•æŒ‡æ ‡"""
        self.debug_metrics['steps'].append(step)
        self.debug_metrics['predicted_alpha'].append(alpha_value)
        self.debug_metrics['safety_loss'].append(safety_loss)
        self.debug_metrics['alpha_reg_loss'].append(alpha_reg_loss)
        self.debug_metrics['total_loss'].append(total_loss)
        self.debug_metrics['min_distance'].append(min_distance)
        self.debug_metrics['collision_occurred'].append(collision_occurred)
    
    def _print_debug_info(self, step, alpha_value, safety_loss, alpha_reg_loss, 
                         total_loss, min_distance, collision_occurred):
        """æ‰“å°è¯¦ç»†è°ƒè¯•ä¿¡æ¯"""
        collision_status = "ğŸš¨ ç¢°æ’!" if collision_occurred else "âœ… å®‰å…¨"
        
        print(f"æ­¥éª¤ {step:3d} | "
              f"Alpha: {alpha_value:.3f} | "
              f"å®‰å…¨æŸå¤±: {safety_loss:.4f} | "
              f"Alphaæ­£åˆ™: {alpha_reg_loss:.4f} | "
              f"æ€»æŸå¤±: {total_loss:.4f} | "
              f"æœ€å°è·ç¦»: {min_distance:.3f}m | "
              f"{collision_status}")
    
    def _analyze_results(self):
        """åˆ†æè°ƒè¯•ç»“æœ"""
        print("\nğŸ“Š è°ƒè¯•ç»“æœåˆ†æ:")
        print("=" * 50)
        
        metrics = self.debug_metrics
        
        # åŸºæœ¬ç»Ÿè®¡
        final_alpha = metrics['predicted_alpha'][-1]
        initial_alpha = metrics['predicted_alpha'][0]
        alpha_change = final_alpha - initial_alpha
        
        avg_safety_loss = np.mean(metrics['safety_loss'])
        avg_alpha_reg_loss = np.mean(metrics['alpha_reg_loss'])
        min_distance_achieved = min(metrics['min_distance'])
        collision_rate = sum(metrics['collision_occurred']) / len(metrics['steps'])
        
        print(f"Alphaå­¦ä¹ :")
        print(f"  åˆå§‹Alpha: {initial_alpha:.3f}")
        print(f"  æœ€ç»ˆAlpha: {final_alpha:.3f}")
        print(f"  Alphaå˜åŒ–: {alpha_change:+.3f}")
        
        print(f"\næŸå¤±åˆ†æ:")
        print(f"  å¹³å‡å®‰å…¨æŸå¤±: {avg_safety_loss:.4f}")
        print(f"  å¹³å‡Alphaæ­£åˆ™æŸå¤±: {avg_alpha_reg_loss:.4f}")
        
        print(f"\nå®‰å…¨æ€§èƒ½:")
        print(f"  æœ€å°è·ç¦»: {min_distance_achieved:.3f}m")
        print(f"  ç¢°æ’ç‡: {collision_rate:.1%}")
        
        # å­¦ä¹ è¶‹åŠ¿åˆ†æ
        if len(metrics['steps']) > 10:
            early_alpha = np.mean(metrics['predicted_alpha'][:10])
            late_alpha = np.mean(metrics['predicted_alpha'][-10:])
            learning_trend = "ä¸Šå‡" if late_alpha > early_alpha else "ä¸‹é™"
            
            print(f"\nå­¦ä¹ è¶‹åŠ¿:")
            print(f"  å‰10æ­¥å¹³å‡Alpha: {early_alpha:.3f}")
            print(f"  å10æ­¥å¹³å‡Alpha: {late_alpha:.3f}")
            print(f"  æ•´ä½“è¶‹åŠ¿: Alpha {learning_trend}")
        
        # ç”Ÿæˆå¯è§†åŒ–
        self._plot_debug_results()
    
    def _plot_debug_results(self):
        """ç»˜åˆ¶è°ƒè¯•ç»“æœ"""
        print("\nğŸ“ˆ ç”Ÿæˆè°ƒè¯•å¯è§†åŒ–å›¾è¡¨...")
        
        metrics = self.debug_metrics
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('åŠ¨æ€Alphaè°ƒè¯•ç»“æœåˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. Alphaé¢„æµ‹å˜åŒ–
        ax1.plot(metrics['steps'], metrics['predicted_alpha'], 'b-', linewidth=2, label='é¢„æµ‹Alpha')
        ax1.axhline(y=1.5, color='r', linestyle='--', alpha=0.7, label='ç›®æ ‡Alpha')
        ax1.set_xlabel('è®­ç»ƒæ­¥æ•°')
        ax1.set_ylabel('Alphaå€¼')
        ax1.set_title('Alphaé¢„æµ‹å­¦ä¹ æ›²çº¿')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. æŸå¤±å˜åŒ–
        ax2.plot(metrics['steps'], metrics['safety_loss'], 'r-', label='å®‰å…¨æŸå¤±', alpha=0.8)
        ax2.plot(metrics['steps'], metrics['alpha_reg_loss'], 'g-', label='Alphaæ­£åˆ™æŸå¤±', alpha=0.8)
        ax2.plot(metrics['steps'], metrics['total_loss'], 'k-', label='æ€»æŸå¤±', linewidth=2)
        ax2.set_xlabel('è®­ç»ƒæ­¥æ•°')
        ax2.set_ylabel('æŸå¤±å€¼')
        ax2.set_title('æŸå¤±å‡½æ•°å˜åŒ–')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_yscale('log')
        
        # 3. æœ€å°è·ç¦»å˜åŒ–
        ax3.plot(metrics['steps'], metrics['min_distance'], 'purple', linewidth=2)
        safety_radius = 2 * self.config['env']['agent_radius']
        ax3.axhline(y=safety_radius, color='r', linestyle='--', alpha=0.7, label=f'å®‰å…¨é˜ˆå€¼ ({safety_radius:.2f}m)')
        ax3.set_xlabel('è®­ç»ƒæ­¥æ•°')
        ax3.set_ylabel('æœ€å°è·ç¦» (m)')
        ax3.set_title('æ™ºèƒ½ä½“é—´æœ€å°è·ç¦»')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ç¢°æ’çŠ¶æ€
        collision_indicator = [1 if c else 0 for c in metrics['collision_occurred']]
        ax4.scatter(metrics['steps'], collision_indicator, c=['red' if c else 'green' for c in collision_indicator], alpha=0.6)
        ax4.set_xlabel('è®­ç»ƒæ­¥æ•°')
        ax4.set_ylabel('ç¢°æ’çŠ¶æ€')
        ax4.set_title('ç¢°æ’å‘ç”Ÿæƒ…å†µ')
        ax4.set_yticks([0, 1])
        ax4.set_yticklabels(['å®‰å…¨', 'ç¢°æ’'])
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('debug_dynamic_alpha_results.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print("   âœ… è°ƒè¯•å›¾è¡¨å·²ä¿å­˜: debug_dynamic_alpha_results.png")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš¨ åŠ¨æ€Alphaè°ƒè¯•å™¨å¯åŠ¨")
    print("=" * 60)
    
    # æ£€æŸ¥å¯ç”¨æ¨¡å‹
    available_models = []
    logs_dir = Path("logs")
    if logs_dir.exists():
        for subdir in logs_dir.iterdir():
            if subdir.is_dir():
                models_dir = subdir / "models"
                if models_dir.exists():
                    for model_dir in models_dir.iterdir():
                        if model_dir.is_dir() and (model_dir / "policy.pt").exists():
                            available_models.append(str(model_dir))
    
    if not available_models:
        print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹!")
        print("   è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹")
        return
    
    print(f"ğŸ” å‘ç° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹:")
    for i, model in enumerate(available_models):
        print(f"   {i+1}. {model}")
    
    # é€‰æ‹©æœ€æ–°çš„æ¨¡å‹ï¼ˆé»˜è®¤ï¼‰
    selected_model = "logs/bptt/models/1000"  # é»„é‡‘åŸºå‡†
    if os.path.exists(selected_model):
        print(f"\nğŸ¯ ä½¿ç”¨é»„é‡‘åŸºå‡†æ¨¡å‹: {selected_model}")
    else:
        # å¦‚æœé»˜è®¤æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„
        selected_model = available_models[0]
        print(f"\nğŸ¯ ä½¿ç”¨æ¨¡å‹: {selected_model}")
    
    try:
        # åˆ›å»ºè°ƒè¯•å™¨
        debugger = AlphaDebugger(selected_model)
        
        # è¿è¡Œè°ƒè¯•å¾ªç¯
        debugger.run_debug_loop(max_steps=100)
        
        print("\nğŸ‰ è°ƒè¯•å®Œæˆ!")
        print("   æŸ¥çœ‹ debug_dynamic_alpha_results.png ä»¥è·å¾—è¯¦ç»†åˆ†æ")
        
    except Exception as e:
        print(f"\nâŒ è°ƒè¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()