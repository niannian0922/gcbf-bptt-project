#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„BPTTå¯è§†åŒ–è„šæœ¬
å®Œå…¨é•œåƒtrain_bptt.pyçš„é…ç½®åŠ è½½ã€ç¯å¢ƒåˆ›å»ºå’Œæ¨¡å‹å®ä¾‹åŒ–é€»è¾‘
"""

import argparse
import os
import random
import numpy as np
import torch
import torch.nn as nn
import yaml
from pathlib import Path
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.env.gcbf_safety_layer import GCBFSafetyLayer
from gcbfplus.policy import BPTTPolicy, create_policy_from_config


def load_trained_model(model_dir, step=None, device='cpu'):
    """
    å®Œå…¨é•œåƒtrain_bptt.pyçš„æ¨¡å‹åŠ è½½é€»è¾‘
    """
    print(f"ğŸ” ç»Ÿä¸€æ¨¡å‹åŠ è½½æµç¨‹")
    print(f"ğŸ“ æ¨¡å‹ç›®å½•: {model_dir}")
    
    # 1. æŸ¥æ‰¾é…ç½®æ–‡ä»¶ - é•œåƒè®­ç»ƒè„šæœ¬çš„é€»è¾‘
    config_path = os.path.join(model_dir, 'config.yaml')
    if not os.path.exists(config_path):
        # å°è¯•çˆ¶ç›®å½•
        config_path = os.path.join(model_dir, '..', 'config.yaml')
        if not os.path.exists(config_path):
            # å°è¯•æ ¹ç›®å½•çš„é…ç½®æ–‡ä»¶
            possible_configs = [
                'config/simple_collaboration.yaml', 
                'config/alpha_medium_obs.yaml',
                'config/bptt_config.yaml'
            ]
            for config_file in possible_configs:
                if os.path.exists(config_file):
                    config_path = config_file
                    break
            else:
                raise ValueError(f"æ— æ³•æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œæ£€æŸ¥è¿‡çš„è·¯å¾„: {possible_configs}")
    
    print(f"ğŸ“‹ ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
    
    # 2. åŠ è½½é…ç½® - å®Œå…¨é•œåƒtrain_bptt.pyçš„é€»è¾‘
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
    
    # 3. æå–é…ç½®éƒ¨åˆ† - é•œåƒtrain_bptt.py
    env_config = config.get('env', {})
    training_config = config.get('training', {})
    network_config = config.get('networks', {})
    
    # å¦‚æœé…ç½®ä¸­æ²¡æœ‰networkséƒ¨åˆ†ï¼Œæ·»åŠ é»˜è®¤å€¼
    if not network_config:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ç¼ºå°‘networkséƒ¨åˆ†ï¼Œæ·»åŠ é»˜è®¤é…ç½®")
        network_config = {
            'policy': {},
            'cbf': {'alpha': 1.0}
        }
        config['networks'] = network_config
    
    # ç¡®ä¿ç¯å¢ƒæœ‰éšœç¢ç‰©é…ç½®ï¼ˆå› ä¸ºæ¨¡å‹æ˜¯åœ¨9ç»´è¾“å…¥ä¸‹è®­ç»ƒçš„ï¼‰
    if 'obstacles' not in env_config:
        print(f"âš ï¸ æ·»åŠ éšœç¢ç‰©é…ç½®ä»¥åŒ¹é…9ç»´è¾“å…¥æ¨¡å‹")
        env_config['obstacles'] = {
            'enabled': True,
            'bottleneck': True,
            'positions': [[0.0, -0.8], [0.0, 0.8]],
            'radii': [0.4, 0.4]
        }
    
    # æå–ç­–ç•¥å’ŒCBFç½‘ç»œé…ç½®
    policy_config = network_config.get('policy', {})
    cbf_network_config = network_config.get('cbf')
    
    print(f"ğŸ“Š ç¯å¢ƒé…ç½®: {list(env_config.keys())}")
    print(f"ğŸ§  ç­–ç•¥é…ç½®: {list(policy_config.keys())}")
    print(f"ğŸ›¡ï¸ CBFé…ç½®: {cbf_network_config is not None}")
    
    # 4. åˆ›å»ºç¯å¢ƒ - å®Œå…¨é•œåƒtrain_bptt.pyçš„é€»è¾‘
    env_type = 'double_integrator'  # é»˜è®¤å€¼ï¼Œé•œåƒè®­ç»ƒè„šæœ¬
    
    if env_type == 'double_integrator':
        env = DoubleIntegratorEnv(env_config)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç¯å¢ƒç±»å‹: {env_type}")
    
    print(f"ğŸŒ ç¯å¢ƒåˆ›å»ºæˆåŠŸ: {env_type}")
    
    # å°†ç¯å¢ƒç§»åŠ¨åˆ°è®¾å¤‡
    env = env.to(device)
    
    # 5. åˆ›å»ºç­–ç•¥ç½‘ç»œ - å®Œå…¨é•œåƒtrain_bptt.pyçš„é€»è¾‘
    if policy_config:
        # ç¡®ä¿ç­–ç•¥é…ç½®å…·æœ‰æ­£ç¡®çš„è§‚æµ‹å’ŒåŠ¨ä½œç»´åº¦
        obs_shape = env.observation_shape
        action_shape = env.action_shape
        
        print(f"ğŸ“ è§‚æµ‹å½¢çŠ¶: {obs_shape}")
        print(f"ğŸ“ åŠ¨ä½œå½¢çŠ¶: {action_shape}")
        
        # DEBUG: æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” DEBUG: obs_shapeç±»å‹={type(obs_shape)}, å€¼={obs_shape}")
        print(f"ğŸ” DEBUG: action_shapeç±»å‹={type(action_shape)}, å€¼={action_shape}")
        
        # å¦‚æœéœ€è¦ï¼Œä¸ºç¼ºå¤±çš„æ„ŸçŸ¥é…ç½®æ·»åŠ é»˜è®¤å€¼ - é•œåƒè®­ç»ƒè„šæœ¬
        if 'perception' not in policy_config:
            policy_config['perception'] = {}
        
        perception_config = policy_config['perception']
        
        # å¤„ç†è§†è§‰è¾“å…¥ - é•œåƒè®­ç»ƒè„šæœ¬
        if len(obs_shape) > 2:  # è§†è§‰è¾“å…¥ [n_agents, channels, height, width]
            perception_config.update({
                'use_vision': True,
                'input_dim': obs_shape[-3:],  # [channels, height, width]
                'output_dim': perception_config.get('output_dim', 256)
            })
        else:  # çŠ¶æ€è¾“å…¥ [n_agents, obs_dim]
            perception_config.update({
                'use_vision': False,
                'input_dim': obs_shape[-1],  # obs_dim
                'output_dim': perception_config.get('output_dim', 128),
                'hidden_dims': perception_config.get('hidden_dims', [256, 256])
            })
        
        print(f"ğŸ” DEBUG: perception_config={perception_config}")
        
        # å¦‚æœéœ€è¦ï¼Œæ·»åŠ é»˜è®¤è®°å¿†é…ç½® - é•œåƒè®­ç»ƒè„šæœ¬
        if 'memory' not in policy_config:
            policy_config['memory'] = {}
        
        memory_config = policy_config['memory']
        memory_config.update({
            'hidden_dim': memory_config.get('hidden_dim', 128),
            'num_layers': memory_config.get('num_layers', 1)
        })
        
        # ç¡®ä¿policy_headå…·æœ‰æ‰€æœ‰å¿…éœ€å‚æ•° - é•œåƒè®­ç»ƒè„šæœ¬
        if 'policy_head' not in policy_config:
            # ä»æ„ŸçŸ¥æˆ–è®°å¿†é…ç½®è·å–hidden_dimï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
            if len(obs_shape) > 2:  # è§†è§‰æƒ…å†µ
                hidden_dims = perception_config.get('output_dim', 256)
            else:  # çŠ¶æ€æƒ…å†µ
                hidden_dims = perception_config.get('hidden_dims', [256, 256])
                if isinstance(hidden_dims, list):
                    hidden_dims = hidden_dims[0] if hidden_dims else 256
            
            policy_config['policy_head'] = {
                'output_dim': action_shape[-1],  # action_dim
                'hidden_dims': [hidden_dims],
                'activation': 'relu',
                'predict_alpha': True  # å¯ç”¨è‡ªé€‚åº”å®‰å…¨è¾¹è·
            }
        else:
            policy_head_config = policy_config['policy_head']
            policy_head_config['output_dim'] = action_shape[-1]  # ç¡®ä¿æ­£ç¡®çš„åŠ¨ä½œç»´åº¦
            if 'predict_alpha' not in policy_head_config:
                policy_head_config['predict_alpha'] = True  # é»˜è®¤å¯ç”¨åŠ¨æ€alpha
        
        print(f"ğŸ¯ æœ€ç»ˆç­–ç•¥é…ç½®: {policy_config}")
    else:
        # åå¤‡æ–¹æ¡ˆï¼šå¦‚æœYAMLä¸­æ²¡æœ‰ç­–ç•¥é…ç½®ï¼Œåˆ›å»ºé»˜è®¤é…ç½® - é•œåƒè®­ç»ƒè„šæœ¬
        obs_shape = env.observation_shape
        action_shape = env.action_shape
        
        print(f"ğŸ“ åå¤‡ - è§‚æµ‹å½¢çŠ¶: {obs_shape}")
        print(f"ğŸ“ åå¤‡ - åŠ¨ä½œå½¢çŠ¶: {action_shape}")
        
        if len(obs_shape) > 2:  # è§†è§‰è¾“å…¥
            policy_config = {
                'perception': {
                    'use_vision': True,
                    'input_dim': obs_shape[-3:],  # [channels, height, width]
                    'output_dim': 256,
                    'vision': {
                        'input_channels': obs_shape[-3],
                        'channels': [32, 64, 128],
                        'height': obs_shape[-2],
                        'width': obs_shape[-1]
                    }
                },
                'memory': {
                    'hidden_dim': 128,
                    'num_layers': 1
                },
                'policy_head': {
                    'output_dim': action_shape[-1],
                    'hidden_dims': [256],
                    'activation': 'relu',
                    'predict_alpha': True
                }
            }
        else:  # çŠ¶æ€è¾“å…¥
            policy_config = {
                'perception': {
                    'use_vision': False,
                    'input_dim': obs_shape[-1],
                    'output_dim': 128,
                    'hidden_dims': [256, 256],
                    'activation': 'relu'
                },
                'memory': {
                    'hidden_dim': 128,
                    'num_layers': 1
                },
                'policy_head': {
                    'output_dim': action_shape[-1],
                    'hidden_dims': [256, 256],
                    'activation': 'relu',
                    'predict_alpha': True
                }
            }
    
    # 6. åˆ›å»ºç­–ç•¥ç½‘ç»œ - æ ¹æ®å®é™…æ¨¡å‹æƒé‡æ¨æ–­æ¶æ„
    print(f"ğŸ§  åˆ›å»ºç­–ç•¥ç½‘ç»œ...")
    
    # é¦–å…ˆå°è¯•ä»æ¨¡å‹æ–‡ä»¶æ¨æ–­æ­£ç¡®çš„æ¶æ„
    model_step = None
    models_dir = os.path.join(model_dir, 'models')
    if os.path.exists(models_dir):
        steps = [int(d) for d in os.listdir(models_dir) if d.isdigit()]
        if steps:
            model_step = max(steps)
    
    if model_step:
        policy_path = os.path.join(model_dir, 'models', str(model_step), 'policy.pt')
        if os.path.exists(policy_path):
            print(f"ğŸ” ä»æ¨¡å‹æƒé‡æ¨æ–­ç½‘ç»œæ¶æ„: {policy_path}")
            policy_state_dict = torch.load(policy_path, map_location='cpu', weights_only=True)
            
            # åˆ†ææƒé‡æ¥æ¨æ–­æ­£ç¡®çš„æ¶æ„
            perception_out_features = None
            memory_hidden_dim = None
            
            # ä»ç¬¬ä¸€å±‚æ¨æ–­perceptionè¾“å‡ºç»´åº¦
            if 'perception.mlp.0.weight' in policy_state_dict:
                perception_out_features = policy_state_dict['perception.mlp.0.weight'].shape[0]
                print(f"ğŸ” æ¨æ–­perceptionè¾“å‡ºç»´åº¦: {perception_out_features}")
            
            # ä»memoryå±‚æ¨æ–­hidden_dim
            if 'memory.gru.weight_hh_l0' in policy_state_dict:
                memory_hidden_dim = policy_state_dict['memory.gru.weight_hh_l0'].shape[1]
                print(f"ğŸ” æ¨æ–­memory hiddenç»´åº¦: {memory_hidden_dim}")
            
            # æ›´æ–°policyé…ç½®
            if perception_out_features:
                policy_config['perception']['output_dim'] = perception_out_features
                policy_config['perception']['hidden_dims'] = [perception_out_features, perception_out_features]
            
            if memory_hidden_dim:
                policy_config['memory']['hidden_dim'] = memory_hidden_dim
            
            # æ›´æ–°policy_headé…ç½®
            if perception_out_features:
                policy_config['policy_head']['hidden_dims'] = [perception_out_features]
            
            print(f"ğŸ¯ æ¨æ–­åçš„ç­–ç•¥é…ç½®: {policy_config}")
    
    policy_network = create_policy_from_config(policy_config)
    policy_network = policy_network.to(device)
    print(f"âœ… ç­–ç•¥ç½‘ç»œåˆ›å»ºæˆåŠŸ")
    
    # 7. åˆ›å»ºCBFç½‘ç»œ - é•œåƒtrain_bptt.pyçš„é€»è¾‘
    cbf_network = None
    if cbf_network_config:
        print(f"ğŸ›¡ï¸ åˆ›å»ºCBFç½‘ç»œ...")
        # ä»é…ç½®ä¸­æå–CBF alphaå‚æ•°
        cbf_alpha = cbf_network_config.get('alpha', 1.0)
        
        # åŸºäºCBFç½‘ç»œé…ç½®åˆ›å»ºCBFç½‘ç»œ - é•œåƒè®­ç»ƒè„šæœ¬
        obs_dim = obs_shape[-1] if len(obs_shape) <= 2 else np.prod(obs_shape[-3:])
        
        print(f"ğŸ” DEBUG: obs_dim={obs_dim}, num_agents={env_config.get('num_agents', 8)}")
        
        # âŒ è¿™é‡Œæ˜¯å…³é”®é—®é¢˜ï¼è®­ç»ƒè„šæœ¬ä¸­çš„CBFç½‘ç»œåˆ›å»ºé€»è¾‘æ˜¯é”™è¯¯çš„
        # éœ€è¦æ ¹æ®å®é™…çš„CBFæ¨¡å‹æ–‡ä»¶æ¥ç¡®å®šæ­£ç¡®çš„æ¶æ„
        
        # è®©æˆ‘ä»¬é¦–å…ˆå°è¯•åŠ è½½CBFæ¨¡å‹æ¥ç¡®å®šæ­£ç¡®çš„è¾“å…¥ç»´åº¦
        cbf_model_path = None
        if step:
            cbf_model_path = os.path.join(model_dir, 'models', str(step), 'cbf.pt')
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„CBFæ¨¡å‹
            models_dir = os.path.join(model_dir, 'models')
            if os.path.exists(models_dir):
                steps = [int(d) for d in os.listdir(models_dir) if d.isdigit()]
                if steps:
                    latest_step = max(steps)
                    cbf_model_path = os.path.join(model_dir, 'models', str(latest_step), 'cbf.pt')
        
        if cbf_model_path and os.path.exists(cbf_model_path):
            # å°è¯•åˆ†æCBFæ¨¡å‹çš„å®é™…æ¶æ„
            cbf_state_dict = torch.load(cbf_model_path, map_location='cpu', weights_only=True)
            
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªçº¿æ€§å±‚æ¥ç¡®å®šè¾“å…¥ç»´åº¦
            first_layer_key = None
            for key in cbf_state_dict.keys():
                if 'weight' in key and len(cbf_state_dict[key].shape) == 2:
                    first_layer_key = key
                    break
            
            if first_layer_key:
                actual_input_dim = cbf_state_dict[first_layer_key].shape[1]
                print(f"ğŸ” DEBUG: CBFå®é™…è¾“å…¥ç»´åº¦={actual_input_dim}")
                
                # æ ¹æ®å®é™…ç»´åº¦åˆ›å»ºCBFç½‘ç»œ
                if actual_input_dim == 6:
                    # å•ä¸ªæ™ºèƒ½ä½“çš„6ç»´çŠ¶æ€
                    cbf_network = nn.Sequential(
                        nn.Linear(6, 128),
                        nn.ReLU(),
                        nn.Linear(128, 128),
                        nn.ReLU(),
                        nn.Linear(128, 1)
                    ).to(device)
                elif actual_input_dim == 9:
                    # å•ä¸ªæ™ºèƒ½ä½“çš„9ç»´çŠ¶æ€ï¼ˆåŒ…å«éšœç¢ç‰©ï¼‰
                    cbf_network = nn.Sequential(
                        nn.Linear(9, 128),
                        nn.ReLU(),
                        nn.Linear(128, 128),
                        nn.ReLU(),
                        nn.Linear(128, 1)
                    ).to(device)
                else:
                    # ä½¿ç”¨æ£€æµ‹åˆ°çš„ç»´åº¦
                    hidden_sizes = [128, 128]  # ä»æ¨¡å‹ä¸­æ¨æ–­
                    layers = []
                    in_dim = actual_input_dim
                    for hidden_size in hidden_sizes:
                        layers.extend([nn.Linear(in_dim, hidden_size), nn.ReLU()])
                        in_dim = hidden_size
                    layers.append(nn.Linear(in_dim, 1))
                    cbf_network = nn.Sequential(*layers).to(device)
                
                print(f"âœ… CBFç½‘ç»œåˆ›å»ºæˆåŠŸ (è¾“å…¥ç»´åº¦: {actual_input_dim})")
            else:
                print(f"âš ï¸ æ— æ³•ç¡®å®šCBFç½‘ç»œæ¶æ„ï¼Œè·³è¿‡CBFç½‘ç»œ")
        else:
            print(f"âš ï¸ CBFæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {cbf_model_path}")
    
    # 8. ç¡®å®šæ¨¡å‹æ­¥éª¤ï¼ˆå¦‚æœå‰é¢æ²¡æœ‰æ‰¾åˆ°ï¼‰
    if step is None:
        if model_step:
            step = model_step
        else:
            # æŸ¥æ‰¾æœ€æ–°æ­¥éª¤
            models_dir = os.path.join(model_dir, 'models')
            if os.path.exists(models_dir):
                steps = [int(d) for d in os.listdir(models_dir) if d.isdigit()]
                if steps:
                    step = max(steps)
                else:
                    raise ValueError("åœ¨modelsç›®å½•ä¸­æ‰¾ä¸åˆ°è®­ç»ƒæ­¥éª¤")
            else:
                raise ValueError("æ‰¾ä¸åˆ°modelsç›®å½•")
    
    print(f"ğŸ“ˆ ä½¿ç”¨æ¨¡å‹æ­¥éª¤: {step}")
    
    # 9. åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    model_path = os.path.join(model_dir, 'models', str(step))
    
    # åŠ è½½ç­–ç•¥ç½‘ç»œæƒé‡ï¼ˆå¦‚æœå‰é¢æ²¡æœ‰åŠ è½½ï¼‰
    policy_path = os.path.join(model_path, 'policy.pt')
    if os.path.exists(policy_path):
        try:
            if 'policy_state_dict' not in locals():
                policy_state_dict = torch.load(policy_path, map_location=device, weights_only=True)
            policy_network.load_state_dict(policy_state_dict)
            print(f"âœ… ç­–ç•¥ç½‘ç»œæƒé‡åŠ è½½æˆåŠŸ: {policy_path}")
        except Exception as e:
            print(f"âŒ ç­–ç•¥ç½‘ç»œæƒé‡åŠ è½½å¤±è´¥: {e}")
            raise
    else:
        raise ValueError(f"ç­–ç•¥æ–‡ä»¶ä¸å­˜åœ¨: {policy_path}")
    
    # åŠ è½½CBFç½‘ç»œæƒé‡
    if cbf_network:
        cbf_path = os.path.join(model_path, 'cbf.pt')
        if os.path.exists(cbf_path):
            try:
                cbf_state_dict = torch.load(cbf_path, map_location=device, weights_only=True)
                cbf_network.load_state_dict(cbf_state_dict)
                print(f"âœ… CBFç½‘ç»œæƒé‡åŠ è½½æˆåŠŸ: {cbf_path}")
            except Exception as e:
                print(f"âŒ CBFç½‘ç»œæƒé‡åŠ è½½å¤±è´¥: {e}")
                print(f"ğŸ”§ å°†CBFç½‘ç»œè®¾ç½®ä¸ºNone")
                cbf_network = None
    
    return env, policy_network, cbf_network, config


def run_simulation_with_diagnostics(env, policy_network, cbf_network, device, num_steps=100):
    """
    è¿è¡Œä»¿çœŸå¹¶æ·»åŠ è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
    """
    print(f"ğŸ¬ å¼€å§‹ä»¿çœŸ (åŒ…å«è¯¦ç»†è¯Šæ–­)")
    print(f"ğŸ“ è®¡åˆ’æ­¥æ•°: {num_steps}")
    
    # è®¾ç½®ç½‘ç»œä¸ºè¯„ä¼°æ¨¡å¼
    policy_network.eval()
    if cbf_network:
        cbf_network.eval()
    
    # åˆå§‹åŒ–ç¯å¢ƒ
    state = env.reset()
    print(f"ğŸ” DEBUG: åˆå§‹çŠ¶æ€ç±»å‹={type(state)}")
    print(f"ğŸ” DEBUG: åˆå§‹çŠ¶æ€.positionså½¢çŠ¶={state.positions.shape}")
    
    # å­˜å‚¨è½¨è¿¹æ•°æ®
    trajectory_data = {
        'positions': [],
        'velocities': [],
        'actions': [],
        'alphas': [],
        'cbf_values': []
    }
    
    with torch.no_grad():
        for step in range(num_steps):
            print(f"\n--- æ­¥éª¤ {step} è¯Šæ–­ ---")
            
            # è®°å½•å½“å‰çŠ¶æ€
            current_positions = state.positions[0].cpu().numpy()
            current_velocities = state.velocities[0].cpu().numpy()
            trajectory_data['positions'].append(current_positions.copy())
            trajectory_data['velocities'].append(current_velocities.copy())
            
            # 1. è·å–è§‚æµ‹ - æ·»åŠ è¯Šæ–­
            observations = env.get_observations(state)
            print(f"ğŸ” DEBUG: è§‚æµ‹å½¢çŠ¶={observations.shape}")
            print(f"ğŸ” DEBUG: è§‚æµ‹dtype={observations.dtype}")
            print(f"ğŸ” DEBUG: è§‚æµ‹è®¾å¤‡={observations.device}")
            print(f"ğŸ” DEBUG: è§‚æµ‹èŒƒå›´=[{torch.min(observations):.4f}, {torch.max(observations):.4f}]")
            
            # ç¡®ä¿è§‚æµ‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            observations = observations.to(device)
            
            # 2. ç­–ç•¥ç½‘ç»œæ¨ç† - æ·»åŠ è¯Šæ–­
            try:
                print(f"ğŸ§  ç­–ç•¥ç½‘ç»œæ¨ç†...")
                policy_output = policy_network(observations, state)
                
                print(f"ğŸ” DEBUG: policy_outputç±»å‹={type(policy_output)}")
                
                if hasattr(policy_output, 'actions'):
                    actions = policy_output.actions
                    print(f"ğŸ” DEBUG: actionså½¢çŠ¶={actions.shape}")
                    print(f"ğŸ” DEBUG: actionsè®¾å¤‡={actions.device}")
                    print(f"ğŸ” DEBUG: actionsèŒƒå›´=[{torch.min(actions):.4f}, {torch.max(actions):.4f}]")
                else:
                    print(f"âŒ policy_outputæ²¡æœ‰actionså±æ€§")
                    raise ValueError("ç­–ç•¥è¾“å‡ºç¼ºå°‘actions")
                
                if hasattr(policy_output, 'alphas'):
                    alphas = policy_output.alphas
                    print(f"ğŸ” DEBUG: alphaså½¢çŠ¶={alphas.shape}")
                    print(f"ğŸ” DEBUG: alphasè®¾å¤‡={alphas.device}")
                    print(f"ğŸ” DEBUG: alphasèŒƒå›´=[{torch.min(alphas):.4f}, {torch.max(alphas):.4f}]")
                else:
                    print(f"âš ï¸ policy_outputæ²¡æœ‰alphaså±æ€§ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    alphas = torch.ones(actions.shape[0], actions.shape[1], 1, device=device) * 0.5
                
            except Exception as e:
                print(f"âŒ ç­–ç•¥ç½‘ç»œæ¨ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                break
            
            # 3. CBFç½‘ç»œæ¨ç† - æ·»åŠ è¯Šæ–­
            cbf_values = None
            if cbf_network:
                try:
                    print(f"ğŸ›¡ï¸ CBFç½‘ç»œæ¨ç†...")
                    
                    # ç¡®å®šCBFç½‘ç»œçš„è¾“å…¥æ ¼å¼
                    batch_size, num_agents, obs_dim = observations.shape
                    print(f"ğŸ” DEBUG: CBFè¾“å…¥ - batch_size={batch_size}, num_agents={num_agents}, obs_dim={obs_dim}")
                    
                    # æ ¹æ®ä¹‹å‰çš„ä¿®å¤ï¼ŒCBFç½‘ç»œæœŸæœ›å•ä¸ªæ™ºèƒ½ä½“çš„è¾“å…¥
                    cbf_values_list = []
                    for agent_idx in range(num_agents):
                        agent_obs = observations[0, agent_idx, :]  # å–ç¬¬ä¸€ä¸ªbatchçš„ç¬¬agent_idxä¸ªæ™ºèƒ½ä½“
                        print(f"ğŸ” DEBUG: æ™ºèƒ½ä½“{agent_idx} CBFè¾“å…¥å½¢çŠ¶={agent_obs.shape}")
                        
                        cbf_val = cbf_network(agent_obs.unsqueeze(0))  # æ·»åŠ batchç»´åº¦
                        cbf_values_list.append(cbf_val)
                        print(f"ğŸ” DEBUG: æ™ºèƒ½ä½“{agent_idx} CBFè¾“å‡º={cbf_val.item():.4f}")
                    
                    cbf_values = torch.stack(cbf_values_list, dim=1)  # [batch_size, num_agents, 1]
                    print(f"ğŸ” DEBUG: æœ€ç»ˆCBFå€¼å½¢çŠ¶={cbf_values.shape}")
                    
                except Exception as e:
                    print(f"âŒ CBFç½‘ç»œæ¨ç†å¤±è´¥: {e}")
                    cbf_values = None
            
            # è®°å½•æ•°æ®
            trajectory_data['actions'].append(actions[0].cpu().numpy())
            trajectory_data['alphas'].append(alphas[0].cpu().numpy())
            if cbf_values is not None:
                trajectory_data['cbf_values'].append(cbf_values[0].cpu().numpy())
            else:
                trajectory_data['cbf_values'].append(np.zeros((len(current_positions), 1)))
            
            # 4. ç¯å¢ƒæ­¥è¿› - æ·»åŠ è¯Šæ–­
            try:
                print(f"ğŸŒ ç¯å¢ƒæ­¥è¿›...")
                print(f"ğŸ” DEBUG: æ­¥è¿›å‰state.positionså½¢çŠ¶={state.positions.shape}")
                print(f"ğŸ” DEBUG: æ­¥è¿›actionså½¢çŠ¶={actions.shape}")
                print(f"ğŸ” DEBUG: æ­¥è¿›alphaså½¢çŠ¶={alphas.shape}")
                
                step_result = env.step(state, actions, alphas)
                
                print(f"ğŸ” DEBUG: æ­¥è¿›ånext_state.positionså½¢çŠ¶={step_result.next_state.positions.shape}")
                print(f"âœ… ç¯å¢ƒæ­¥è¿›æˆåŠŸ")
                
                # æ›´æ–°çŠ¶æ€
                state = step_result.next_state
                
            except Exception as e:
                print(f"âŒ ç¯å¢ƒæ­¥è¿›å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                break
            
            # æ˜¾ç¤ºè¿›åº¦
            if step % 20 == 0:
                action_magnitude = torch.norm(actions, dim=-1).mean().item()
                print(f"ğŸ“Š æ­¥éª¤ {step}: å¹³å‡åŠ¨ä½œå¼ºåº¦={action_magnitude:.4f}")
    
    print(f"âœ… ä»¿çœŸå®Œæˆï¼Œå…± {len(trajectory_data['positions'])} æ­¥")
    return trajectory_data


def create_final_visualization(trajectory_data, env_config, output_path):
    """
    åˆ›å»ºæœ€ç»ˆçš„å¯è§†åŒ–åŠ¨ç”»
    """
    print(f"ğŸ¨ åˆ›å»ºæœ€ç»ˆå¯è§†åŒ–åŠ¨ç”»...")
    
    positions_history = trajectory_data['positions']
    actions_history = trajectory_data['actions']
    alphas_history = trajectory_data['alphas']
    cbf_values_history = trajectory_data['cbf_values']
    
    if not positions_history:
        print(f"âŒ æ²¡æœ‰è½¨è¿¹æ•°æ®")
        return False
    
    num_steps = len(positions_history)
    num_agents = len(positions_history[0])
    
    print(f"ğŸ“Š åŠ¨ç”»å‚æ•°: {num_steps} æ­¥, {num_agents} æ™ºèƒ½ä½“")
    
    # åˆ›å»ºå›¾å½¢
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle('ğŸ¯ æœ€ç»ˆåä½œå¯è§†åŒ–ç»“æœ - ç»Ÿä¸€ä»£ç è·¯å¾„', fontsize=18, fontweight='bold')
    
    # ä¸»è½¨è¿¹å›¾
    ax1.set_xlim(-3.0, 3.0)
    ax1.set_ylim(-2.0, 2.0)
    ax1.set_aspect('equal')
    ax1.set_title('ğŸš çœŸå®è®­ç»ƒæ¨¡å‹è½¨è¿¹', fontsize=14)
    ax1.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶éšœç¢ç‰©
    obstacles = env_config.get('obstacles', {})
    if obstacles.get('enabled', False):
        for i, (pos, radius) in enumerate(zip(obstacles.get('positions', []), obstacles.get('radii', []))):
            circle = plt.Circle(pos, radius, color='red', alpha=0.8, 
                              label='éšœç¢ç‰©' if i == 0 else "")
            ax1.add_patch(circle)
    
    # èµ·å§‹å’Œç›®æ ‡åŒºåŸŸ
    start_zone = plt.Rectangle((-2.5, -1.5), 1.0, 3.0, fill=False, 
                              edgecolor='green', linestyle='--', linewidth=2, 
                              alpha=0.8, label='èµ·å§‹åŒºåŸŸ')
    ax1.add_patch(start_zone)
    
    target_zone = plt.Rectangle((1.5, -1.5), 1.0, 3.0, fill=False, 
                               edgecolor='blue', linestyle='--', linewidth=2, 
                               alpha=0.8, label='ç›®æ ‡åŒºåŸŸ')
    ax1.add_patch(target_zone)
    
    # æ™ºèƒ½ä½“é¢œè‰²
    colors = ['#FF4444', '#44FF44', '#4444FF', '#FFAA44', '#FF44AA', '#44AAFF'][:num_agents]
    
    # åˆå§‹åŒ–åŠ¨ç”»å…ƒç´ 
    trail_lines = []
    drone_dots = []
    
    for i in range(num_agents):
        line, = ax1.plot([], [], '-', color=colors[i], alpha=0.8, linewidth=3,
                        label=f'æ™ºèƒ½ä½“{i+1}' if i < 3 else "")
        trail_lines.append(line)
        
        drone, = ax1.plot([], [], 'o', color=colors[i], markersize=14, 
                         markeredgecolor='black', markeredgewidth=2, zorder=5)
        drone_dots.append(drone)
    
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # åˆ†æå›¾è¡¨
    ax2.set_title('ğŸ§  ç­–ç•¥ç½‘ç»œè¾“å‡º', fontsize=12)
    ax2.set_xlabel('æ—¶é—´æ­¥')
    ax2.set_ylabel('åŠ¨ä½œå¼ºåº¦')
    ax2.grid(True, alpha=0.3)
    
    ax3.set_title('âš–ï¸ Alphaå€¼ç›‘æ§', fontsize=12)
    ax3.set_xlabel('æ—¶é—´æ­¥')
    ax3.set_ylabel('Alphaå€¼')
    ax3.grid(True, alpha=0.3)
    
    ax4.set_title('ğŸ›¡ï¸ CBFå®‰å…¨å€¼', fontsize=12)
    ax4.set_xlabel('æ—¶é—´æ­¥')
    ax4.set_ylabel('CBFå€¼')
    ax4.grid(True, alpha=0.3)
    
    def animate(frame):
        if frame >= num_steps:
            return trail_lines + drone_dots
        
        current_positions = positions_history[frame]
        
        # æ›´æ–°è½¨è¿¹å’Œæ™ºèƒ½ä½“
        for i in range(num_agents):
            trail_x = [pos[i, 0] for pos in positions_history[:frame+1]]
            trail_y = [pos[i, 1] for pos in positions_history[:frame+1]]
            trail_lines[i].set_data(trail_x, trail_y)
            
            drone_dots[i].set_data([current_positions[i, 0]], [current_positions[i, 1]])
        
        # æ›´æ–°åˆ†æå›¾è¡¨
        if frame > 5:
            steps = list(range(frame+1))
            
            # ç­–ç•¥è¾“å‡º
            if len(actions_history) > frame:
                action_magnitudes = []
                for step in range(frame+1):
                    if step < len(actions_history):
                        step_actions = actions_history[step]
                        avg_magnitude = np.mean([np.linalg.norm(a) for a in step_actions])
                        action_magnitudes.append(avg_magnitude)
                    else:
                        action_magnitudes.append(0)
                
                ax2.clear()
                ax2.plot(steps, action_magnitudes, 'purple', linewidth=3, label='å¹³å‡åŠ¨ä½œå¼ºåº¦')
                ax2.fill_between(steps, action_magnitudes, alpha=0.3, color='purple')
                ax2.set_title(f'ğŸ§  ç­–ç•¥ç½‘ç»œè¾“å‡º (æ­¥æ•°: {frame})')
                ax2.set_xlabel('æ—¶é—´æ­¥')
                ax2.set_ylabel('åŠ¨ä½œå¼ºåº¦')
                ax2.legend()
                ax2.grid(True, alpha=0.3)
            
            # Alphaå€¼
            if len(alphas_history) > frame:
                alpha_values = []
                for step in range(frame+1):
                    if step < len(alphas_history):
                        avg_alpha = np.mean(alphas_history[step])
                        alpha_values.append(avg_alpha)
                    else:
                        alpha_values.append(0.5)
                
                ax3.clear()
                ax3.plot(steps, alpha_values, 'orange', linewidth=3, label='å¹³å‡Alphaå€¼')
                ax3.fill_between(steps, alpha_values, alpha=0.3, color='orange')
                ax3.set_title(f'âš–ï¸ Alphaå€¼ç›‘æ§ (æ­¥æ•°: {frame})')
                ax3.set_xlabel('æ—¶é—´æ­¥')
                ax3.set_ylabel('Alphaå€¼')
                ax3.legend()
                ax3.grid(True, alpha=0.3)
            
            # CBFå€¼
            if len(cbf_values_history) > frame:
                cbf_avg_values = []
                for step in range(frame+1):
                    if step < len(cbf_values_history):
                        avg_cbf = np.mean(cbf_values_history[step])
                        cbf_avg_values.append(avg_cbf)
                    else:
                        cbf_avg_values.append(0)
                
                ax4.clear()
                ax4.plot(steps, cbf_avg_values, 'red', linewidth=3, label='å¹³å‡CBFå€¼')
                ax4.fill_between(steps, cbf_avg_values, alpha=0.3, color='red')
                ax4.set_title(f'ğŸ›¡ï¸ CBFå®‰å…¨å€¼ (æ­¥æ•°: {frame})')
                ax4.set_xlabel('æ—¶é—´æ­¥')
                ax4.set_ylabel('CBFå€¼')
                ax4.legend()
                ax4.grid(True, alpha=0.3)
        
        return trail_lines + drone_dots
    
    # åˆ›å»ºåŠ¨ç”»
    anim = FuncAnimation(fig, animate, frames=num_steps, interval=150, blit=False, repeat=True)
    
    # ä¿å­˜åŠ¨ç”»
    try:
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆå¯è§†åŒ–: {output_path}")
        
        # å°è¯•ä¿å­˜ä¸ºMP4
        if output_path.endswith('.mp4'):
            anim.save(output_path, writer='ffmpeg', fps=8, dpi=150)
        else:
            anim.save(output_path, writer='pillow', fps=8, dpi=150)
        
        file_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"âœ… ä¿å­˜æˆåŠŸ: {output_path}")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size:.2f}MB")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return False
    finally:
        plt.close()


def main():
    """
    ä¸»å‡½æ•° - å®Œå…¨ç»Ÿä¸€çš„å¯è§†åŒ–æµç¨‹
    """
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€çš„BPTTå¯è§†åŒ–è„šæœ¬')
    parser.add_argument('--model_dir', type=str, default='logs/full_collaboration_training', 
                       help='æ¨¡å‹ç›®å½•è·¯å¾„')
    parser.add_argument('--step', type=int, help='æ¨¡å‹æ­¥éª¤ (é»˜è®¤ä½¿ç”¨æœ€æ–°)')
    parser.add_argument('--device', type=str, default='cpu', help='è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--output', type=str, default='FINAL_COLLABORATION_RESULT.mp4', 
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--num_steps', type=int, default=120, help='ä»¿çœŸæ­¥æ•°')
    
    args = parser.parse_args()
    
    print(f"ğŸ¯ ç»Ÿä¸€BPTTå¯è§†åŒ–ç³»ç»Ÿ")
    print(f"=" * 80)
    print(f"ğŸ“ æ¨¡å‹ç›®å½•: {args.model_dir}")
    print(f"ğŸ“ˆ æ¨¡å‹æ­¥éª¤: {args.step if args.step else 'æœ€æ–°'}")
    print(f"ğŸ’» è®¾å¤‡: {args.device}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"=" * 80)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device)
    
    try:
        # 1. åŠ è½½è®­ç»ƒæ¨¡å‹ - ç»Ÿä¸€è·¯å¾„
        print(f"\nğŸ”„ ç¬¬1æ­¥: åŠ è½½è®­ç»ƒæ¨¡å‹")
        env, policy_network, cbf_network, config = load_trained_model(
            args.model_dir, args.step, device
        )
        
        # 2. è¿è¡Œä»¿çœŸ - åŒ…å«è¯Šæ–­
        print(f"\nğŸ”„ ç¬¬2æ­¥: è¿è¡Œä»¿çœŸ (åŒ…å«è¯Šæ–­)")
        trajectory_data = run_simulation_with_diagnostics(
            env, policy_network, cbf_network, device, args.num_steps
        )
        
        # 3. åˆ›å»ºå¯è§†åŒ–
        print(f"\nğŸ”„ ç¬¬3æ­¥: åˆ›å»ºæœ€ç»ˆå¯è§†åŒ–")
        success = create_final_visualization(
            trajectory_data, config.get('env', {}), args.output
        )
        
        if success:
            print(f"\nğŸ‰ ç»Ÿä¸€å¯è§†åŒ–ç”ŸæˆæˆåŠŸ!")
            print(f"ğŸ“ æœ€ç»ˆç»“æœ: {args.output}")
            print(f"âœ… ä»£ç è·¯å¾„å·²å®Œå…¨ç»Ÿä¸€")
            print(f"ğŸ§  è¿™æ˜¯æ‚¨çœŸå®è®­ç»ƒæ¨¡å‹çš„è¡¨ç°")
        else:
            print(f"\nâŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥")
            
    except Exception as e:
        print(f"\nâŒ ç»Ÿä¸€å¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
 
"""
ç»Ÿä¸€çš„BPTTå¯è§†åŒ–è„šæœ¬
å®Œå…¨é•œåƒtrain_bptt.pyçš„é…ç½®åŠ è½½ã€ç¯å¢ƒåˆ›å»ºå’Œæ¨¡å‹å®ä¾‹åŒ–é€»è¾‘
"""

import argparse
import os
import random
import numpy as np
import torch
import torch.nn as nn
import yaml
from pathlib import Path
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

from gcbfplus.env import DoubleIntegratorEnv
from gcbfplus.env.gcbf_safety_layer import GCBFSafetyLayer
from gcbfplus.policy import BPTTPolicy, create_policy_from_config


def load_trained_model(model_dir, step=None, device='cpu'):
    """
    å®Œå…¨é•œåƒtrain_bptt.pyçš„æ¨¡å‹åŠ è½½é€»è¾‘
    """
    print(f"ğŸ” ç»Ÿä¸€æ¨¡å‹åŠ è½½æµç¨‹")
    print(f"ğŸ“ æ¨¡å‹ç›®å½•: {model_dir}")
    
    # 1. æŸ¥æ‰¾é…ç½®æ–‡ä»¶ - é•œåƒè®­ç»ƒè„šæœ¬çš„é€»è¾‘
    config_path = os.path.join(model_dir, 'config.yaml')
    if not os.path.exists(config_path):
        # å°è¯•çˆ¶ç›®å½•
        config_path = os.path.join(model_dir, '..', 'config.yaml')
        if not os.path.exists(config_path):
            # å°è¯•æ ¹ç›®å½•çš„é…ç½®æ–‡ä»¶
            possible_configs = [
                'config/simple_collaboration.yaml', 
                'config/alpha_medium_obs.yaml',
                'config/bptt_config.yaml'
            ]
            for config_file in possible_configs:
                if os.path.exists(config_file):
                    config_path = config_file
                    break
            else:
                raise ValueError(f"æ— æ³•æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œæ£€æŸ¥è¿‡çš„è·¯å¾„: {possible_configs}")
    
    print(f"ğŸ“‹ ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
    
    # 2. åŠ è½½é…ç½® - å®Œå…¨é•œåƒtrain_bptt.pyçš„é€»è¾‘
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
    
    # 3. æå–é…ç½®éƒ¨åˆ† - é•œåƒtrain_bptt.py
    env_config = config.get('env', {})
    training_config = config.get('training', {})
    network_config = config.get('networks', {})
    
    # å¦‚æœé…ç½®ä¸­æ²¡æœ‰networkséƒ¨åˆ†ï¼Œæ·»åŠ é»˜è®¤å€¼
    if not network_config:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ç¼ºå°‘networkséƒ¨åˆ†ï¼Œæ·»åŠ é»˜è®¤é…ç½®")
        network_config = {
            'policy': {},
            'cbf': {'alpha': 1.0}
        }
        config['networks'] = network_config
    
    # ç¡®ä¿ç¯å¢ƒæœ‰éšœç¢ç‰©é…ç½®ï¼ˆå› ä¸ºæ¨¡å‹æ˜¯åœ¨9ç»´è¾“å…¥ä¸‹è®­ç»ƒçš„ï¼‰
    if 'obstacles' not in env_config:
        print(f"âš ï¸ æ·»åŠ éšœç¢ç‰©é…ç½®ä»¥åŒ¹é…9ç»´è¾“å…¥æ¨¡å‹")
        env_config['obstacles'] = {
            'enabled': True,
            'bottleneck': True,
            'positions': [[0.0, -0.8], [0.0, 0.8]],
            'radii': [0.4, 0.4]
        }
    
    # æå–ç­–ç•¥å’ŒCBFç½‘ç»œé…ç½®
    policy_config = network_config.get('policy', {})
    cbf_network_config = network_config.get('cbf')
    
    print(f"ğŸ“Š ç¯å¢ƒé…ç½®: {list(env_config.keys())}")
    print(f"ğŸ§  ç­–ç•¥é…ç½®: {list(policy_config.keys())}")
    print(f"ğŸ›¡ï¸ CBFé…ç½®: {cbf_network_config is not None}")
    
    # 4. åˆ›å»ºç¯å¢ƒ - å®Œå…¨é•œåƒtrain_bptt.pyçš„é€»è¾‘
    env_type = 'double_integrator'  # é»˜è®¤å€¼ï¼Œé•œåƒè®­ç»ƒè„šæœ¬
    
    if env_type == 'double_integrator':
        env = DoubleIntegratorEnv(env_config)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç¯å¢ƒç±»å‹: {env_type}")
    
    print(f"ğŸŒ ç¯å¢ƒåˆ›å»ºæˆåŠŸ: {env_type}")
    
    # å°†ç¯å¢ƒç§»åŠ¨åˆ°è®¾å¤‡
    env = env.to(device)
    
    # 5. åˆ›å»ºç­–ç•¥ç½‘ç»œ - å®Œå…¨é•œåƒtrain_bptt.pyçš„é€»è¾‘
    if policy_config:
        # ç¡®ä¿ç­–ç•¥é…ç½®å…·æœ‰æ­£ç¡®çš„è§‚æµ‹å’ŒåŠ¨ä½œç»´åº¦
        obs_shape = env.observation_shape
        action_shape = env.action_shape
        
        print(f"ğŸ“ è§‚æµ‹å½¢çŠ¶: {obs_shape}")
        print(f"ğŸ“ åŠ¨ä½œå½¢çŠ¶: {action_shape}")
        
        # DEBUG: æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” DEBUG: obs_shapeç±»å‹={type(obs_shape)}, å€¼={obs_shape}")
        print(f"ğŸ” DEBUG: action_shapeç±»å‹={type(action_shape)}, å€¼={action_shape}")
        
        # å¦‚æœéœ€è¦ï¼Œä¸ºç¼ºå¤±çš„æ„ŸçŸ¥é…ç½®æ·»åŠ é»˜è®¤å€¼ - é•œåƒè®­ç»ƒè„šæœ¬
        if 'perception' not in policy_config:
            policy_config['perception'] = {}
        
        perception_config = policy_config['perception']
        
        # å¤„ç†è§†è§‰è¾“å…¥ - é•œåƒè®­ç»ƒè„šæœ¬
        if len(obs_shape) > 2:  # è§†è§‰è¾“å…¥ [n_agents, channels, height, width]
            perception_config.update({
                'use_vision': True,
                'input_dim': obs_shape[-3:],  # [channels, height, width]
                'output_dim': perception_config.get('output_dim', 256)
            })
        else:  # çŠ¶æ€è¾“å…¥ [n_agents, obs_dim]
            perception_config.update({
                'use_vision': False,
                'input_dim': obs_shape[-1],  # obs_dim
                'output_dim': perception_config.get('output_dim', 128),
                'hidden_dims': perception_config.get('hidden_dims', [256, 256])
            })
        
        print(f"ğŸ” DEBUG: perception_config={perception_config}")
        
        # å¦‚æœéœ€è¦ï¼Œæ·»åŠ é»˜è®¤è®°å¿†é…ç½® - é•œåƒè®­ç»ƒè„šæœ¬
        if 'memory' not in policy_config:
            policy_config['memory'] = {}
        
        memory_config = policy_config['memory']
        memory_config.update({
            'hidden_dim': memory_config.get('hidden_dim', 128),
            'num_layers': memory_config.get('num_layers', 1)
        })
        
        # ç¡®ä¿policy_headå…·æœ‰æ‰€æœ‰å¿…éœ€å‚æ•° - é•œåƒè®­ç»ƒè„šæœ¬
        if 'policy_head' not in policy_config:
            # ä»æ„ŸçŸ¥æˆ–è®°å¿†é…ç½®è·å–hidden_dimï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
            if len(obs_shape) > 2:  # è§†è§‰æƒ…å†µ
                hidden_dims = perception_config.get('output_dim', 256)
            else:  # çŠ¶æ€æƒ…å†µ
                hidden_dims = perception_config.get('hidden_dims', [256, 256])
                if isinstance(hidden_dims, list):
                    hidden_dims = hidden_dims[0] if hidden_dims else 256
            
            policy_config['policy_head'] = {
                'output_dim': action_shape[-1],  # action_dim
                'hidden_dims': [hidden_dims],
                'activation': 'relu',
                'predict_alpha': True  # å¯ç”¨è‡ªé€‚åº”å®‰å…¨è¾¹è·
            }
        else:
            policy_head_config = policy_config['policy_head']
            policy_head_config['output_dim'] = action_shape[-1]  # ç¡®ä¿æ­£ç¡®çš„åŠ¨ä½œç»´åº¦
            if 'predict_alpha' not in policy_head_config:
                policy_head_config['predict_alpha'] = True  # é»˜è®¤å¯ç”¨åŠ¨æ€alpha
        
        print(f"ğŸ¯ æœ€ç»ˆç­–ç•¥é…ç½®: {policy_config}")
    else:
        # åå¤‡æ–¹æ¡ˆï¼šå¦‚æœYAMLä¸­æ²¡æœ‰ç­–ç•¥é…ç½®ï¼Œåˆ›å»ºé»˜è®¤é…ç½® - é•œåƒè®­ç»ƒè„šæœ¬
        obs_shape = env.observation_shape
        action_shape = env.action_shape
        
        print(f"ğŸ“ åå¤‡ - è§‚æµ‹å½¢çŠ¶: {obs_shape}")
        print(f"ğŸ“ åå¤‡ - åŠ¨ä½œå½¢çŠ¶: {action_shape}")
        
        if len(obs_shape) > 2:  # è§†è§‰è¾“å…¥
            policy_config = {
                'perception': {
                    'use_vision': True,
                    'input_dim': obs_shape[-3:],  # [channels, height, width]
                    'output_dim': 256,
                    'vision': {
                        'input_channels': obs_shape[-3],
                        'channels': [32, 64, 128],
                        'height': obs_shape[-2],
                        'width': obs_shape[-1]
                    }
                },
                'memory': {
                    'hidden_dim': 128,
                    'num_layers': 1
                },
                'policy_head': {
                    'output_dim': action_shape[-1],
                    'hidden_dims': [256],
                    'activation': 'relu',
                    'predict_alpha': True
                }
            }
        else:  # çŠ¶æ€è¾“å…¥
            policy_config = {
                'perception': {
                    'use_vision': False,
                    'input_dim': obs_shape[-1],
                    'output_dim': 128,
                    'hidden_dims': [256, 256],
                    'activation': 'relu'
                },
                'memory': {
                    'hidden_dim': 128,
                    'num_layers': 1
                },
                'policy_head': {
                    'output_dim': action_shape[-1],
                    'hidden_dims': [256, 256],
                    'activation': 'relu',
                    'predict_alpha': True
                }
            }
    
    # 6. åˆ›å»ºç­–ç•¥ç½‘ç»œ - æ ¹æ®å®é™…æ¨¡å‹æƒé‡æ¨æ–­æ¶æ„
    print(f"ğŸ§  åˆ›å»ºç­–ç•¥ç½‘ç»œ...")
    
    # é¦–å…ˆå°è¯•ä»æ¨¡å‹æ–‡ä»¶æ¨æ–­æ­£ç¡®çš„æ¶æ„
    model_step = None
    models_dir = os.path.join(model_dir, 'models')
    if os.path.exists(models_dir):
        steps = [int(d) for d in os.listdir(models_dir) if d.isdigit()]
        if steps:
            model_step = max(steps)
    
    if model_step:
        policy_path = os.path.join(model_dir, 'models', str(model_step), 'policy.pt')
        if os.path.exists(policy_path):
            print(f"ğŸ” ä»æ¨¡å‹æƒé‡æ¨æ–­ç½‘ç»œæ¶æ„: {policy_path}")
            policy_state_dict = torch.load(policy_path, map_location='cpu', weights_only=True)
            
            # åˆ†ææƒé‡æ¥æ¨æ–­æ­£ç¡®çš„æ¶æ„
            perception_out_features = None
            memory_hidden_dim = None
            
            # ä»ç¬¬ä¸€å±‚æ¨æ–­perceptionè¾“å‡ºç»´åº¦
            if 'perception.mlp.0.weight' in policy_state_dict:
                perception_out_features = policy_state_dict['perception.mlp.0.weight'].shape[0]
                print(f"ğŸ” æ¨æ–­perceptionè¾“å‡ºç»´åº¦: {perception_out_features}")
            
            # ä»memoryå±‚æ¨æ–­hidden_dim
            if 'memory.gru.weight_hh_l0' in policy_state_dict:
                memory_hidden_dim = policy_state_dict['memory.gru.weight_hh_l0'].shape[1]
                print(f"ğŸ” æ¨æ–­memory hiddenç»´åº¦: {memory_hidden_dim}")
            
            # æ›´æ–°policyé…ç½®
            if perception_out_features:
                policy_config['perception']['output_dim'] = perception_out_features
                policy_config['perception']['hidden_dims'] = [perception_out_features, perception_out_features]
            
            if memory_hidden_dim:
                policy_config['memory']['hidden_dim'] = memory_hidden_dim
            
            # æ›´æ–°policy_headé…ç½®
            if perception_out_features:
                policy_config['policy_head']['hidden_dims'] = [perception_out_features]
            
            print(f"ğŸ¯ æ¨æ–­åçš„ç­–ç•¥é…ç½®: {policy_config}")
    
    policy_network = create_policy_from_config(policy_config)
    policy_network = policy_network.to(device)
    print(f"âœ… ç­–ç•¥ç½‘ç»œåˆ›å»ºæˆåŠŸ")
    
    # 7. åˆ›å»ºCBFç½‘ç»œ - é•œåƒtrain_bptt.pyçš„é€»è¾‘
    cbf_network = None
    if cbf_network_config:
        print(f"ğŸ›¡ï¸ åˆ›å»ºCBFç½‘ç»œ...")
        # ä»é…ç½®ä¸­æå–CBF alphaå‚æ•°
        cbf_alpha = cbf_network_config.get('alpha', 1.0)
        
        # åŸºäºCBFç½‘ç»œé…ç½®åˆ›å»ºCBFç½‘ç»œ - é•œåƒè®­ç»ƒè„šæœ¬
        obs_dim = obs_shape[-1] if len(obs_shape) <= 2 else np.prod(obs_shape[-3:])
        
        print(f"ğŸ” DEBUG: obs_dim={obs_dim}, num_agents={env_config.get('num_agents', 8)}")
        
        # âŒ è¿™é‡Œæ˜¯å…³é”®é—®é¢˜ï¼è®­ç»ƒè„šæœ¬ä¸­çš„CBFç½‘ç»œåˆ›å»ºé€»è¾‘æ˜¯é”™è¯¯çš„
        # éœ€è¦æ ¹æ®å®é™…çš„CBFæ¨¡å‹æ–‡ä»¶æ¥ç¡®å®šæ­£ç¡®çš„æ¶æ„
        
        # è®©æˆ‘ä»¬é¦–å…ˆå°è¯•åŠ è½½CBFæ¨¡å‹æ¥ç¡®å®šæ­£ç¡®çš„è¾“å…¥ç»´åº¦
        cbf_model_path = None
        if step:
            cbf_model_path = os.path.join(model_dir, 'models', str(step), 'cbf.pt')
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„CBFæ¨¡å‹
            models_dir = os.path.join(model_dir, 'models')
            if os.path.exists(models_dir):
                steps = [int(d) for d in os.listdir(models_dir) if d.isdigit()]
                if steps:
                    latest_step = max(steps)
                    cbf_model_path = os.path.join(model_dir, 'models', str(latest_step), 'cbf.pt')
        
        if cbf_model_path and os.path.exists(cbf_model_path):
            # å°è¯•åˆ†æCBFæ¨¡å‹çš„å®é™…æ¶æ„
            cbf_state_dict = torch.load(cbf_model_path, map_location='cpu', weights_only=True)
            
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªçº¿æ€§å±‚æ¥ç¡®å®šè¾“å…¥ç»´åº¦
            first_layer_key = None
            for key in cbf_state_dict.keys():
                if 'weight' in key and len(cbf_state_dict[key].shape) == 2:
                    first_layer_key = key
                    break
            
            if first_layer_key:
                actual_input_dim = cbf_state_dict[first_layer_key].shape[1]
                print(f"ğŸ” DEBUG: CBFå®é™…è¾“å…¥ç»´åº¦={actual_input_dim}")
                
                # æ ¹æ®å®é™…ç»´åº¦åˆ›å»ºCBFç½‘ç»œ
                if actual_input_dim == 6:
                    # å•ä¸ªæ™ºèƒ½ä½“çš„6ç»´çŠ¶æ€
                    cbf_network = nn.Sequential(
                        nn.Linear(6, 128),
                        nn.ReLU(),
                        nn.Linear(128, 128),
                        nn.ReLU(),
                        nn.Linear(128, 1)
                    ).to(device)
                elif actual_input_dim == 9:
                    # å•ä¸ªæ™ºèƒ½ä½“çš„9ç»´çŠ¶æ€ï¼ˆåŒ…å«éšœç¢ç‰©ï¼‰
                    cbf_network = nn.Sequential(
                        nn.Linear(9, 128),
                        nn.ReLU(),
                        nn.Linear(128, 128),
                        nn.ReLU(),
                        nn.Linear(128, 1)
                    ).to(device)
                else:
                    # ä½¿ç”¨æ£€æµ‹åˆ°çš„ç»´åº¦
                    hidden_sizes = [128, 128]  # ä»æ¨¡å‹ä¸­æ¨æ–­
                    layers = []
                    in_dim = actual_input_dim
                    for hidden_size in hidden_sizes:
                        layers.extend([nn.Linear(in_dim, hidden_size), nn.ReLU()])
                        in_dim = hidden_size
                    layers.append(nn.Linear(in_dim, 1))
                    cbf_network = nn.Sequential(*layers).to(device)
                
                print(f"âœ… CBFç½‘ç»œåˆ›å»ºæˆåŠŸ (è¾“å…¥ç»´åº¦: {actual_input_dim})")
            else:
                print(f"âš ï¸ æ— æ³•ç¡®å®šCBFç½‘ç»œæ¶æ„ï¼Œè·³è¿‡CBFç½‘ç»œ")
        else:
            print(f"âš ï¸ CBFæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {cbf_model_path}")
    
    # 8. ç¡®å®šæ¨¡å‹æ­¥éª¤ï¼ˆå¦‚æœå‰é¢æ²¡æœ‰æ‰¾åˆ°ï¼‰
    if step is None:
        if model_step:
            step = model_step
        else:
            # æŸ¥æ‰¾æœ€æ–°æ­¥éª¤
            models_dir = os.path.join(model_dir, 'models')
            if os.path.exists(models_dir):
                steps = [int(d) for d in os.listdir(models_dir) if d.isdigit()]
                if steps:
                    step = max(steps)
                else:
                    raise ValueError("åœ¨modelsç›®å½•ä¸­æ‰¾ä¸åˆ°è®­ç»ƒæ­¥éª¤")
            else:
                raise ValueError("æ‰¾ä¸åˆ°modelsç›®å½•")
    
    print(f"ğŸ“ˆ ä½¿ç”¨æ¨¡å‹æ­¥éª¤: {step}")
    
    # 9. åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    model_path = os.path.join(model_dir, 'models', str(step))
    
    # åŠ è½½ç­–ç•¥ç½‘ç»œæƒé‡ï¼ˆå¦‚æœå‰é¢æ²¡æœ‰åŠ è½½ï¼‰
    policy_path = os.path.join(model_path, 'policy.pt')
    if os.path.exists(policy_path):
        try:
            if 'policy_state_dict' not in locals():
                policy_state_dict = torch.load(policy_path, map_location=device, weights_only=True)
            policy_network.load_state_dict(policy_state_dict)
            print(f"âœ… ç­–ç•¥ç½‘ç»œæƒé‡åŠ è½½æˆåŠŸ: {policy_path}")
        except Exception as e:
            print(f"âŒ ç­–ç•¥ç½‘ç»œæƒé‡åŠ è½½å¤±è´¥: {e}")
            raise
    else:
        raise ValueError(f"ç­–ç•¥æ–‡ä»¶ä¸å­˜åœ¨: {policy_path}")
    
    # åŠ è½½CBFç½‘ç»œæƒé‡
    if cbf_network:
        cbf_path = os.path.join(model_path, 'cbf.pt')
        if os.path.exists(cbf_path):
            try:
                cbf_state_dict = torch.load(cbf_path, map_location=device, weights_only=True)
                cbf_network.load_state_dict(cbf_state_dict)
                print(f"âœ… CBFç½‘ç»œæƒé‡åŠ è½½æˆåŠŸ: {cbf_path}")
            except Exception as e:
                print(f"âŒ CBFç½‘ç»œæƒé‡åŠ è½½å¤±è´¥: {e}")
                print(f"ğŸ”§ å°†CBFç½‘ç»œè®¾ç½®ä¸ºNone")
                cbf_network = None
    
    return env, policy_network, cbf_network, config


def run_simulation_with_diagnostics(env, policy_network, cbf_network, device, num_steps=100):
    """
    è¿è¡Œä»¿çœŸå¹¶æ·»åŠ è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
    """
    print(f"ğŸ¬ å¼€å§‹ä»¿çœŸ (åŒ…å«è¯¦ç»†è¯Šæ–­)")
    print(f"ğŸ“ è®¡åˆ’æ­¥æ•°: {num_steps}")
    
    # è®¾ç½®ç½‘ç»œä¸ºè¯„ä¼°æ¨¡å¼
    policy_network.eval()
    if cbf_network:
        cbf_network.eval()
    
    # åˆå§‹åŒ–ç¯å¢ƒ
    state = env.reset()
    print(f"ğŸ” DEBUG: åˆå§‹çŠ¶æ€ç±»å‹={type(state)}")
    print(f"ğŸ” DEBUG: åˆå§‹çŠ¶æ€.positionså½¢çŠ¶={state.positions.shape}")
    
    # å­˜å‚¨è½¨è¿¹æ•°æ®
    trajectory_data = {
        'positions': [],
        'velocities': [],
        'actions': [],
        'alphas': [],
        'cbf_values': []
    }
    
    with torch.no_grad():
        for step in range(num_steps):
            print(f"\n--- æ­¥éª¤ {step} è¯Šæ–­ ---")
            
            # è®°å½•å½“å‰çŠ¶æ€
            current_positions = state.positions[0].cpu().numpy()
            current_velocities = state.velocities[0].cpu().numpy()
            trajectory_data['positions'].append(current_positions.copy())
            trajectory_data['velocities'].append(current_velocities.copy())
            
            # 1. è·å–è§‚æµ‹ - æ·»åŠ è¯Šæ–­
            observations = env.get_observations(state)
            print(f"ğŸ” DEBUG: è§‚æµ‹å½¢çŠ¶={observations.shape}")
            print(f"ğŸ” DEBUG: è§‚æµ‹dtype={observations.dtype}")
            print(f"ğŸ” DEBUG: è§‚æµ‹è®¾å¤‡={observations.device}")
            print(f"ğŸ” DEBUG: è§‚æµ‹èŒƒå›´=[{torch.min(observations):.4f}, {torch.max(observations):.4f}]")
            
            # ç¡®ä¿è§‚æµ‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            observations = observations.to(device)
            
            # 2. ç­–ç•¥ç½‘ç»œæ¨ç† - æ·»åŠ è¯Šæ–­
            try:
                print(f"ğŸ§  ç­–ç•¥ç½‘ç»œæ¨ç†...")
                policy_output = policy_network(observations, state)
                
                print(f"ğŸ” DEBUG: policy_outputç±»å‹={type(policy_output)}")
                
                if hasattr(policy_output, 'actions'):
                    actions = policy_output.actions
                    print(f"ğŸ” DEBUG: actionså½¢çŠ¶={actions.shape}")
                    print(f"ğŸ” DEBUG: actionsè®¾å¤‡={actions.device}")
                    print(f"ğŸ” DEBUG: actionsèŒƒå›´=[{torch.min(actions):.4f}, {torch.max(actions):.4f}]")
                else:
                    print(f"âŒ policy_outputæ²¡æœ‰actionså±æ€§")
                    raise ValueError("ç­–ç•¥è¾“å‡ºç¼ºå°‘actions")
                
                if hasattr(policy_output, 'alphas'):
                    alphas = policy_output.alphas
                    print(f"ğŸ” DEBUG: alphaså½¢çŠ¶={alphas.shape}")
                    print(f"ğŸ” DEBUG: alphasè®¾å¤‡={alphas.device}")
                    print(f"ğŸ” DEBUG: alphasèŒƒå›´=[{torch.min(alphas):.4f}, {torch.max(alphas):.4f}]")
                else:
                    print(f"âš ï¸ policy_outputæ²¡æœ‰alphaså±æ€§ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    alphas = torch.ones(actions.shape[0], actions.shape[1], 1, device=device) * 0.5
                
            except Exception as e:
                print(f"âŒ ç­–ç•¥ç½‘ç»œæ¨ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                break
            
            # 3. CBFç½‘ç»œæ¨ç† - æ·»åŠ è¯Šæ–­
            cbf_values = None
            if cbf_network:
                try:
                    print(f"ğŸ›¡ï¸ CBFç½‘ç»œæ¨ç†...")
                    
                    # ç¡®å®šCBFç½‘ç»œçš„è¾“å…¥æ ¼å¼
                    batch_size, num_agents, obs_dim = observations.shape
                    print(f"ğŸ” DEBUG: CBFè¾“å…¥ - batch_size={batch_size}, num_agents={num_agents}, obs_dim={obs_dim}")
                    
                    # æ ¹æ®ä¹‹å‰çš„ä¿®å¤ï¼ŒCBFç½‘ç»œæœŸæœ›å•ä¸ªæ™ºèƒ½ä½“çš„è¾“å…¥
                    cbf_values_list = []
                    for agent_idx in range(num_agents):
                        agent_obs = observations[0, agent_idx, :]  # å–ç¬¬ä¸€ä¸ªbatchçš„ç¬¬agent_idxä¸ªæ™ºèƒ½ä½“
                        print(f"ğŸ” DEBUG: æ™ºèƒ½ä½“{agent_idx} CBFè¾“å…¥å½¢çŠ¶={agent_obs.shape}")
                        
                        cbf_val = cbf_network(agent_obs.unsqueeze(0))  # æ·»åŠ batchç»´åº¦
                        cbf_values_list.append(cbf_val)
                        print(f"ğŸ” DEBUG: æ™ºèƒ½ä½“{agent_idx} CBFè¾“å‡º={cbf_val.item():.4f}")
                    
                    cbf_values = torch.stack(cbf_values_list, dim=1)  # [batch_size, num_agents, 1]
                    print(f"ğŸ” DEBUG: æœ€ç»ˆCBFå€¼å½¢çŠ¶={cbf_values.shape}")
                    
                except Exception as e:
                    print(f"âŒ CBFç½‘ç»œæ¨ç†å¤±è´¥: {e}")
                    cbf_values = None
            
            # è®°å½•æ•°æ®
            trajectory_data['actions'].append(actions[0].cpu().numpy())
            trajectory_data['alphas'].append(alphas[0].cpu().numpy())
            if cbf_values is not None:
                trajectory_data['cbf_values'].append(cbf_values[0].cpu().numpy())
            else:
                trajectory_data['cbf_values'].append(np.zeros((len(current_positions), 1)))
            
            # 4. ç¯å¢ƒæ­¥è¿› - æ·»åŠ è¯Šæ–­
            try:
                print(f"ğŸŒ ç¯å¢ƒæ­¥è¿›...")
                print(f"ğŸ” DEBUG: æ­¥è¿›å‰state.positionså½¢çŠ¶={state.positions.shape}")
                print(f"ğŸ” DEBUG: æ­¥è¿›actionså½¢çŠ¶={actions.shape}")
                print(f"ğŸ” DEBUG: æ­¥è¿›alphaså½¢çŠ¶={alphas.shape}")
                
                step_result = env.step(state, actions, alphas)
                
                print(f"ğŸ” DEBUG: æ­¥è¿›ånext_state.positionså½¢çŠ¶={step_result.next_state.positions.shape}")
                print(f"âœ… ç¯å¢ƒæ­¥è¿›æˆåŠŸ")
                
                # æ›´æ–°çŠ¶æ€
                state = step_result.next_state
                
            except Exception as e:
                print(f"âŒ ç¯å¢ƒæ­¥è¿›å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                break
            
            # æ˜¾ç¤ºè¿›åº¦
            if step % 20 == 0:
                action_magnitude = torch.norm(actions, dim=-1).mean().item()
                print(f"ğŸ“Š æ­¥éª¤ {step}: å¹³å‡åŠ¨ä½œå¼ºåº¦={action_magnitude:.4f}")
    
    print(f"âœ… ä»¿çœŸå®Œæˆï¼Œå…± {len(trajectory_data['positions'])} æ­¥")
    return trajectory_data


def create_final_visualization(trajectory_data, env_config, output_path):
    """
    åˆ›å»ºæœ€ç»ˆçš„å¯è§†åŒ–åŠ¨ç”»
    """
    print(f"ğŸ¨ åˆ›å»ºæœ€ç»ˆå¯è§†åŒ–åŠ¨ç”»...")
    
    positions_history = trajectory_data['positions']
    actions_history = trajectory_data['actions']
    alphas_history = trajectory_data['alphas']
    cbf_values_history = trajectory_data['cbf_values']
    
    if not positions_history:
        print(f"âŒ æ²¡æœ‰è½¨è¿¹æ•°æ®")
        return False
    
    num_steps = len(positions_history)
    num_agents = len(positions_history[0])
    
    print(f"ğŸ“Š åŠ¨ç”»å‚æ•°: {num_steps} æ­¥, {num_agents} æ™ºèƒ½ä½“")
    
    # åˆ›å»ºå›¾å½¢
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle('ğŸ¯ æœ€ç»ˆåä½œå¯è§†åŒ–ç»“æœ - ç»Ÿä¸€ä»£ç è·¯å¾„', fontsize=18, fontweight='bold')
    
    # ä¸»è½¨è¿¹å›¾
    ax1.set_xlim(-3.0, 3.0)
    ax1.set_ylim(-2.0, 2.0)
    ax1.set_aspect('equal')
    ax1.set_title('ğŸš çœŸå®è®­ç»ƒæ¨¡å‹è½¨è¿¹', fontsize=14)
    ax1.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶éšœç¢ç‰©
    obstacles = env_config.get('obstacles', {})
    if obstacles.get('enabled', False):
        for i, (pos, radius) in enumerate(zip(obstacles.get('positions', []), obstacles.get('radii', []))):
            circle = plt.Circle(pos, radius, color='red', alpha=0.8, 
                              label='éšœç¢ç‰©' if i == 0 else "")
            ax1.add_patch(circle)
    
    # èµ·å§‹å’Œç›®æ ‡åŒºåŸŸ
    start_zone = plt.Rectangle((-2.5, -1.5), 1.0, 3.0, fill=False, 
                              edgecolor='green', linestyle='--', linewidth=2, 
                              alpha=0.8, label='èµ·å§‹åŒºåŸŸ')
    ax1.add_patch(start_zone)
    
    target_zone = plt.Rectangle((1.5, -1.5), 1.0, 3.0, fill=False, 
                               edgecolor='blue', linestyle='--', linewidth=2, 
                               alpha=0.8, label='ç›®æ ‡åŒºåŸŸ')
    ax1.add_patch(target_zone)
    
    # æ™ºèƒ½ä½“é¢œè‰²
    colors = ['#FF4444', '#44FF44', '#4444FF', '#FFAA44', '#FF44AA', '#44AAFF'][:num_agents]
    
    # åˆå§‹åŒ–åŠ¨ç”»å…ƒç´ 
    trail_lines = []
    drone_dots = []
    
    for i in range(num_agents):
        line, = ax1.plot([], [], '-', color=colors[i], alpha=0.8, linewidth=3,
                        label=f'æ™ºèƒ½ä½“{i+1}' if i < 3 else "")
        trail_lines.append(line)
        
        drone, = ax1.plot([], [], 'o', color=colors[i], markersize=14, 
                         markeredgecolor='black', markeredgewidth=2, zorder=5)
        drone_dots.append(drone)
    
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # åˆ†æå›¾è¡¨
    ax2.set_title('ğŸ§  ç­–ç•¥ç½‘ç»œè¾“å‡º', fontsize=12)
    ax2.set_xlabel('æ—¶é—´æ­¥')
    ax2.set_ylabel('åŠ¨ä½œå¼ºåº¦')
    ax2.grid(True, alpha=0.3)
    
    ax3.set_title('âš–ï¸ Alphaå€¼ç›‘æ§', fontsize=12)
    ax3.set_xlabel('æ—¶é—´æ­¥')
    ax3.set_ylabel('Alphaå€¼')
    ax3.grid(True, alpha=0.3)
    
    ax4.set_title('ğŸ›¡ï¸ CBFå®‰å…¨å€¼', fontsize=12)
    ax4.set_xlabel('æ—¶é—´æ­¥')
    ax4.set_ylabel('CBFå€¼')
    ax4.grid(True, alpha=0.3)
    
    def animate(frame):
        if frame >= num_steps:
            return trail_lines + drone_dots
        
        current_positions = positions_history[frame]
        
        # æ›´æ–°è½¨è¿¹å’Œæ™ºèƒ½ä½“
        for i in range(num_agents):
            trail_x = [pos[i, 0] for pos in positions_history[:frame+1]]
            trail_y = [pos[i, 1] for pos in positions_history[:frame+1]]
            trail_lines[i].set_data(trail_x, trail_y)
            
            drone_dots[i].set_data([current_positions[i, 0]], [current_positions[i, 1]])
        
        # æ›´æ–°åˆ†æå›¾è¡¨
        if frame > 5:
            steps = list(range(frame+1))
            
            # ç­–ç•¥è¾“å‡º
            if len(actions_history) > frame:
                action_magnitudes = []
                for step in range(frame+1):
                    if step < len(actions_history):
                        step_actions = actions_history[step]
                        avg_magnitude = np.mean([np.linalg.norm(a) for a in step_actions])
                        action_magnitudes.append(avg_magnitude)
                    else:
                        action_magnitudes.append(0)
                
                ax2.clear()
                ax2.plot(steps, action_magnitudes, 'purple', linewidth=3, label='å¹³å‡åŠ¨ä½œå¼ºåº¦')
                ax2.fill_between(steps, action_magnitudes, alpha=0.3, color='purple')
                ax2.set_title(f'ğŸ§  ç­–ç•¥ç½‘ç»œè¾“å‡º (æ­¥æ•°: {frame})')
                ax2.set_xlabel('æ—¶é—´æ­¥')
                ax2.set_ylabel('åŠ¨ä½œå¼ºåº¦')
                ax2.legend()
                ax2.grid(True, alpha=0.3)
            
            # Alphaå€¼
            if len(alphas_history) > frame:
                alpha_values = []
                for step in range(frame+1):
                    if step < len(alphas_history):
                        avg_alpha = np.mean(alphas_history[step])
                        alpha_values.append(avg_alpha)
                    else:
                        alpha_values.append(0.5)
                
                ax3.clear()
                ax3.plot(steps, alpha_values, 'orange', linewidth=3, label='å¹³å‡Alphaå€¼')
                ax3.fill_between(steps, alpha_values, alpha=0.3, color='orange')
                ax3.set_title(f'âš–ï¸ Alphaå€¼ç›‘æ§ (æ­¥æ•°: {frame})')
                ax3.set_xlabel('æ—¶é—´æ­¥')
                ax3.set_ylabel('Alphaå€¼')
                ax3.legend()
                ax3.grid(True, alpha=0.3)
            
            # CBFå€¼
            if len(cbf_values_history) > frame:
                cbf_avg_values = []
                for step in range(frame+1):
                    if step < len(cbf_values_history):
                        avg_cbf = np.mean(cbf_values_history[step])
                        cbf_avg_values.append(avg_cbf)
                    else:
                        cbf_avg_values.append(0)
                
                ax4.clear()
                ax4.plot(steps, cbf_avg_values, 'red', linewidth=3, label='å¹³å‡CBFå€¼')
                ax4.fill_between(steps, cbf_avg_values, alpha=0.3, color='red')
                ax4.set_title(f'ğŸ›¡ï¸ CBFå®‰å…¨å€¼ (æ­¥æ•°: {frame})')
                ax4.set_xlabel('æ—¶é—´æ­¥')
                ax4.set_ylabel('CBFå€¼')
                ax4.legend()
                ax4.grid(True, alpha=0.3)
        
        return trail_lines + drone_dots
    
    # åˆ›å»ºåŠ¨ç”»
    anim = FuncAnimation(fig, animate, frames=num_steps, interval=150, blit=False, repeat=True)
    
    # ä¿å­˜åŠ¨ç”»
    try:
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆå¯è§†åŒ–: {output_path}")
        
        # å°è¯•ä¿å­˜ä¸ºMP4
        if output_path.endswith('.mp4'):
            anim.save(output_path, writer='ffmpeg', fps=8, dpi=150)
        else:
            anim.save(output_path, writer='pillow', fps=8, dpi=150)
        
        file_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"âœ… ä¿å­˜æˆåŠŸ: {output_path}")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size:.2f}MB")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return False
    finally:
        plt.close()


def main():
    """
    ä¸»å‡½æ•° - å®Œå…¨ç»Ÿä¸€çš„å¯è§†åŒ–æµç¨‹
    """
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€çš„BPTTå¯è§†åŒ–è„šæœ¬')
    parser.add_argument('--model_dir', type=str, default='logs/full_collaboration_training', 
                       help='æ¨¡å‹ç›®å½•è·¯å¾„')
    parser.add_argument('--step', type=int, help='æ¨¡å‹æ­¥éª¤ (é»˜è®¤ä½¿ç”¨æœ€æ–°)')
    parser.add_argument('--device', type=str, default='cpu', help='è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--output', type=str, default='FINAL_COLLABORATION_RESULT.mp4', 
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--num_steps', type=int, default=120, help='ä»¿çœŸæ­¥æ•°')
    
    args = parser.parse_args()
    
    print(f"ğŸ¯ ç»Ÿä¸€BPTTå¯è§†åŒ–ç³»ç»Ÿ")
    print(f"=" * 80)
    print(f"ğŸ“ æ¨¡å‹ç›®å½•: {args.model_dir}")
    print(f"ğŸ“ˆ æ¨¡å‹æ­¥éª¤: {args.step if args.step else 'æœ€æ–°'}")
    print(f"ğŸ’» è®¾å¤‡: {args.device}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"=" * 80)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device)
    
    try:
        # 1. åŠ è½½è®­ç»ƒæ¨¡å‹ - ç»Ÿä¸€è·¯å¾„
        print(f"\nğŸ”„ ç¬¬1æ­¥: åŠ è½½è®­ç»ƒæ¨¡å‹")
        env, policy_network, cbf_network, config = load_trained_model(
            args.model_dir, args.step, device
        )
        
        # 2. è¿è¡Œä»¿çœŸ - åŒ…å«è¯Šæ–­
        print(f"\nğŸ”„ ç¬¬2æ­¥: è¿è¡Œä»¿çœŸ (åŒ…å«è¯Šæ–­)")
        trajectory_data = run_simulation_with_diagnostics(
            env, policy_network, cbf_network, device, args.num_steps
        )
        
        # 3. åˆ›å»ºå¯è§†åŒ–
        print(f"\nğŸ”„ ç¬¬3æ­¥: åˆ›å»ºæœ€ç»ˆå¯è§†åŒ–")
        success = create_final_visualization(
            trajectory_data, config.get('env', {}), args.output
        )
        
        if success:
            print(f"\nğŸ‰ ç»Ÿä¸€å¯è§†åŒ–ç”ŸæˆæˆåŠŸ!")
            print(f"ğŸ“ æœ€ç»ˆç»“æœ: {args.output}")
            print(f"âœ… ä»£ç è·¯å¾„å·²å®Œå…¨ç»Ÿä¸€")
            print(f"ğŸ§  è¿™æ˜¯æ‚¨çœŸå®è®­ç»ƒæ¨¡å‹çš„è¡¨ç°")
        else:
            print(f"\nâŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥")
            
    except Exception as e:
        print(f"\nâŒ ç»Ÿä¸€å¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
 
 
 
 